<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="/wide-baseline-stereo-blog/feed.xml" rel="self" type="application/atom+xml" /><link href="/wide-baseline-stereo-blog/" rel="alternate" type="text/html" /><updated>2020-09-08T10:32:50-05:00</updated><id>/wide-baseline-stereo-blog/feed.xml</id><title type="html">Wide baseline stereo meets deep learning</title><subtitle>Everything you (didn't) want to know about image matching</subtitle><entry><title type="html">How to match images taken from really extreme viewpoints?</title><link href="/wide-baseline-stereo-blog/2020/08/06/affine-view-synthesis.html" rel="alternate" type="text/html" title="How to match images taken from really extreme viewpoints?" /><published>2020-08-06T00:00:00-05:00</published><updated>2020-08-06T00:00:00-05:00</updated><id>/wide-baseline-stereo-blog/2020/08/06/affine-view-synthesis</id><content type="html" xml:base="/wide-baseline-stereo-blog/2020/08/06/affine-view-synthesis.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-08-06-affine-view-synthesis.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;In-this-post&quot;&gt;In this post&lt;a class=&quot;anchor-link&quot; href=&quot;#In-this-post&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;What to do, if you are in a desperate need of matching this particular image pair?&lt;/li&gt;
&lt;li&gt;What are the limitations of the affine-covariant detectors like Hessian-Affine or HesAffNet?&lt;/li&gt;
&lt;li&gt;ASIFT: brute-force affine view synthesis&lt;/li&gt;
&lt;li&gt;Do as little as possible: MODS&lt;/li&gt;
&lt;li&gt;What is the key factor of affine view synthesis? Ablation study&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;How-to-match-images-taken-from-really-extreme-viewpoints?&quot;&gt;How to match images taken from really extreme viewpoints?&lt;a class=&quot;anchor-link&quot; href=&quot;#How-to-match-images-taken-from-really-extreme-viewpoints?&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Standard wide-baseline stereo or 3d reconstruction pipelines work well in the many situations. Even if some image pair is not matched, it is usually not a problem. For example, one could match images from very different viewpoints, if there is a sequence of images in between, as shown in Figure below, from &quot;From Single Image Query to Detailed 3D Reconstruction&quot; paper[&lt;a class=&quot;latex_cit&quot; id=&quot;call-SingleImage3dRec2015&quot; href=&quot;#cit-SingleImage3dRec2015&quot;&gt;SingleImage3dRec2015&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-08-04-affine-view-synthesis_files/att_00000.png&quot; alt=&quot;&quot; title=&quot;One could handle extreme viewpoint changes by proxy images. Figure from From Single Image Query to Detailed 3D Reconstruction&quot; /&gt;&lt;/p&gt;
&lt;p&gt;However, that might not always be possible. For example, the number of pictures is limited because they historical and there is no way how one could go and take more without inventing a time machine.&lt;/p&gt;
&lt;p&gt;What to do? One way would be to use &lt;a href=&quot;https://ducha-aiki.github.io/wide-baseline-stereo-blog/2020/07/17/affine-correspondences.html&quot;&gt;affine features&lt;/a&gt; like Hessian-AffNet[&lt;a class=&quot;latex_cit&quot; id=&quot;call-AffNet2018&quot; href=&quot;#cit-AffNet2018&quot;&gt;AffNet2018&lt;/a&gt;] or MSER[&lt;a class=&quot;latex_cit&quot; id=&quot;call-MSER2002&quot; href=&quot;#cit-MSER2002&quot;&gt;MSER2002&lt;/a&gt;]. However, they help only up to some extent and what if the view, we need to match are more extreme?&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-08-04-affine-view-synthesis_files/MODS-match-historically.png&quot; alt=&quot;&quot; title=&quot;Non-matchable by standard methods pair of historical photographies. Matched only with help of affine view synthesis in the MODS framework. Images from Location recognition over large time lags dataset&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The image pair above is from &quot;Location recognition over large time lags dataset&quot; paper [&lt;a class=&quot;latex_cit&quot; id=&quot;call-LostInPast2015&quot; href=&quot;#cit-LostInPast2015&quot;&gt;LostInPast2015&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;The solution is to simulate real viewpoint change by affine or perspective warps of the current image. This idea was first proposed by Lepetit and Fua in 2006[&lt;a class=&quot;latex_cit&quot; id=&quot;call-AffineTree2006&quot; href=&quot;#cit-AffineTree2006&quot;&gt;AffineTree2006&lt;/a&gt;]. You can think about it as a special version of test-time augmentation, popular nowadays in deep learning. Later affine view synthesis for wide baseline stereo was extended and mathematically justified by Morel &amp;amp; Yu in ASIFT paper[&lt;a class=&quot;latex_cit&quot; id=&quot;call-ASIFT2009&quot; href=&quot;#cit-ASIFT2009&quot;&gt;ASIFT2009&lt;/a&gt;]. They proved that perspective image warps are can be approximated by synthetic affine views.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;What-is-wrong-with-affine-covariant-local-detectors?&quot;&gt;What is wrong with affine-covariant local detectors?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-wrong-with-affine-covariant-local-detectors?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;One could say that the goal of affine-covariant detectors like MSER, Hessian-Affine or Hessian-AffNet is to detect the same region on a planar surface, regardless the camera angle change.
It is true to some extent, as we demostrate on toy example below with Hessian-Affine feature.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-08-04-affine-view-synthesis_files/together_single.png&quot; alt=&quot;&quot; title=&quot;Hessian-Affine detector detects the same support region under synthetic tilt&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The problem arises, when the image content, e.g. 3 blobs on the figure below are situated close to each other, so under the tilt transform the merge into a single blob. So it is not the shape of region, which is detected incorrectly, but the center of the features themselves. For clarity, we omited affine shape estimation on the image below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-08-04-affine-view-synthesis_files/blobs_merging.png&quot; alt=&quot;&quot; title=&quot;When blobs are close to each other, they merge together when picture is taked from a side viewpoint. This results in failure of the blob detector.&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;ASIFT:-brute-force-affine-view-synthesis&quot;&gt;ASIFT: brute-force affine view synthesis&lt;a class=&quot;anchor-link&quot; href=&quot;#ASIFT:-brute-force-affine-view-synthesis&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;So, to solve the problem explained above, Morel &amp;amp; Yu [&lt;a class=&quot;latex_cit&quot; id=&quot;call-ASIFT2009&quot; href=&quot;#cit-ASIFT2009&quot;&gt;ASIFT2009&lt;/a&gt;] proposed to do a lot affine warps of each image, as shown on the Figure below, as match each view against all others, which is $O(n^2)$ complexity, where $n$ is number of views generated.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-08-04-affine-view-synthesis_files/att_00006.png&quot; alt=&quot;&quot; title=&quot;ASIFT algorithm: generate a lot of synthetic views, match all to all. Figure from Fast Affine Invariant Image Matching&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The motivation do doing so it that assuming, original image to be a fronto-parallel one, to cover viewsphere really dense, as shown in the Figure below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-08-04-affine-view-synthesis_files/att_00007.png&quot; alt=&quot;&quot; title=&quot;Viewsphere covering by ASIFT. Figure from Fast Affine Invariant Image Matching&quot; /&gt;&lt;/p&gt;
&lt;p&gt;This leads to impressive performance on a very challenging image pairs, see an example below&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-08-04-affine-view-synthesis_files/att_00009.png&quot; alt=&quot;image.png&quot; title=&quot;ASIFT find a lot of correspondences on challenging pair. Figure from ASIFT: An Algorithm for Fully Affine Invariant Comparison&quot; /&gt;&lt;/p&gt;
&lt;p&gt;In this section I have used great illustrations done by &lt;a href=&quot;https://rdguez-mariano.github.io/&quot;&gt;Mariano Rodr√≠guez&lt;/a&gt; for his paper &quot;Fast Affine Invariant Image Matching&quot; [&lt;a class=&quot;latex_cit&quot; id=&quot;call-FastASIFT2018&quot; href=&quot;#cit-FastASIFT2018&quot;&gt;FastASIFT2018&lt;/a&gt;]. Please, checkout his &lt;a href=&quot;https://rdguez-mariano.github.io/&quot;&gt;blog&lt;/a&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;MODS:-do-as-little-as-possible&quot;&gt;MODS: do as little as possible&lt;a class=&quot;anchor-link&quot; href=&quot;#MODS:-do-as-little-as-possible&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The main drawback of ASIFT algorithm is a huge computational cost: 82 views are generated regardless of the image pair difficulty. To overcome this, we proposed MODS[&lt;a class=&quot;latex_cit&quot; id=&quot;call-MODS2015&quot; href=&quot;#cit-MODS2015&quot;&gt;MODS2015&lt;/a&gt;] algorithm: Matching with On-Demand Synthesis.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-08-04-affine-view-synthesis_files/att_00004.png&quot; alt=&quot;&quot; title=&quot;MODS algorithm: synthetize more views until match&quot; /&gt;&lt;/p&gt;
&lt;p&gt;One starts with the fastest detector-descriptor without view synthesys and then uses more and more computationally expensive methods if needed. Moreover, by using affine-covariant detectors like MSER or Hessian-Affine, one could synthetise significantly less views, saving computations spent on local descriptor and matching.&lt;/p&gt;
&lt;p&gt;This, together with &lt;a href=&quot;https://medium.com/@ducha.aiki/how-to-match-to-learn-or-not-to-learn-part-2-1ab52ede2022&quot;&gt;FGINN matching strategy&lt;/a&gt;, specifically designed for the handling re-detections, MODS is able to match more challenging image pairs in less time than ASIFT.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-08-04-affine-view-synthesis_files/att_00010.png&quot; alt=&quot;&quot; title=&quot;MODS outperforms ASIFT on a several dataset both in terms of speed and quality&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Why-does-affine-synthesis-help?&quot;&gt;Why does affine synthesis help?&lt;a class=&quot;anchor-link&quot; href=&quot;#Why-does-affine-synthesis-help?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Despite that ASIFT and other view-synthesis based approaches are know more than decade, we are not aware of a study, why does affine synthesis helps in practice. Could one get a similar performance without view synthesis?
Specificallly:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;May it be that the most of improvements come from the fact that we have much more features? That is why we fix the number of features for all approaches.&lt;/li&gt;
&lt;li&gt;Some regions from ASIFT, when reprojected to the original image, are quite narrow. Could we get them just by removing edge-like feature filtering, which is done in SIFT, Hessian and other detectors.  Denoted &lt;strong&gt;+edge&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Instead of doing affine view synthesis, one could directly use the same affine parameters to get the affine regions to describe, so the each keypoint would have several associated regions+descriptors. Denoted &lt;strong&gt;+MD&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Using AffNet to directly estimated local affine shape without multiple descriptors. Denoted &lt;strong&gt;+AffNet&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Combine (1), (2) and (3).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So, we did the study on HPatches Sequences dataset, the hardest image pairs (1-6) of viewpoint subset. The metric is similar to one used in the &quot;&lt;a href=&quot;https://arxiv.org/abs/2003.01587&quot;&gt;Image Matching across Wide Baselines: From Paper to Practice&lt;/a&gt;&quot; and CVPR 2020 &lt;a href=&quot;http://cmp.felk.cvut.cz/cvpr2020-ransac-tutorial/&quot;&gt;RANSAC in 2020&lt;/a&gt; - mean average accuracy of the estimated homography.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-08-04-affine-view-synthesis_files/att_00011.png&quot; alt=&quot;&quot; title=&quot; mean average accuracy of the estimated homography - used metric&quot; /&gt;&lt;/p&gt;
&lt;p&gt;We run Hessian detector with RootSIFT descriptor, FLANN matching and LO-RANSAC, as implemented in &lt;a href=&quot;https://github.com/ducha-aiki/mods-light-zmq&quot;&gt;MODS&lt;/a&gt;. Features are sorted according the the detector response and their total number is clipped to 2048 or 8000 to ensure that the improvements do not come from just having more features.&lt;/p&gt;
&lt;p&gt;Note, that we do not study, if view synthesis helps for the regular image pairs - it might actually hurt performance, similarly to &lt;a href=&quot;https://ducha-aiki.github.io/wide-baseline-stereo-blog/2020/07/17/affine-correspondences.html&quot;&gt;affine features&lt;/a&gt;. 
Instead we are focusing on the case, when view synthesis definitely helps: matching obscure views of the mostly planar scenes.&lt;/p&gt;
&lt;h3 id=&quot;8000-feature-budget&quot;&gt;8000 feature budget&lt;a class=&quot;anchor-link&quot; href=&quot;#8000-feature-budget&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Results are in Figure below. Indeed, all of the factors: detecting more edge-like features, having multiple descriptors or better affine shape improve results over the plain Hessian detector, but even all of the combined are not good enough to match performance of the affine view synthesis + plain Hessian detector.&lt;/p&gt;
&lt;p&gt;But the best setup is to use both Hessian-AffNet and view synthesis.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-08-04-affine-view-synthesis_files/8k_budget.png&quot; alt=&quot;&quot; title=&quot;Ablation study of the different benefit from the affine view synthesis: ability to detect elongated blobs, multiple affine decriptors or everything together. Feature budget: 8k&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;2048-feature-budget&quot;&gt;2048 feature budget&lt;a class=&quot;anchor-link&quot; href=&quot;#2048-feature-budget&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The picture is a bit different in a small feature budget: neither multiple-(affine)-descriptors per keypoint, nor allowing edge-like feature help. From other hand, affine view synthesis still improves results of the Hessian. And, again, the best performance is achieved with combination of view synthesis and AffNet shape estimation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-08-04-affine-view-synthesis_files/2k_budget.png&quot; alt=&quot;&quot; title=&quot;Ablation study of the different benefit from the affine view synthesis: ability to detect elongated blobs, multiple affine decriptors or everything together. Feature budget: 8k&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;Summary&quot;&gt;Summary&lt;a class=&quot;anchor-link&quot; href=&quot;#Summary&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Affine view synthesis helps for matching challenging image pairs and its improvement are not just because of more local features used. It can be done effective and efficient -- in the iterative MODS framework.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;References&quot;&gt;References&lt;a class=&quot;anchor-link&quot; href=&quot;#References&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;[&lt;a id=&quot;cit-SingleImage3dRec2015&quot; href=&quot;#call-SingleImage3dRec2015&quot;&gt;SingleImage3dRec2015&lt;/a&gt;] J.L. Schonberger, F. Radenovic, O. Chum &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;From Single Image Query to Detailed 3D Reconstruction&lt;/em&gt;'', Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-AffNet2018&quot; href=&quot;#call-AffNet2018&quot;&gt;AffNet2018&lt;/a&gt;] D. Mishkin, F. Radenovic and J. Matas, ``&lt;em&gt;Repeatability is Not Enough: Learning Affine Regions via Discriminability&lt;/em&gt;'', ECCV,  2018.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-MSER2002&quot; href=&quot;#call-MSER2002&quot;&gt;MSER2002&lt;/a&gt;] J. Matas, O. Chum, M. Urban &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Robust Wide Baseline Stereo from Maximally Stable Extrema Regions&lt;/em&gt;'', BMVC,  2002.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-LostInPast2015&quot; href=&quot;#call-LostInPast2015&quot;&gt;LostInPast2015&lt;/a&gt;] Fernando Basura, Tommasi Tatiana and Tuytelaars Tinne, ``&lt;em&gt;Location recognition over large time lags&lt;/em&gt;'', Computer Vision and Image Understanding, vol. 139, number , pp. ,  2015.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-AffineTree2006&quot; href=&quot;#call-AffineTree2006&quot;&gt;AffineTree2006&lt;/a&gt;] Lepetit Vincent and Fua Pascal, ``&lt;em&gt;Keypoint Recognition Using Randomized Trees&lt;/em&gt;'', IEEE Trans. Pattern Anal. Mach. Intell., vol. 28, number 9, pp. , sep 2006.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-ASIFT2009&quot; href=&quot;#call-ASIFT2009&quot;&gt;ASIFT2009&lt;/a&gt;] Morel Jean-Michel and Yu Guoshen, ``&lt;em&gt;ASIFT: A New Framework for Fully Affine Invariant Image Comparison&lt;/em&gt;'', SIAM J. Img. Sci., vol. 2, number 2, pp. , apr 2009.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-FastASIFT2018&quot; href=&quot;#call-FastASIFT2018&quot;&gt;FastASIFT2018&lt;/a&gt;] Rodr√≠guez Mariano, Delon Julie and Morel Jean-Michel, ``&lt;em&gt;Fast Affine Invariant Image Matching&lt;/em&gt;'', Image Processing On Line, vol. 8, number , pp. ,  2018.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-MODS2015&quot; href=&quot;#call-MODS2015&quot;&gt;MODS2015&lt;/a&gt;] Mishkin Dmytro, Matas Jiri and Perdoch Michal, ``&lt;em&gt;MODS: Fast and robust method for two-view matching &lt;/em&gt;'', Computer Vision and Image Understanding , vol. , number , pp. ,  2015.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/wide-baseline-stereo-blog/images/transition.png" /><media:content medium="image" url="/wide-baseline-stereo-blog/images/transition.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Patch extraction: devil in details</title><link href="/wide-baseline-stereo-blog/2020/07/22/patch-extraction.html" rel="alternate" type="text/html" title="Patch extraction: devil in details" /><published>2020-07-22T00:00:00-05:00</published><updated>2020-07-22T00:00:00-05:00</updated><id>/wide-baseline-stereo-blog/2020/07/22/patch-extraction</id><content type="html" xml:base="/wide-baseline-stereo-blog/2020/07/22/patch-extraction.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-07-22-patch-extraction.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;When working with local features one needs to pay attention to even a smallest details, or the whole process can be ruined. One of such details is how to extract the patch, which will be described by local descriptor such as SIFT or HardNet.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-22-patch-extraction_files/att_00000.png&quot; alt=&quot;&quot; title=&quot;In order to describe the local patch one first has to extract it. Figure from A Few Things One Should Know About Feature Extraction, Description and Matching&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Unfortunately, we cannot just extract patch from the image by cropping the patch and then resizing it. Or can we? 
Let's check. We will use two versions of image: original and 4x smaller one and would like to extract same-looking fixed size patch from both of them. The patch we want to crop is showed by oriented red circle.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-22-patch-extraction_files/both_imgs.png&quot; alt=&quot;&quot; title=&quot;Two versions of the same image: original and 4x smaller&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Aliasing&quot;&gt;Aliasing&lt;a class=&quot;anchor-link&quot; href=&quot;#Aliasing&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;And here what we get by doing a simple crop and resize to 32x32 pixels.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-22-patch-extraction_files/naive_patches.png&quot; alt=&quot;&quot; title=&quot;Patches, which are extracted from images of different sizes look different, if extraction done in a naive way&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Doesn't look good. It is called &quot;&lt;a href=&quot;https://en.wikipedia.org/wiki/Aliasing&quot;&gt;aliasing&lt;/a&gt;&quot; - a problem, which arise when we are trying to downscale big images into small resolution. Specifically: the original image contains finer details, than we could represent in thumbnail, which leads to artifacts. 
&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-22-patch-extraction_files/ep.jpg&quot; alt=&quot;&quot; title=&quot;One does not simply extract patch from the image&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;The-solution:-anti-aliasing&quot;&gt;The solution: anti-aliasing&lt;a class=&quot;anchor-link&quot; href=&quot;#The-solution:-anti-aliasing&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The solution,  which follows out of &lt;a href=&quot;https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem&quot;&gt;sampling theorem&lt;/a&gt; is known: remove the details, which cannot be seens in small image first, then resample image to small size.&lt;/p&gt;
&lt;p&gt;The simplest way to remove the fine details is to blur image with the Gaussian kernel.&lt;/p&gt;
&lt;p&gt;Lets do it and compare the results. 
&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-22-patch-extraction_files/all_patches.png&quot; alt=&quot;&quot; title=&quot;When extracted properly, patches look same&quot; /&gt;&lt;/p&gt;
&lt;p&gt;By the way, you can try for yourself, all the required code is &lt;a href=&quot;https://github.com/kornia/kornia-examples/blob/master/aliased-and-not-aliased-patch-extraction.ipynb&quot;&gt;here, in kornia-examples&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Performance&quot;&gt;Performance&lt;a class=&quot;anchor-link&quot; href=&quot;#Performance&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The problem is solved. Or is it?&lt;/p&gt;
&lt;p&gt;The problem with properly antialiased patch extraction is that it is quite slow for two reasons. First, blurring a whole image is a costly operation. But, the worst part is that the required amount of blur depends on the patch size in the original image, or, in other words, keypoint scale. So for extracting, say 8000 patches, one needs to perform blurring 8000 times. Moreover, if one wants to extract elongated region and warp it to the square patch, the amount of blur in vertical and horizontal directions should be different!&lt;/p&gt;
&lt;p&gt;What can be done? Well, instead of doing blurring 8000 times, one could create so called scale pyramid and then pick the level, which is the closest to optimal one, predicted by theorem.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-22-patch-extraction_files/Image_pyramid.png&quot; alt=&quot;&quot; title=&quot;Image from Wikimedia by wiki-user Cmglee https://en.wikipedia.org/wiki/File:Image_pyramid.svg&quot; /&gt;&lt;/p&gt;
&lt;p&gt;This is exactly, what kornia function &lt;a href=&quot;https://kornia.readthedocs.io/en/latest/feature.html#kornia.feature.extract_patches_from_pyramid&quot;&gt;extract_patches_from_pyramid&lt;/a&gt; does.&lt;/p&gt;
&lt;p&gt;Also - I have a bit cheated with you above: the &quot;anti-aliased&quot; patches were actually extracted using the function above.&lt;/p&gt;
&lt;h2 id=&quot;How-it-impacts-local-descriptor-matching?&quot;&gt;How it impacts local descriptor matching?&lt;a class=&quot;anchor-link&quot; href=&quot;#How-it-impacts-local-descriptor-matching?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Let's do the toy example first - describe four patches we have in the example above with HardNet descriptor and calculate the distance between them.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-22-patch-extraction_files/att_00001.png&quot; alt=&quot;&quot; title=&quot;Descriptor difference between antialiased patches is 0.09 and between naively extracted -- 0.44&quot; /&gt;&lt;/p&gt;
&lt;p&gt;So the descriptor difference between antialiased patches is 0.09 and between naively extracted -- 0.44. 0.09 is not a big deal, but 0.44 is a lot, actually.&lt;/p&gt;
&lt;p&gt;Let's move to the non-toy example from the paper devoted to this topic: &quot;&lt;a href=&quot;http://cmp.felk.cvut.cz/~mishkdmy/lenc-2014-features-cvww.pdf&quot;&gt;A Few Things One Should Know About Feature Extraction, Description and
Matching&lt;/a&gt;&quot;[&lt;a class=&quot;latex_cit&quot; id=&quot;call-PatchExtraction2014&quot; href=&quot;#cit-PatchExtraction2014&quot;&gt;PatchExtraction2014&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;The original data is lost I am too lazy to redo the experiments for the post, so I will just copy-past images with results. Here are abbrevations used in the paper:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;OPE&lt;/strong&gt; -- Optimal Patch Extraction. The most correct and slow way of extracting, including different amount of bluring in different directions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;NBPE&lt;/strong&gt; -- No-Blur Patch Extraction. The most naive way we started with&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;PNBPE&lt;/strong&gt; -- Pyramid, No-Blur Patch Extraction. The one, we described above - sampling patches from scale pyramid.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;PSPE&lt;/strong&gt; Pyramid-Smoothing Patch Extraction. Pick the matching pyramid level and then add anisotropic blur missing.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As you can see, doing things optimally is quite slow. 
&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-22-patch-extraction_files/att_00003.png&quot; alt=&quot;image.png&quot; title=&quot;Time spent on a various stages of local feature extraction&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Now let's see how it influences performance.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-22-patch-extraction_files/att_00005.png&quot; alt=&quot;&quot; title=&quot;Number of matches for MSER and Hessian-Affine + SIFT on Grffity sequence of Oxford-Affine dataset for different patch extraction methods&quot; /&gt;&lt;/p&gt;
&lt;p&gt;It looks like that influence is smaller than we thought. But recall that the experiment above is for SIFT descriptor only.  Doing pyramid helps for the small viewpoint change almost as good, as going fully optimal, but with increasing the viewpoint difference, such approximation degrades. Moreover, it influnces MSER detector much more that than Hessian-Affine.&lt;/p&gt;
&lt;p&gt;How does it work with deep descriptors like HardNet or SoSNet?  That is the question which not answered yet. Drop me a message if you want to do it yourself and we can do the follow-up post together.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;References&quot;&gt;References&lt;a class=&quot;anchor-link&quot; href=&quot;#References&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;[&lt;a id=&quot;cit-PatchExtraction2014&quot; href=&quot;#call-PatchExtraction2014&quot;&gt;PatchExtraction2014&lt;/a&gt;] K. Lenc, J. Matas and D. Mishkin, ``&lt;em&gt;A few things one should know about feature extraction, description and matching&lt;/em&gt;'', Proceedings of the Computer Vision Winter Workshop,  2014.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/wide-baseline-stereo-blog/images/ep.jpg" /><media:content medium="image" url="/wide-baseline-stereo-blog/images/ep.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Local affine features: useful side product</title><link href="/wide-baseline-stereo-blog/2020/07/17/affine-correspondences.html" rel="alternate" type="text/html" title="Local affine features: useful side product" /><published>2020-07-17T00:00:00-05:00</published><updated>2020-07-17T00:00:00-05:00</updated><id>/wide-baseline-stereo-blog/2020/07/17/affine-correspondences</id><content type="html" xml:base="/wide-baseline-stereo-blog/2020/07/17/affine-correspondences.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-07-17-affine-correspondences.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Keypoints-are-not-just-points&quot;&gt;Keypoints are not just points&lt;a class=&quot;anchor-link&quot; href=&quot;#Keypoints-are-not-just-points&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/att_00000.png&quot; alt=&quot;image.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Wide baseline stereo matching often as perceived as establishing (key-)point correspondences between images. While this might be true for the some local features like SuperPoint [&lt;a class=&quot;latex_cit&quot; id=&quot;call-SuperPoint2017&quot; href=&quot;#cit-SuperPoint2017&quot;&gt;SuperPoint2017&lt;/a&gt;], typically it is more than that.&lt;/p&gt;
&lt;p&gt;Specifically, detectors like DoG[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Lowe99&quot; href=&quot;#cit-Lowe99&quot;&gt;Lowe99&lt;/a&gt;], Harris[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Harris88&quot; href=&quot;#cit-Harris88&quot;&gt;Harris88&lt;/a&gt;], Hessian[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Hessian78&quot; href=&quot;#cit-Hessian78&quot;&gt;Hessian78&lt;/a&gt;], KeyNet[&lt;a class=&quot;latex_cit&quot; id=&quot;call-KeyNet2019&quot; href=&quot;#cit-KeyNet2019&quot;&gt;KeyNet2019&lt;/a&gt;], ORB[&lt;a class=&quot;latex_cit&quot; id=&quot;call-ORB2011&quot; href=&quot;#cit-ORB2011&quot;&gt;ORB2011&lt;/a&gt;], and many others rate on scale-space provide at least 3 parameters: x, y, and scale.&lt;/p&gt;
&lt;p&gt;Most of the local descriptors  -- SIFT[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Lowe99&quot; href=&quot;#cit-Lowe99&quot;&gt;Lowe99&lt;/a&gt;], HardNet[&lt;a class=&quot;latex_cit&quot; id=&quot;call-HardNet2017&quot; href=&quot;#cit-HardNet2017&quot;&gt;HardNet2017&lt;/a&gt;] and so on -- are not rotation invariant and those which are - mostly require complex matching function[&lt;a class=&quot;latex_cit&quot; id=&quot;call-RIFT2005&quot; href=&quot;#cit-RIFT2005&quot;&gt;RIFT2005&lt;/a&gt;], [&lt;a class=&quot;latex_cit&quot; id=&quot;call-sGLOH2&quot; href=&quot;#cit-sGLOH2&quot;&gt;sGLOH2&lt;/a&gt;], so the patch orientation has to be estimated anyway, in order to match reliably. This can be done by various methods:
corners center of mass (ORB[&lt;a class=&quot;latex_cit&quot; id=&quot;call-ORB2011&quot; href=&quot;#cit-ORB2011&quot;&gt;ORB2011&lt;/a&gt;], dominant gradient orientation (SIFT)[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Lowe99&quot; href=&quot;#cit-Lowe99&quot;&gt;Lowe99&lt;/a&gt;] or by some learned estimator (OriNets[&lt;a class=&quot;latex_cit&quot; id=&quot;call-OriNet2016&quot; href=&quot;#cit-OriNet2016&quot;&gt;OriNet2016&lt;/a&gt;],[&lt;a class=&quot;latex_cit&quot; id=&quot;call-AffNet2018&quot; href=&quot;#cit-AffNet2018&quot;&gt;AffNet2018&lt;/a&gt;]). Sometimes it is possible to rely on smartphone camera IMU or photographer and assume that images are upright[&lt;a class=&quot;latex_cit&quot; id=&quot;call-PerdochRetrieval2009&quot; href=&quot;#cit-PerdochRetrieval2009&quot;&gt;PerdochRetrieval2009&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;Thus, we can assume that if local descriptors match, this means the local feature scale and orientation also match, at least approximately -- see Figure below. Possible exceptions are cases, when the patch is symmetrical and orientation is ambiguous up to some symmetry group.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/matches_patches.png&quot; alt=&quot;&quot; title=&quot;Selected matching SIFT keypoints and corresponding patches. One can see that not only patch centers correspond to each other, but also other pixels, although less precise. Image pair from Sacre Coeur IMW dataset.&quot; /&gt;&lt;/p&gt;
&lt;p&gt;In addition, one could assume that we observe the patch not from the fronto-parallel position and try to estimate local normal, or, more precisely, affine shape of the feature point, modeling it as an ellipse instead of circle. One could also think of affine shape estimation as finding the camera position, from where the patch is seen in some &quot;canonical&quot; view.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/affinematches_patches.png&quot; alt=&quot;&quot; title=&quot;Selected matching SIFT-AffNet keypoints and corresponding patches. One can see that not only patch centers correspond to each other, but also other pixels, although less precise. Image pair from Sacre Coeur IMW dataset.&quot; /&gt;&lt;/p&gt;
&lt;p&gt;This gives us 3 points correspondences from a single local feature match, see an example in Figure below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/laf-check-illustration.png&quot; alt=&quot;&quot; title=&quot;Local affine correspondences. While centers of both regions A and B are correct point matches, only A is a correct affine correspondence.&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Why is it important and how to use it -- see in current post. How to esimate local affine features robustly -- in the next post.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Benefits-of-local-affine-features&quot;&gt;Benefits of local affine features&lt;a class=&quot;anchor-link&quot; href=&quot;#Benefits-of-local-affine-features&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Making-descriptor-job-easier&quot;&gt;Making descriptor job easier&lt;a class=&quot;anchor-link&quot; href=&quot;#Making-descriptor-job-easier&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The most straightforward benefit of using local affine features is that they increase the repeatability of the detector and potentially reduce appearance changes of a local patch caused by viewpoint difference. This makes possible matching more challenging image pairs.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/test.png&quot; alt=&quot;&quot; title=&quot;Hessian features + HardNet matches + RANSAC inliers, Right: HessAffNet features + HardNet matches + RANSAC inliers. Image pair from Tanks &amp;amp; Temples. Epipolar lines are shown in cyan.&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/att_00009.png&quot; alt=&quot;&quot; title=&quot;Repeatability and the number of correspondences. AffNet compared with the de facto standard Baumberg iteration according to the Mikolajczyk protocol. Left ‚Äì images with illumination differences, right ‚Äì with viewpoint and scale changes. SS ‚Äì patch from the scale-space pyramid at the level of the detection, image ‚Äì from the original image; 19 and 33 are patch sizes.&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The practice is a little bit more complicated. Our recent benchmark[&lt;a class=&quot;latex_cit&quot; id=&quot;call-IMW2020&quot; href=&quot;#cit-IMW2020&quot;&gt;IMW2020&lt;/a&gt;], which measure the accuracy of the output fundamental matrix, shows that the difference in using affine and similarity-covariant features is quite minor. Specifically, the relative difference between SIFT vs SIFT-Affine features is 0.5% and between Hessian-SIFT and Hessian-AffNet SIFT is 5.1%, see Table below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/att_00011.png&quot; alt=&quot;&quot; title=&quot;Test ‚Äì Stereo results with 8k features. We report: NF --  Number of Features; NI -- Number of Inliers produced by RANSAC; and mAA@10¬∞.&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Therefore, if the benefit of local features would be to only improve descriptor extraction stage, it would be arguably not worth it. Luckily, there are more benefits, which are more pronounced.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Making-RANSAC-job-easier&quot;&gt;Making RANSAC job easier&lt;a class=&quot;anchor-link&quot; href=&quot;#Making-RANSAC-job-easier&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Let's recall how RANSAC works.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Randomly sample a minimally required number of tentative correspondences to fit the geometrical model of the scene: 4 for homography, 7 for epipolar geometry and estimate the model.&lt;/li&gt;
&lt;li&gt;Calculate &quot;support&quot;: other correspondeces, which are consistent with the model. &lt;/li&gt;
&lt;li&gt;Repeat steps (1), (2) and output the model which is supported with the most of correspondences. If you were lucky and have sampled all-inlier sample, meaning that all correspondences used to estimate the model were correct, you would have a correct model.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Reality is more complicated than I have just described, but the principle is the same. 
The most important part is the sampling and it is sensitive to inlier ratio $\nu$ - the percentage of the correct correspondences in the set. Lets denote the minimal number of correspondences required to estimate the model as &lt;strong&gt;m&lt;/strong&gt;.
To recover the correct model with the confidence &lt;strong&gt;p&lt;/strong&gt; one needs to sample the number of correspondences, which is described by formula:&lt;/p&gt;
\begin{equation}
N = \frac{\log{(1 - p)}}{\log{(1 - \nu^{m})}}
\end{equation}&lt;p&gt;Lets plot the how the number of required samples changes with inlier ratio for confidence equal 99%. Note the log scale on Y axis. Different lines are for different minimal sample size &lt;strong&gt;m&lt;/strong&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/att_00016.png&quot; alt=&quot;&quot; title=&quot;Number of samples to find correct model as a function of inlier ratio.&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;As you can see from the plot above, reducing the minimal sample size required for the model estimation even by 1 saves and order of magnitude of computation. 
In reality the benefit is a smaller, as modern RANSACs like GC-RANSAC[&lt;a class=&quot;latex_cit&quot; id=&quot;call-gcransac2018&quot; href=&quot;#cit-gcransac2018&quot;&gt;gcransac2018&lt;/a&gt;] and MAGSAC[&lt;a class=&quot;latex_cit&quot; id=&quot;call-magsac2019&quot; href=&quot;#cit-magsac2019&quot;&gt;magsac2019&lt;/a&gt;] could estimate the correct model from the sample containing outliers, but it is still huge, especially for low inlier rate cases.&lt;/p&gt;
&lt;h3 id=&quot;Image-retrieval&quot;&gt;Image retrieval&lt;a class=&quot;anchor-link&quot; href=&quot;#Image-retrieval&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The ideal case would be to estimate a model from just a single sample and that is exactly what is done in spatial reranking paper &quot;&lt;strong&gt;Object retrieval with large vocabularies and fast spatial matching&lt;/strong&gt;&quot;  by Philbin et.al [&lt;a class=&quot;latex_cit&quot; id=&quot;call-Philbin07&quot; href=&quot;#cit-Philbin07&quot;&gt;Philbin07&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;Specifically, they are solving a particular object retrieval problem: given an image containing some object, return all the images from the database, which also containg the same object.&lt;/p&gt;
&lt;p&gt;The inital list of images is formed by the descriptor distance and then is reranked.
The authors propose to approximate a perspective change between two images as an affine image transformation, and count number of feature points, which are reprojected inside the second image. This number produces better ranking that the original short-list.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/att_00012.png&quot; alt=&quot;&quot; title=&quot;Figure from  Philbin et.al&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Back-to-wide-baseline-stereo&quot;&gt;Back to wide baseline stereo&lt;a class=&quot;anchor-link&quot; href=&quot;#Back-to-wide-baseline-stereo&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;While working for spatial re-ranking, 3-degrees of freedom camera model is too rough for the wide baseline stereo.
Yet, going from 4 point correspondences (PC) to 2 affine correspondences (AC) for homogaphy and from 7 PC to 3 AC for the fundamental matrix would be huge benefit anyway for the robust model estimation.&lt;/p&gt;
&lt;p&gt;Various variant of RANSAC working for local features were proposed in the last 15 years: Perdoch et.al[&lt;a class=&quot;latex_cit&quot; id=&quot;call-perd2006epipolar&quot; href=&quot;#cit-perd2006epipolar&quot;&gt;perd2006epipolar&lt;/a&gt;], Pritts et.al.[&lt;a class=&quot;latex_cit&quot; id=&quot;call-PrittsRANSAC2013&quot; href=&quot;#cit-PrittsRANSAC2013&quot;&gt;PrittsRANSAC2013&lt;/a&gt;], Barath and Kukelova [&lt;a class=&quot;latex_cit&quot; id=&quot;call-Barath2019ICCV&quot; href=&quot;#cit-Barath2019ICCV&quot;&gt;Barath2019ICCV&lt;/a&gt;], Rodr√≠guez et.al[&lt;a class=&quot;latex_cit&quot; id=&quot;call-RANSACAffine2020&quot; href=&quot;#cit-RANSACAffine2020&quot;&gt;RANSACAffine2020&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;Finally, the systematic study of using is presented by Barath et.al[&lt;a class=&quot;latex_cit&quot; id=&quot;call-barath2020making&quot; href=&quot;#cit-barath2020making&quot;&gt;barath2020making&lt;/a&gt;] in &quot;Making Affine Correspondences Work in Camera Geometry Computation&quot; paper. Authors show that if used naively, affine correspondence lead to worse results, because they are more noisy than point correspondences. 
However, there is a bag of tricks presented in the paper, which allow to solve the noise issue and make the affine RANSAC working in practice, resulting in orders of magnitude faster computation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/att_00013.png&quot; alt=&quot;&quot; title=&quot;Figure from Making Affine Correspondences Work in Camera Geometry Computation&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Moreover, for a special cases like autonomous driving, where the motion is mostly horizonal, one could even use 2 affine correspondes for both motion estimation and consistency check, significantly improving the efficiency of the outliers removal compared to the standard RANSAC loop[&lt;a class=&quot;latex_cit&quot; id=&quot;call-guan2020relative&quot; href=&quot;#cit-guan2020relative&quot;&gt;guan2020relative&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/att_00001.png&quot; alt=&quot;&quot; title=&quot;Empirical cumulative error distributions for KITTI sequence 00. Figure from Relative Pose Estimation for Multi-Camera Systems from Affine Correspondences.&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Besides the special case considerations, additional contraints can also come from running other algorithms, like monocular depth estimation. Such a constraint could reduce the required number of matches from two affine correspondences to a single one for calibrated camera case[&lt;a class=&quot;latex_cit&quot; id=&quot;call-OneACMonoDepth2020&quot; href=&quot;#cit-OneACMonoDepth2020&quot;&gt;OneACMonoDepth2020&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/att_00002.png&quot; alt=&quot;&quot; title=&quot;Figure from Relative Pose from Deep Learned Depth and a Single Affine Correspondence&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/att_00003.png&quot; alt=&quot;&quot; title=&quot;Figure from Relative Pose from Deep Learned Depth and a Single Affine Correspondence&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Application-specific-benefits&quot;&gt;Application-specific benefits&lt;a class=&quot;anchor-link&quot; href=&quot;#Application-specific-benefits&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Besides the wide baseline stereo, local affine features and correspondences have other applications. I will briefly describe some of them here (to be updated).&lt;/p&gt;
&lt;h3 id=&quot;Image-rectification&quot;&gt;Image rectification&lt;a class=&quot;anchor-link&quot; href=&quot;#Image-rectification&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Instead of matching local features between two images one might match them within a single image. Why would someone do it? This allows finding repeated pattern: think about windows, doors and so on. Typically they have the same physical size, therefore the diffrence in local features around them could tell us about the geometry of the scene and lens distortion.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/att_00008.png&quot; alt=&quot;&quot; title=&quot;Repeated patterns detection with MSER andRootSIFT local features. Figure from the Minimal Solvers for Rectifying from Radially-Distorted Conjugate Translations, PAMI 2020 paper.&quot; /&gt;&lt;/p&gt;
&lt;p&gt;This is the idea of the series of works by Pritts and Chum.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/att_00007.png&quot; alt=&quot;&quot; title=&quot;Figure from the &amp;#39;Minimal Solvers for Rectifying from Radially-Distorted Conjugate Translations&amp;#39;, PAMI 2020 paper.&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;Surface-normals-estimation&quot;&gt;Surface normals estimation&lt;a class=&quot;anchor-link&quot; href=&quot;#Surface-normals-estimation&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Ivan Eichhardt and Levente Hajder have a series of works, exploiting the local affine correspondences for surface normals estimation[&lt;a class=&quot;latex_cit&quot; id=&quot;call-SurfaceNormals2019&quot; href=&quot;#cit-SurfaceNormals2019&quot;&gt;SurfaceNormals2019&lt;/a&gt;]&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/att_00015.png&quot; alt=&quot;&quot; title=&quot;Estimated surface normals. Figure from Optimal Multi-View Surface Normal Estimation Using Affine Correspondences&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Summary&quot;&gt;Summary&lt;a class=&quot;anchor-link&quot; href=&quot;#Summary&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Despite not being popular right now, treating keypoints as local affine features has a lot of advantages over the traditional treatment the local correspondence as the point correspondences. In the next post I will describe a way of estimating the local feature affine shape and orientation.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;References&quot;&gt;References&lt;a class=&quot;anchor-link&quot; href=&quot;#References&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;[&lt;a id=&quot;cit-SuperPoint2017&quot; href=&quot;#call-SuperPoint2017&quot;&gt;SuperPoint2017&lt;/a&gt;] Detone D., Malisiewicz T. and Rabinovich A., ``&lt;em&gt;Superpoint: Self-Supervised Interest Point Detection and Description&lt;/em&gt;'', CVPRW Deep Learning for Visual SLAM, vol. , number , pp. ,  2018.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Lowe99&quot; href=&quot;#call-Lowe99&quot;&gt;Lowe99&lt;/a&gt;] D. Lowe, ``&lt;em&gt;Object Recognition from Local Scale-Invariant Features&lt;/em&gt;'', ICCV,  1999.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Harris88&quot; href=&quot;#call-Harris88&quot;&gt;Harris88&lt;/a&gt;] C. Harris and M. Stephens, ``&lt;em&gt;A Combined Corner and Edge Detector&lt;/em&gt;'', Fourth Alvey Vision Conference,  1988.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Hessian78&quot; href=&quot;#call-Hessian78&quot;&gt;Hessian78&lt;/a&gt;] P.R. Beaudet, ``&lt;em&gt;Rotationally invariant image operators&lt;/em&gt;'', Proceedings of the 4th International Joint Conference on Pattern Recognition,  1978.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-KeyNet2019&quot; href=&quot;#call-KeyNet2019&quot;&gt;KeyNet2019&lt;/a&gt;] A. Barroso-Laguna, E. Riba, D. Ponsa &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Key.Net: Keypoint Detection by Handcrafted and Learned CNN Filters&lt;/em&gt;'', ICCV,  2019.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-ORB2011&quot; href=&quot;#call-ORB2011&quot;&gt;ORB2011&lt;/a&gt;] E. Rublee, V. Rabaud, K. Konolidge &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;ORB: An Efficient Alternative to SIFT or SURF&lt;/em&gt;'', ICCV,  2011.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-HardNet2017&quot; href=&quot;#call-HardNet2017&quot;&gt;HardNet2017&lt;/a&gt;] A. Mishchuk, D. Mishkin, F. Radenovic &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Working Hard to Know Your Neighbor's Margins: Local Descriptor Learning Loss&lt;/em&gt;'', NeurIPS,  2017.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-RIFT2005&quot; href=&quot;#call-RIFT2005&quot;&gt;RIFT2005&lt;/a&gt;] {Lazebnik} S., {Schmid} C. and {Ponce} J., ``&lt;em&gt;A sparse texture representation using local affine regions&lt;/em&gt;'', IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 27, number 8, pp. 1265-1278,  2005.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-sGLOH2&quot; href=&quot;#call-sGLOH2&quot;&gt;sGLOH2&lt;/a&gt;] {Bellavia} F. and {Colombo} C., ``&lt;em&gt;Rethinking the sGLOH Descriptor&lt;/em&gt;'', IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 40, number 4, pp. 931-944,  2018.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-OriNet2016&quot; href=&quot;#call-OriNet2016&quot;&gt;OriNet2016&lt;/a&gt;] K. M., Y. Verdie, P. Fua &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Learning to Assign Orientations to Feature Points&lt;/em&gt;'', CVPR,  2016.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-AffNet2018&quot; href=&quot;#call-AffNet2018&quot;&gt;AffNet2018&lt;/a&gt;] D. Mishkin, F. Radenovic and J. Matas, ``&lt;em&gt;Repeatability is Not Enough: Learning Affine Regions via Discriminability&lt;/em&gt;'', ECCV,  2018.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-PerdochRetrieval2009&quot; href=&quot;#call-PerdochRetrieval2009&quot;&gt;PerdochRetrieval2009&lt;/a&gt;] M. {Perd'och}, O. {Chum} and J. {Matas}, ``&lt;em&gt;Efficient representation of local geometry for large scale object retrieval&lt;/em&gt;'', CVPR,  2009.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-IMW2020&quot; href=&quot;#call-IMW2020&quot;&gt;IMW2020&lt;/a&gt;] Jin Yuhe, Mishkin Dmytro, Mishchuk Anastasiia &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Image Matching across Wide Baselines: From Paper to Practice&lt;/em&gt;'', arXiv preprint arXiv:2003.01587, vol. , number , pp. ,  2020.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-gcransac2018&quot; href=&quot;#call-gcransac2018&quot;&gt;gcransac2018&lt;/a&gt;] D. Barath and J. Matas, ``&lt;em&gt;Graph-Cut RANSAC&lt;/em&gt;'', The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),  2018.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-magsac2019&quot; href=&quot;#call-magsac2019&quot;&gt;magsac2019&lt;/a&gt;] J.N. Daniel Barath, ``&lt;em&gt;MAGSAC: marginalizing sample consensus&lt;/em&gt;'', CVPR,  2019.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Philbin07&quot; href=&quot;#call-Philbin07&quot;&gt;Philbin07&lt;/a&gt;] J. Philbin, O. Chum, M. Isard &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Object Retrieval with Large Vocabularies and Fast Spatial Matching&lt;/em&gt;'', CVPR,  2007.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-perd2006epipolar&quot; href=&quot;#call-perd2006epipolar&quot;&gt;perd2006epipolar&lt;/a&gt;] M. Perd'och, J. Matas and O. Chum, ``&lt;em&gt;Epipolar geometry from two correspondences&lt;/em&gt;'', ICPR,  2006.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-PrittsRANSAC2013&quot; href=&quot;#call-PrittsRANSAC2013&quot;&gt;PrittsRANSAC2013&lt;/a&gt;] J. {Pritts}, O. {Chum} and J. {Matas}, ``&lt;em&gt;Approximate models for fast and accurate epipolar geometry estimation&lt;/em&gt;'', 2013 28th International Conference on Image and Vision Computing New Zealand (IVCNZ 2013),  2013.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Barath2019ICCV&quot; href=&quot;#call-Barath2019ICCV&quot;&gt;Barath2019ICCV&lt;/a&gt;] D. Barath and Z. Kukelova, ``&lt;em&gt;Homography From Two Orientation- and Scale-Covariant Features&lt;/em&gt;'', ICCV,  2019.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-RANSACAffine2020&quot; href=&quot;#call-RANSACAffine2020&quot;&gt;RANSACAffine2020&lt;/a&gt;] M. {Rodr√≠guez}, G. {Facciolo}, R. G. &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Robust estimation of local affine maps and its applications to image matching&lt;/em&gt;'', 2020 IEEE Winter Conference on Applications of Computer Vision (WACV),  2020.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-barath2020making&quot; href=&quot;#call-barath2020making&quot;&gt;barath2020making&lt;/a&gt;] Barath Daniel, Polic Michal, F√∂rstner Wolfgang &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Making Affine Correspondences Work in Camera Geometry Computation&lt;/em&gt;'', arXiv preprint arXiv:2007.10032, vol. , number , pp. ,  2020.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-guan2020relative&quot; href=&quot;#call-guan2020relative&quot;&gt;guan2020relative&lt;/a&gt;] Guan Banglei, Zhao Ji, Barath Daniel &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Relative Pose Estimation for Multi-Camera Systems from Affine Correspondences&lt;/em&gt;'', arXiv preprint arXiv:2007.10700, vol. , number , pp. ,  2020.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-OneACMonoDepth2020&quot; href=&quot;#call-OneACMonoDepth2020&quot;&gt;OneACMonoDepth2020&lt;/a&gt;] D.B. Ivan Eichhardt, ``&lt;em&gt;Relative Pose from Deep Learned Depth and a Single Affine Correspondence&lt;/em&gt;'', ECCV,  2020.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-SurfaceNormals2019&quot; href=&quot;#call-SurfaceNormals2019&quot;&gt;SurfaceNormals2019&lt;/a&gt;] {Bar√°th} D., {Eichhardt} I. and {Hajder} L., ``&lt;em&gt;Optimal Multi-View Surface Normal Estimation Using Affine Correspondences&lt;/em&gt;'', IEEE Transactions on Image Processing, vol. 28, number 7, pp. 3301-3311,  2019.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/wide-baseline-stereo-blog/images/affine_matches.png" /><media:content medium="image" url="/wide-baseline-stereo-blog/images/affine_matches.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">WxBS: Wide Multiple Baseline Stereo as a task</title><link href="/wide-baseline-stereo-blog/2020/07/09/wxbs.html" rel="alternate" type="text/html" title="WxBS: Wide Multiple Baseline Stereo as a task" /><published>2020-07-09T00:00:00-05:00</published><updated>2020-07-09T00:00:00-05:00</updated><id>/wide-baseline-stereo-blog/2020/07/09/wxbs</id><content type="html" xml:base="/wide-baseline-stereo-blog/2020/07/09/wxbs.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-07-09-wxbs.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Definition-of-WxBS&quot;&gt;Definition of WxBS&lt;a class=&quot;anchor-link&quot; href=&quot;#Definition-of-WxBS&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Let us denote observations $O_{i}, i=1..n$, each of which belongs to one of the views $V_{j}, i=1..m$, $m \leq n$. 
Observations can be, for example, pixels and views are images, respectfully. Observations and views can be of different nature and dimentionality. E.g. $V_1$, $V_2$  - RGB images, $V_3$ - point cloud from a laser scaner, $V_4$ - image from a thermal camera, and so on.&lt;/p&gt;
&lt;p&gt;We will call two of observations $(O_{i},O_{k})$, a correspondence $c_{ik}$ if they are belong to different views $V_{j}$. The group of observations is called correspondence set $C_o$, when there is exactly one observation $O_i$ per view $V_j$. Some of observations $O_i$ can be empty $\varnothing$, i.e. not observed in the specific view $V_j$. Multiple correspondences are called &lt;strong&gt;consistent&lt;/strong&gt; if they form a correspondence set.&lt;/p&gt;
&lt;p&gt;We can now define a wide baseline stereo.&lt;/p&gt;
&lt;p&gt;By &lt;strong&gt;wide baseline stereo&lt;/strong&gt; we understand the process of establishing correspondence sets $C_o$ from observations $O_i$ and images $V_{j}$ under the following constraints:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;images $V_j$ belong to camera planes taken by cameras $K_j$ of the same static rigid scene $S$. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition to recovering the correspondence sets wide baseline stereo process also recovers (unknown) camera poses $K_j$.&lt;/p&gt;
&lt;p&gt;By &quot;rigid&quot; we mean that the only &quot;motion&quot; possible is the pose difference of the cameras, which is called &quot;baseline&quot; for the case where there are only two cameras.&lt;/p&gt;
&lt;p&gt;We could also assume some scene structure or model, consisting of latent objects $X_i$, which we are could only inderectly observe by observations $O_i$.&lt;/p&gt;
&lt;p&gt;For example, on image below, observations $O_i$ are blue circles and the correspondences $c_{jk}$ are shown as lines. The assumed object $X_i$ is a red circle.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/imgs/WxBS_house.jpeg&quot; alt=&quot;&quot; title=&quot;Example of multimodal wide baseline stereo&quot; /&gt;&lt;/p&gt;
&lt;p&gt;We will call &quot;&lt;strong&gt;wide multiple baseline stereo&lt;/strong&gt;&quot; or &lt;strong&gt;WxBS&lt;/strong&gt; [&lt;a class=&quot;latex_cit&quot; id=&quot;call-Mishkin2015WXBS&quot; href=&quot;#cit-Mishkin2015WXBS&quot;&gt;Mishkin2015WXBS&lt;/a&gt;] if the observations have different nature or the conditions under which observation were made are different.&lt;/p&gt;
&lt;p&gt;The different between &lt;strong&gt;wide baseline stereo&lt;/strong&gt; and &lt;strong&gt;short baseline stereo&lt;/strong&gt;, or, simply &lt;strong&gt;stereo&lt;/strong&gt; is the follwing. In &lt;strong&gt;stereo&lt;/strong&gt; the baseline is small -- less then 1 meter -- and typically known and fixed. The task is to establish correspondences, which can be done by 1D search along the known epipolar lines.&lt;/p&gt;
&lt;p&gt;In contrast, in  &lt;strong&gt;wide baseline stereo&lt;/strong&gt; the baseline is unknown, mostly unconstrained and the viewpoints of the cameras can vary drastically.&lt;/p&gt;
&lt;p&gt;The wide baseline stereo, which also outputs the estimation of the latent objects, e.g. in form of 3d point world coordinates we would call &lt;strong&gt;rigid structure-from-motion&lt;/strong&gt; (rigid SfM) or &lt;strong&gt;3D reconstruction&lt;/strong&gt;. We do not consider object shape approximation with voxels, meshes, etc in the current thesis. Nor we consider the recovery of scene albedo, illumination, and other appearance properties.&lt;/p&gt;
&lt;p&gt;While the difference between &lt;strong&gt;SfM&lt;/strong&gt; and &lt;strong&gt;WBS&lt;/strong&gt; is often blurred and the terms are used interchangeably, we would  consider WBS as a part of SfM pipeline prior to recovering 3d point cloud.&lt;/p&gt;
&lt;p&gt;Other correspondence problems, as tracking, optical flow or establishing semantic correspondences could be defined using the terminilogy we established.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/imgs/wxbs_problems.png&quot; alt=&quot;&quot; title=&quot;Example of  WxBS problems&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;References&quot;&gt;References&lt;a class=&quot;anchor-link&quot; href=&quot;#References&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;(&lt;a id=&quot;cit-Mishkin2015WXBS&quot; href=&quot;#call-Mishkin2015WXBS&quot;&gt;Mishkin, Matas &lt;em&gt;et al.&lt;/em&gt;, 2015&lt;/a&gt;) D. Mishkin, J. Matas, M. Perdoch &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;WxBS: Wide Baseline Stereo Generalizations&lt;/em&gt;'', BMVC,  2015.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/wide-baseline-stereo-blog/images/wxbs_problems400.png" /><media:content medium="image" url="/wide-baseline-stereo-blog/images/wxbs_problems400.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The Role of Wide Baseline Stereo in the Deep Learning World</title><link href="/wide-baseline-stereo-blog/2020/03/27/intro.html" rel="alternate" type="text/html" title="The Role of Wide Baseline Stereo in the Deep Learning World" /><published>2020-03-27T00:00:00-05:00</published><updated>2020-03-27T00:00:00-05:00</updated><id>/wide-baseline-stereo-blog/2020/03/27/intro</id><content type="html" xml:base="/wide-baseline-stereo-blog/2020/03/27/intro.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-03-27-intro.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Rise-of-Wide-Baseline-Stereo&quot;&gt;Rise of Wide Baseline Stereo&lt;a class=&quot;anchor-link&quot; href=&quot;#Rise-of-Wide-Baseline-Stereo&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The wide baseline stereo (WBS) is a process of establishing correspondences between pixels and/or regions between
images depicting the same object or scene and estimation geometric relationship between the cameras, which produced that images.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/00_intro_files/match_doll.png&quot; alt=&quot;&quot; title=&quot;Correspondences between two views found by wide baseline stereo algorithm. Photo and doll created by Olha Mishkina&quot; /&gt;&lt;/p&gt;
&lt;!--- ![Wide baseline stereo model. &quot;Baseline&quot; is the distance between cameras. Image by Arne Nordmann (WikiMedia)](00_intro_files/Epipolar_geometry.svg) 
--&gt;

&lt;p&gt;One of the first succesful solutions for the WBS problem was proposed by Schmid and Mohr [&lt;a class=&quot;latex_cit&quot; id=&quot;call-Schmid1995&quot; href=&quot;#cit-Schmid1995&quot;&gt;Schmid1995&lt;/a&gt;] in 1995.
It was later extended by Beardsley, Torr and Zisserman[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Beardsley96&quot; href=&quot;#cit-Beardsley96&quot;&gt;Beardsley96&lt;/a&gt;] by adding RANSAC robust geometry estimation and later refined by Pritchett and Zisserman [&lt;a class=&quot;latex_cit&quot; id=&quot;call-Pritchett1998&quot; href=&quot;#cit-Pritchett1998&quot;&gt;Pritchett1998&lt;/a&gt;,&lt;a class=&quot;latex_cit&quot; id=&quot;call-Pritchett1998b&quot; href=&quot;#cit-Pritchett1998b&quot;&gt;Pritchett1998b&lt;/a&gt;] in 1998. The general pipeline remains mostly the same until now [&lt;a class=&quot;latex_cit&quot; id=&quot;call-WBSTorr99&quot; href=&quot;#cit-WBSTorr99&quot;&gt;WBSTorr99&lt;/a&gt;,&lt;a class=&quot;latex_cit&quot; id=&quot;call-CsurkaReview2018&quot; href=&quot;#cit-CsurkaReview2018&quot;&gt;CsurkaReview2018&lt;/a&gt;]. The currently adopted version of the wide baseline stereo algorithm is shown below.&lt;/p&gt;
&lt;!--- 
![image.png](/wide-baseline-stereo-blog/images/copied_from_nb/00_intro_files/att_00002.png)
--&gt;


&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/00_intro_files/matching-filtering.png&quot; alt=&quot;&quot; title=&quot;Commonly used wide baseline stereo pipeline&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The algorithm can be summarized as the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Compute interest points/regions in all images independently&lt;/li&gt;
&lt;li&gt;For each interest point/region compute a descriptor of their neigborhood (local patch).&lt;/li&gt;
&lt;li&gt;Establish tentative correspondences between interest points based on their descriptors.&lt;/li&gt;
&lt;li&gt;Robustly estimate geometric relation between two images based on tentative correspondences with RANSAC.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The reason of steps 1 and 2 done on the both images separately is that in general wide baseline stereo is not limited to pairs of images, but rather to a collections of them. If all the steps are done pairwise, then the computational complexity is $O(n^2)$. The more steps done seperately - the more efficient algorithm is.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Quick-expansion&quot;&gt;Quick expansion&lt;a class=&quot;anchor-link&quot; href=&quot;#Quick-expansion&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;This algorithm significantly changed computer vision landscape for next forteen years.&lt;/p&gt;
&lt;p&gt;Soon after introducing the algorithm, there it become clear that its quality significantly depends on quality of each component, i.e. local feature detector, descriptor, and geometry estimation. Pleora of new detectors and descriptors were proposed, with the most cited computer vision paper ever SIFT local feature[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Lowe99&quot; href=&quot;#cit-Lowe99&quot;&gt;Lowe99&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;It is worth noting, that SIFT became popular only after Mikolajczyk benchmark paper [&lt;a class=&quot;latex_cit&quot; id=&quot;call-MikoDescEval2003&quot; href=&quot;#cit-MikoDescEval2003&quot;&gt;MikoDescEval2003&lt;/a&gt;,&lt;a class=&quot;latex_cit&quot; id=&quot;call-Mikolajczyk05&quot; href=&quot;#cit-Mikolajczyk05&quot;&gt;Mikolajczyk05&lt;/a&gt;], showed it superiority to the rest of alternatives.&lt;/p&gt;
&lt;p&gt;Robust geometry estimation was also a hot topic: a lot of improvements over vanilla RANSAC were proposed: LO-RANSAC[&lt;a class=&quot;latex_cit&quot; id=&quot;call-LOransac2003&quot; href=&quot;#cit-LOransac2003&quot;&gt;LOransac2003&lt;/a&gt;], DEGENSAC[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Degensac2005&quot; href=&quot;#cit-Degensac2005&quot;&gt;Degensac2005&lt;/a&gt;], MLESAC[&lt;a class=&quot;latex_cit&quot; id=&quot;call-MLESAC00&quot; href=&quot;#cit-MLESAC00&quot;&gt;MLESAC00&lt;/a&gt;]&lt;/p&gt;
&lt;p&gt;Success of wide baseline stereo with SIFT features led to aplication of its components to other computer vision tasks, which were reformulated through wide baseline stereo lens:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Scalable image search&lt;/strong&gt;. Sivic and Zisserman in famous &quot;Video Google&quot; paper[&lt;a class=&quot;latex_cit&quot; id=&quot;call-VideoGoogle2003&quot; href=&quot;#cit-VideoGoogle2003&quot;&gt;VideoGoogle2003&lt;/a&gt;] proposed to treat local features as &quot;visual words&quot; and use ideas from text processing for searching in image collections.  Later even more WBS elements were re-introduced to image search, most notable -- &lt;strong&gt;spatial verification&lt;/strong&gt;[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Philbin07&quot; href=&quot;#cit-Philbin07&quot;&gt;Philbin07&lt;/a&gt;]: simplified RANSAC procedure to verify if visual word matches were spatially consistent.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/00_intro_files/att_00004.png&quot; alt=&quot;&quot; title=&quot;Bag of words image search. Image credit: Filip Radenovic http://cmp.felk.cvut.cz/~radenfil/publications/Radenovic-CMPcolloq-2015.11.12.pdf&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Image classification&lt;/strong&gt; was performed by placing some classifier (SVM, random forest, etc) on top of some encoding of the SIFT-like descriptors, extracted sparsely[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Fergus03&quot; href=&quot;#cit-Fergus03&quot;&gt;Fergus03&lt;/a&gt;,&lt;a class=&quot;latex_cit&quot; id=&quot;call-CsurkaBoK2004&quot; href=&quot;#cit-CsurkaBoK2004&quot;&gt;CsurkaBoK2004&lt;/a&gt;] or densely[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Lazebnik06&quot; href=&quot;#cit-Lazebnik06&quot;&gt;Lazebnik06&lt;/a&gt;]. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/00_intro_files/att_00005.png&quot; alt=&quot;&quot; title=&quot;Bag of local features representation for classification from Fergus03&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Object detection&lt;/strong&gt; was formulated as relaxed wide baseline stereo problem[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Chum2007Exemplar&quot; href=&quot;#cit-Chum2007Exemplar&quot;&gt;Chum2007Exemplar&lt;/a&gt;] or as classification of SIFT-like features inside a sliding window [&lt;a class=&quot;latex_cit&quot; id=&quot;call-HoG2005&quot; href=&quot;#cit-HoG2005&quot;&gt;HoG2005&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/00_intro_files/att_00003.png&quot; alt=&quot;&quot; title=&quot;Exemplar-representation of the classes using local features, cite{Chum2007Exemplar}&quot; /&gt;&lt;/p&gt;
&lt;!--- 
![HoG-based pedestrian detection algorithm](/wide-baseline-stereo-blog/images/copied_from_nb/00_intro_files/att_00006.png)
![Histogram of gradient visualization](/wide-baseline-stereo-blog/images/copied_from_nb/00_intro_files/att_00007.png)
--&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Semantic segmentation&lt;/strong&gt; was performed by classicication of local region descriptors, typically, SIFT and color features and postprocessing afterwards[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Superparsing2010&quot; href=&quot;#cit-Superparsing2010&quot;&gt;Superparsing2010&lt;/a&gt;]. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of course,wide  baseline stereo was also used for its direct applications:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;3D reconstruction&lt;/strong&gt; was based on camera poses and 3D points, estimated with help of SIFT features [&lt;a class=&quot;latex_cit&quot; id=&quot;call-PhotoTourism2006&quot; href=&quot;#cit-PhotoTourism2006&quot;&gt;PhotoTourism2006&lt;/a&gt;,&lt;a class=&quot;latex_cit&quot; id=&quot;call-RomeInDay2009&quot; href=&quot;#cit-RomeInDay2009&quot;&gt;RomeInDay2009&lt;/a&gt;,&lt;a class=&quot;latex_cit&quot; id=&quot;call-COLMAP2016&quot; href=&quot;#cit-COLMAP2016&quot;&gt;COLMAP2016&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/00_intro_files/att_00008.png&quot; alt=&quot;&quot; title=&quot;SfM pipeline from COLMAP&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;SLAM(Simultaneous localization and mapping)&lt;/strong&gt; [&lt;a class=&quot;latex_cit&quot; id=&quot;call-Se02&quot; href=&quot;#cit-Se02&quot;&gt;Se02&lt;/a&gt;,&lt;a class=&quot;latex_cit&quot; id=&quot;call-PTAM2007&quot; href=&quot;#cit-PTAM2007&quot;&gt;PTAM2007&lt;/a&gt;,&lt;a class=&quot;latex_cit&quot; id=&quot;call-Mur15&quot; href=&quot;#cit-Mur15&quot;&gt;Mur15&lt;/a&gt;] were based on fast version of local feature detectors and descriptors.&lt;/p&gt;
&lt;!--- 
![ORBSLAM pipeline](/wide-baseline-stereo-blog/images/copied_from_nb/00_intro_files/att_00009.png)
--&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Panorama stiching&lt;/strong&gt; [&lt;a class=&quot;latex_cit&quot; id=&quot;call-Brown07&quot; href=&quot;#cit-Brown07&quot;&gt;Brown07&lt;/a&gt;] and, more generally, &lt;strong&gt;feature-based image registration&lt;/strong&gt;[&lt;a class=&quot;latex_cit&quot; id=&quot;call-DualBootstrap2003&quot; href=&quot;#cit-DualBootstrap2003&quot;&gt;DualBootstrap2003&lt;/a&gt;] were initalized with a geometry obtained by WBS and then further optimized&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Deep-Learning-Invasion:-retreal-to-the-geometrical-fortress&quot;&gt;Deep Learning Invasion: retreal to the geometrical fortress&lt;a class=&quot;anchor-link&quot; href=&quot;#Deep-Learning-Invasion:-retreal-to-the-geometrical-fortress&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In 2012 deep learning-based AlexNet[&lt;a class=&quot;latex_cit&quot; id=&quot;call-AlexNet2012&quot; href=&quot;#cit-AlexNet2012&quot;&gt;AlexNet2012&lt;/a&gt;] approach beat all the methods in image classification. Soon after, Razavian et.al[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Astounding2014&quot; href=&quot;#cit-Astounding2014&quot;&gt;Astounding2014&lt;/a&gt;] have shown that convolutional neural networks (CNNs) pre-trained on the Imagenet outperform more complex traditional solutions in image and scene classification, object detection and image search.
Deep learning solutions, be it pretrained or end-to-end learned networks quickly become the default solution for the most of computer vision problems.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/00_intro_files/att_00010.png&quot; alt=&quot;&quot; title=&quot;CNN representation beats complex traditional pipelines. Reds are CNN-based and greens are the handcrafted. From Astounding2014&quot; /&gt;&lt;/p&gt;
&lt;p&gt;However, there was still an area, where deep learned solutions failed, sometimes spectacularly: geometry-related tasks. Wide baseline stereo[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Melekhov2017relativePoseCnn&quot; href=&quot;#cit-Melekhov2017relativePoseCnn&quot;&gt;Melekhov2017relativePoseCnn&lt;/a&gt;], visual localization[&lt;a class=&quot;latex_cit&quot; id=&quot;call-PoseNet2015&quot; href=&quot;#cit-PoseNet2015&quot;&gt;PoseNet2015&lt;/a&gt;]}, SLAM are still areas, where the classical wide baseline stereo dominates[&lt;a class=&quot;latex_cit&quot; id=&quot;call-sattler2019understanding&quot; href=&quot;#cit-sattler2019understanding&quot;&gt;sattler2019understanding&lt;/a&gt;,&lt;a class=&quot;latex_cit&quot; id=&quot;call-zhou2019learn&quot; href=&quot;#cit-zhou2019learn&quot;&gt;zhou2019learn&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;The full reasons why convolution pipelines are failing for geometrical tasks are yet to understand, but the current hypothesis are the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CNN-based pose predictions predictions are roughly equivalent to retrieval of most similar image from the training set and outputing its pose.[&lt;a class=&quot;latex_cit&quot; id=&quot;call-sattler2019understanding&quot; href=&quot;#cit-sattler2019understanding&quot;&gt;sattler2019understanding&lt;/a&gt;] This phenomenum is also observed in related area: single-view 3D reconstruction[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Tatarchenko2019&quot; href=&quot;#cit-Tatarchenko2019&quot;&gt;Tatarchenko2019&lt;/a&gt;].&lt;/li&gt;
&lt;li&gt;Geometric and arithmetic operations are hard to represent via vanilla neural networks (i.e. matrix multiplication with non-linearity) and they may require specialized building blocks, resembling operations of algorithmic or geometric methods, e.g. spatial transformers[&lt;a class=&quot;latex_cit&quot; id=&quot;call-STN2015&quot; href=&quot;#cit-STN2015&quot;&gt;STN2015&lt;/a&gt;] and arithmetic units[&lt;a class=&quot;latex_cit&quot; id=&quot;call-NALU2018&quot; href=&quot;#cit-NALU2018&quot;&gt;NALU2018&lt;/a&gt;,&lt;a class=&quot;latex_cit&quot; id=&quot;call-NAU2020&quot; href=&quot;#cit-NAU2020&quot;&gt;NAU2020&lt;/a&gt;]. Even with special structure such networks require &quot;careful initialization, restricting parameter space, and regularizing for sparsity&quot;[&lt;a class=&quot;latex_cit&quot; id=&quot;call-NAU2020&quot; href=&quot;#cit-NAU2020&quot;&gt;NAU2020&lt;/a&gt;].&lt;/li&gt;
&lt;li&gt;Vanilla CNNs are not covariant to even simple geometric transformation like translation [&lt;a class=&quot;latex_cit&quot; id=&quot;call-MakeCNNShiftInvariant2019&quot; href=&quot;#cit-MakeCNNShiftInvariant2019&quot;&gt;MakeCNNShiftInvariant2019&lt;/a&gt;], scaling and especially rotation [&lt;a class=&quot;latex_cit&quot; id=&quot;call-GroupEqCNN2016&quot; href=&quot;#cit-GroupEqCNN2016&quot;&gt;GroupEqCNN2016&lt;/a&gt;]. Unlike them, WBS baseline is grounded on scale-space theory [&lt;a class=&quot;latex_cit&quot; id=&quot;call-lindeberg2013scale&quot; href=&quot;#cit-lindeberg2013scale&quot;&gt;lindeberg2013scale&lt;/a&gt;] and local patches are geometrically normalilzed before description. &lt;/li&gt;
&lt;li&gt;Predictions of the CNNs can be altered by change in a small localized area [&lt;a class=&quot;latex_cit&quot; id=&quot;call-AdvPatch2017&quot; href=&quot;#cit-AdvPatch2017&quot;&gt;AdvPatch2017&lt;/a&gt;] or even single pixel [&lt;a class=&quot;latex_cit&quot; id=&quot;call-OnePixelAttack2019&quot; href=&quot;#cit-OnePixelAttack2019&quot;&gt;OnePixelAttack2019&lt;/a&gt;], while the wide baseline stereo methods require the consensus of different independent regions. &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Today:-assimilation-and-merging&quot;&gt;Today: assimilation and merging&lt;a class=&quot;anchor-link&quot; href=&quot;#Today:-assimilation-and-merging&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;h3 id=&quot;Wide-baseline-stereo-as-a-task:-formulate-differentiably-and-learn-modules&quot;&gt;Wide baseline stereo as a task: formulate differentiably and learn modules&lt;a class=&quot;anchor-link&quot; href=&quot;#Wide-baseline-stereo-as-a-task:-formulate-differentiably-and-learn-modules&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Wide baseline stereo as a task is solved today typically by using learned components as a replacement of specific blocks in WBS algorithm[&lt;a class=&quot;latex_cit&quot; id=&quot;call-jin2020image&quot; href=&quot;#cit-jin2020image&quot;&gt;jin2020image&lt;/a&gt;] ,e.g. local descriptor like HardNet[&lt;a class=&quot;latex_cit&quot; id=&quot;call-HardNet2017&quot; href=&quot;#cit-HardNet2017&quot;&gt;HardNet2017&lt;/a&gt;], detectors like KeyNet[&lt;a class=&quot;latex_cit&quot; id=&quot;call-KeyNet2019&quot; href=&quot;#cit-KeyNet2019&quot;&gt;KeyNet2019&lt;/a&gt;], joint detector-descriptor[&lt;a class=&quot;latex_cit&quot; id=&quot;call-SuperPoint2017&quot; href=&quot;#cit-SuperPoint2017&quot;&gt;SuperPoint2017&lt;/a&gt;] matching and filtering like SuperGlue[&lt;a class=&quot;latex_cit&quot; id=&quot;call-sarlin2019superglue&quot; href=&quot;#cit-sarlin2019superglue&quot;&gt;sarlin2019superglue&lt;/a&gt;], etc. 
There are also attempts to formulate the whole downstream task pipeline like SLAM[&lt;a class=&quot;latex_cit&quot; id=&quot;call-gradslam2020&quot; href=&quot;#cit-gradslam2020&quot;&gt;gradslam2020&lt;/a&gt;] in a differentiable way, combining advantages of structured and learning-based approaches.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/00_intro_files/att_00011.png&quot; alt=&quot;&quot; title=&quot;SuperGlue: separate matching module for handcrafter and learned features&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/00_intro_files/gradslam.png&quot; alt=&quot;&quot; title=&quot;gradSLAM: differentiable formulation of SLAM pipeline&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;Wide-baseline-stereo-as-a-idea:-consensus-of-local-independent-predictions&quot;&gt;Wide baseline stereo as a idea: consensus of local independent predictions&lt;a class=&quot;anchor-link&quot; href=&quot;#Wide-baseline-stereo-as-a-idea:-consensus-of-local-independent-predictions&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;On the other hand, as an algorithm, wide baseline stereo is summarized into two main ideas&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Image should be represented as set of local parts, robust to occlusion, and not influencing each other.&lt;/li&gt;
&lt;li&gt;Decision should be based on spatial consensus of local feature correspondences.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;One of modern revisit of wide baseline stereo ideas is Capsule Networks[&lt;a class=&quot;latex_cit&quot; id=&quot;call-CapsNet2011&quot; href=&quot;#cit-CapsNet2011&quot;&gt;CapsNet2011&lt;/a&gt;,&lt;a class=&quot;latex_cit&quot; id=&quot;call-CapsNet2017&quot; href=&quot;#cit-CapsNet2017&quot;&gt;CapsNet2017&lt;/a&gt;]. Unlike CNNs, they encode not only intensity of feature responce, but also its location and require a geometric agreement between object parts for outputing a confident prediction.&lt;/p&gt;
&lt;p&gt;Similar ideas are now explored for ensuring adversarial robustness of CNNs[&lt;a class=&quot;latex_cit&quot; id=&quot;call-li2020extreme&quot; href=&quot;#cit-li2020extreme&quot;&gt;li2020extreme&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;Another way of using &quot;consensus of local independent predictions&quot; is used in &lt;a href=&quot;https://arxiv.org/abs/2007.11498&quot;&gt;Cross-transformers&lt;/a&gt; paper: spatial attention helps to select relevant feature for few-shot learning, see Figure below.&lt;/p&gt;
&lt;p&gt;While wide baseline stereo is far from the mainstream now, it continues to play an important role in computer vision.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-03-27-intro_files/att_00000.png&quot; alt=&quot;&quot; title=&quot;Cross-transformers: spatial attention helps to select relevant feature for few-shot learning&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/00_intro_files/capsules.png&quot; alt=&quot;&quot; title=&quot;Capsule networks: revisiting the WBS idea. Each feature response is accompanied with its pose. Poses should be in agreement, otherwise object would not be recognized. Image by Aur√©lien G√©ron https://www.oreilly.com/content/introducing-capsule-networks/&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;References&quot;&gt;References&lt;a class=&quot;anchor-link&quot; href=&quot;#References&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;[&lt;a id=&quot;cit-Schmid1995&quot; href=&quot;#call-Schmid1995&quot;&gt;Schmid1995&lt;/a&gt;] Schmid Cordelia and Mohr Roger, ``&lt;em&gt;Matching by local invariants&lt;/em&gt;'', , vol. , number , pp. ,  1995.  &lt;a href=&quot;https://hal.inria.fr/file/index/docid/74046/filename/RR-2644.pdf&quot;&gt;online&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Beardsley96&quot; href=&quot;#call-Beardsley96&quot;&gt;Beardsley96&lt;/a&gt;] P. Beardsley, P. Torr and A. Zisserman, ``&lt;em&gt;3D model acquisition from extended image sequences&lt;/em&gt;'', ECCV,  1996.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Pritchett1998&quot; href=&quot;#call-Pritchett1998&quot;&gt;Pritchett1998&lt;/a&gt;] P. Pritchett and A. Zisserman, ``&lt;em&gt;Wide baseline stereo matching&lt;/em&gt;'', ICCV,  1998.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Pritchett1998b&quot; href=&quot;#call-Pritchett1998b&quot;&gt;Pritchett1998b&lt;/a&gt;] P. Pritchett and A. Zisserman, ``&lt;em&gt;&quot;Matching and Reconstruction from Widely Separated Views&quot;&lt;/em&gt;'', 3D Structure from Multiple Images of Large-Scale Environments,  1998.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-WBSTorr99&quot; href=&quot;#call-WBSTorr99&quot;&gt;WBSTorr99&lt;/a&gt;] P. Torr and A. Zisserman, ``&lt;em&gt;Feature Based Methods for Structure and Motion Estimation&lt;/em&gt;'', Workshop on Vision Algorithms,  1999.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-CsurkaReview2018&quot; href=&quot;#call-CsurkaReview2018&quot;&gt;CsurkaReview2018&lt;/a&gt;] {Csurka} Gabriela, {Dance} Christopher R. and {Humenberger} Martin, ``&lt;em&gt;From handcrafted to deep local features&lt;/em&gt;'', arXiv e-prints, vol. , number , pp. ,  2018.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Lowe99&quot; href=&quot;#call-Lowe99&quot;&gt;Lowe99&lt;/a&gt;] D. Lowe, ``&lt;em&gt;Object Recognition from Local Scale-Invariant Features&lt;/em&gt;'', ICCV,  1999.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-MikoDescEval2003&quot; href=&quot;#call-MikoDescEval2003&quot;&gt;MikoDescEval2003&lt;/a&gt;] K. Mikolajczyk and C. Schmid, ``&lt;em&gt;A Performance Evaluation of Local Descriptors&lt;/em&gt;'', CVPR, June 2003.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Mikolajczyk05&quot; href=&quot;#call-Mikolajczyk05&quot;&gt;Mikolajczyk05&lt;/a&gt;] Mikolajczyk K., Tuytelaars T., Schmid C. &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;A Comparison of Affine Region Detectors&lt;/em&gt;'', IJCV, vol. 65, number 1/2, pp. 43--72,  2005.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-LOransac2003&quot; href=&quot;#call-LOransac2003&quot;&gt;LOransac2003&lt;/a&gt;] O. Chum, J. Matas and J. Kittler, ``&lt;em&gt;Locally Optimized RANSAC&lt;/em&gt;'', Pattern Recognition,  2003.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Degensac2005&quot; href=&quot;#call-Degensac2005&quot;&gt;Degensac2005&lt;/a&gt;] O. Chum, T. Werner and J. Matas, ``&lt;em&gt;Two-View Geometry Estimation Unaffected by a Dominant Plane&lt;/em&gt;'', CVPR,  2005.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-MLESAC00&quot; href=&quot;#call-MLESAC00&quot;&gt;MLESAC00&lt;/a&gt;] Torr P.H.S. and Zisserman A., ``&lt;em&gt;MLESAC: A New Robust Estimator with Application to Estimating Image Geometry&lt;/em&gt;'', CVIU, vol. 78, number , pp. 138--156,  2000.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-VideoGoogle2003&quot; href=&quot;#call-VideoGoogle2003&quot;&gt;VideoGoogle2003&lt;/a&gt;] J. Sivic and A. Zisserman, ``&lt;em&gt;Video Google: A Text Retrieval Approach to Object Matching in Videos&lt;/em&gt;'', ICCV,  2003.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Philbin07&quot; href=&quot;#call-Philbin07&quot;&gt;Philbin07&lt;/a&gt;] J. Philbin, O. Chum, M. Isard &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Object Retrieval with Large Vocabularies and Fast Spatial Matching&lt;/em&gt;'', CVPR,  2007.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Fergus03&quot; href=&quot;#call-Fergus03&quot;&gt;Fergus03&lt;/a&gt;] R. Fergus, P. Perona and A. Zisserman, ``&lt;em&gt;Object Class Recognition by Unsupervised Scale-Invariant Learning&lt;/em&gt;'', CVPR,  2003.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-CsurkaBoK2004&quot; href=&quot;#call-CsurkaBoK2004&quot;&gt;CsurkaBoK2004&lt;/a&gt;] C.D. G. Csurka, J. Willamowski, L. Fan &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Visual Categorization with Bags of Keypoints&lt;/em&gt;'', ECCV,  2004.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Lazebnik06&quot; href=&quot;#call-Lazebnik06&quot;&gt;Lazebnik06&lt;/a&gt;] S. Lazebnik, C. Schmid and J. Ponce, ``&lt;em&gt;Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories&lt;/em&gt;'', CVPR,  2006.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Chum2007Exemplar&quot; href=&quot;#call-Chum2007Exemplar&quot;&gt;Chum2007Exemplar&lt;/a&gt;] O. {Chum} and A. {Zisserman}, ``&lt;em&gt;An Exemplar Model for Learning Object Classes&lt;/em&gt;'', CVPR,  2007.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-HoG2005&quot; href=&quot;#call-HoG2005&quot;&gt;HoG2005&lt;/a&gt;] N. {Dalal} and B. {Triggs}, ``&lt;em&gt;Histograms of oriented gradients for human detection&lt;/em&gt;'', CVPR,  2005.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Superparsing2010&quot; href=&quot;#call-Superparsing2010&quot;&gt;Superparsing2010&lt;/a&gt;] J. Tighe and S. Lazebnik, ``&lt;em&gt;SuperParsing: Scalable Nonparametric Image Parsing with Superpixels&lt;/em&gt;'', ECCV,  2010.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-PhotoTourism2006&quot; href=&quot;#call-PhotoTourism2006&quot;&gt;PhotoTourism2006&lt;/a&gt;] Snavely Noah, Seitz Steven M. and Szeliski Richard, ``&lt;em&gt;Photo Tourism: Exploring Photo Collections in 3D&lt;/em&gt;'', ToG, vol. 25, number 3, pp. 835‚Äì846,  2006.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-RomeInDay2009&quot; href=&quot;#call-RomeInDay2009&quot;&gt;RomeInDay2009&lt;/a&gt;] Agarwal Sameer, Furukawa Yasutaka, Snavely Noah &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Building Rome in a day&lt;/em&gt;'', Communications of the ACM, vol. 54, number , pp. 105--112,  2011.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-COLMAP2016&quot; href=&quot;#call-COLMAP2016&quot;&gt;COLMAP2016&lt;/a&gt;] J. Sch\&quot;{o}nberger and J. Frahm, ``&lt;em&gt;Structure-From-Motion Revisited&lt;/em&gt;'', CVPR,  2016.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Se02&quot; href=&quot;#call-Se02&quot;&gt;Se02&lt;/a&gt;] Se S., G. D. and Little J., ``&lt;em&gt;Mobile Robot Localization and Mapping with Uncertainty Using Scale-Invariant Visual Landmarks&lt;/em&gt;'', IJRR, vol. 22, number 8, pp. 735--758,  2002.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-PTAM2007&quot; href=&quot;#call-PTAM2007&quot;&gt;PTAM2007&lt;/a&gt;] G. {Klein} and D. {Murray}, ``&lt;em&gt;Parallel Tracking and Mapping for Small AR Workspaces&lt;/em&gt;'', IEEE and ACM International Symposium on Mixed and Augmented Reality,  2007.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Mur15&quot; href=&quot;#call-Mur15&quot;&gt;Mur15&lt;/a&gt;] Mur-Artal R., Montiel J. and Tard{\'o}s J., ``&lt;em&gt;ORB-Slam: A Versatile and Accurate Monocular Slam System&lt;/em&gt;'', IEEE Transactions on Robotics, vol. 31, number 5, pp. 1147--1163,  2015.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Brown07&quot; href=&quot;#call-Brown07&quot;&gt;Brown07&lt;/a&gt;] Brown M. and Lowe D., ``&lt;em&gt;Automatic Panoramic Image Stitching Using Invariant Features&lt;/em&gt;'', IJCV, vol. 74, number , pp. 59--73,  2007.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-DualBootstrap2003&quot; href=&quot;#call-DualBootstrap2003&quot;&gt;DualBootstrap2003&lt;/a&gt;] V. C., Tsai} {Chia-Ling and {Roysam} B., ``&lt;em&gt;The dual-bootstrap iterative closest point algorithm with application to retinal image registration&lt;/em&gt;'', IEEE Transactions on Medical Imaging, vol. 22, number 11, pp. 1379-1394,  2003.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-AlexNet2012&quot; href=&quot;#call-AlexNet2012&quot;&gt;AlexNet2012&lt;/a&gt;] Alex Krizhevsky, Ilya Sutskever and Geoffrey E., ``&lt;em&gt;ImageNet Classification with Deep Convolutional Neural Networks&lt;/em&gt;'',  2012.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Astounding2014&quot; href=&quot;#call-Astounding2014&quot;&gt;Astounding2014&lt;/a&gt;] A. S., H. {Azizpour}, J. {Sullivan} &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;CNN Features Off-the-Shelf: An Astounding Baseline for Recognition&lt;/em&gt;'', CVPRW,  2014.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Melekhov2017relativePoseCnn&quot; href=&quot;#call-Melekhov2017relativePoseCnn&quot;&gt;Melekhov2017relativePoseCnn&lt;/a&gt;] I. Melekhov, J. Ylioinas, J. Kannala &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Relative Camera Pose Estimation Using Convolutional Neural Networks&lt;/em&gt;'', ,  2017.  &lt;a href=&quot;https://arxiv.org/abs/1702.01381&quot;&gt;online&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-PoseNet2015&quot; href=&quot;#call-PoseNet2015&quot;&gt;PoseNet2015&lt;/a&gt;] A. Kendall, M. Grimes and R. Cipolla, ``&lt;em&gt;PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization&lt;/em&gt;'', ICCV,  2015.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-sattler2019understanding&quot; href=&quot;#call-sattler2019understanding&quot;&gt;sattler2019understanding&lt;/a&gt;] T. Sattler, Q. Zhou, M. Pollefeys &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Understanding the limitations of cnn-based absolute camera pose regression&lt;/em&gt;'', CVPR,  2019.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-zhou2019learn&quot; href=&quot;#call-zhou2019learn&quot;&gt;zhou2019learn&lt;/a&gt;] Q. Zhou, T. Sattler, M. Pollefeys &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;To Learn or Not to Learn: Visual Localization from Essential Matrices&lt;/em&gt;'', ICRA,  2020.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Tatarchenko2019&quot; href=&quot;#call-Tatarchenko2019&quot;&gt;Tatarchenko2019&lt;/a&gt;] M. Tatarchenko, S.R. Richter, R. Ranftl &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;What Do Single-View 3D Reconstruction Networks Learn?&lt;/em&gt;'', CVPR,  2019.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-STN2015&quot; href=&quot;#call-STN2015&quot;&gt;STN2015&lt;/a&gt;] M. Jaderberg, K. Simonyan and A. Zisserman, ``&lt;em&gt;Spatial transformer networks&lt;/em&gt;'', NeurIPS,  2015.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-NALU2018&quot; href=&quot;#call-NALU2018&quot;&gt;NALU2018&lt;/a&gt;] A. Trask, F. Hill, S.E. Reed &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Neural arithmetic logic units&lt;/em&gt;'', NeurIPS,  2018.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-NAU2020&quot; href=&quot;#call-NAU2020&quot;&gt;NAU2020&lt;/a&gt;] A. Madsen and A. Rosenberg, ``&lt;em&gt;Neural Arithmetic Units&lt;/em&gt;'', ICLR,  2020.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-MakeCNNShiftInvariant2019&quot; href=&quot;#call-MakeCNNShiftInvariant2019&quot;&gt;MakeCNNShiftInvariant2019&lt;/a&gt;] R. Zhang, ``&lt;em&gt;Making convolutional networks shift-invariant again&lt;/em&gt;'', ICML,  2019.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-GroupEqCNN2016&quot; href=&quot;#call-GroupEqCNN2016&quot;&gt;GroupEqCNN2016&lt;/a&gt;] T. Cohen and M. Welling, ``&lt;em&gt;Group equivariant convolutional networks&lt;/em&gt;'', ICML,  2016.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-lindeberg2013scale&quot; href=&quot;#call-lindeberg2013scale&quot;&gt;lindeberg2013scale&lt;/a&gt;] Lindeberg Tony, ``&lt;em&gt;Scale-space theory in computer vision&lt;/em&gt;'', , vol. 256, number , pp. ,  2013.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-AdvPatch2017&quot; href=&quot;#call-AdvPatch2017&quot;&gt;AdvPatch2017&lt;/a&gt;] T. Brown, D. Mane, A. Roy &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Adversarial patch&lt;/em&gt;'', NeurIPSW,  2017.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-OnePixelAttack2019&quot; href=&quot;#call-OnePixelAttack2019&quot;&gt;OnePixelAttack2019&lt;/a&gt;] Su Jiawei, Vargas Danilo Vasconcellos and Sakurai Kouichi, ``&lt;em&gt;One pixel attack for fooling deep neural networks&lt;/em&gt;'', IEEE Transactions on Evolutionary Computation, vol. 23, number 5, pp. 828--841,  2019.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-jin2020image&quot; href=&quot;#call-jin2020image&quot;&gt;jin2020image&lt;/a&gt;] Jin Yuhe, Mishkin Dmytro, Mishchuk Anastasiia &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Image Matching across Wide Baselines: From Paper to Practice&lt;/em&gt;'', arXiv preprint arXiv:2003.01587, vol. , number , pp. ,  2020.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-HardNet2017&quot; href=&quot;#call-HardNet2017&quot;&gt;HardNet2017&lt;/a&gt;] A. Mishchuk, D. Mishkin, F. Radenovic &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Working Hard to Know Your Neighbor's Margins: Local Descriptor Learning Loss&lt;/em&gt;'', NeurIPS,  2017.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-KeyNet2019&quot; href=&quot;#call-KeyNet2019&quot;&gt;KeyNet2019&lt;/a&gt;] A. Barroso-Laguna, E. Riba, D. Ponsa &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Key.Net: Keypoint Detection by Handcrafted and Learned CNN Filters&lt;/em&gt;'', ICCV,  2019.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-SuperPoint2017&quot; href=&quot;#call-SuperPoint2017&quot;&gt;SuperPoint2017&lt;/a&gt;] Detone D., Malisiewicz T. and Rabinovich A., ``&lt;em&gt;Superpoint: Self-Supervised Interest Point Detection and Description&lt;/em&gt;'', CVPRW Deep Learning for Visual SLAM, vol. , number , pp. ,  2018.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-sarlin2019superglue&quot; href=&quot;#call-sarlin2019superglue&quot;&gt;sarlin2019superglue&lt;/a&gt;] P. Sarlin, D. DeTone, T. Malisiewicz &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;SuperGlue: Learning Feature Matching with Graph Neural Networks&lt;/em&gt;'', CVPR,  2020.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-gradslam2020&quot; href=&quot;#call-gradslam2020&quot;&gt;gradslam2020&lt;/a&gt;] J. Krishna Murthy, G. Iyer and L. Paull, ``&lt;em&gt;gradSLAM: Dense SLAM meets Automatic Differentiation &lt;/em&gt;'', ICRA,  2020 .&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-CapsNet2011&quot; href=&quot;#call-CapsNet2011&quot;&gt;CapsNet2011&lt;/a&gt;] G.E. Hinton, A. Krizhevsky and S.D. Wang, ``&lt;em&gt;Transforming auto-encoders&lt;/em&gt;'', ICANN,  2011.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-CapsNet2017&quot; href=&quot;#call-CapsNet2017&quot;&gt;CapsNet2017&lt;/a&gt;] S. Sabour, N. Frosst and G.E. Hinton, ``&lt;em&gt;Dynamic routing between capsules&lt;/em&gt;'', NeurIPS,  2017.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-li2020extreme&quot; href=&quot;#call-li2020extreme&quot;&gt;li2020extreme&lt;/a&gt;] Li Jianguo, Sun Mingjie and Zhang Changshui, ``&lt;em&gt;Extreme Values are Accurate and Robust in Deep Networks&lt;/em&gt;'', , vol. , number , pp. ,  2020.  &lt;a href=&quot;https://openreview.net/forum?id=H1gHb1rFwr&quot;&gt;online&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/wide-baseline-stereo-blog/images/doll_wbs_300.png" /><media:content medium="image" url="/wide-baseline-stereo-blog/images/doll_wbs_300.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>