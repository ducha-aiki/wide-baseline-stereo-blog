<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="/wide-baseline-stereo-blog/feed.xml" rel="self" type="application/atom+xml" /><link href="/wide-baseline-stereo-blog/" rel="alternate" type="text/html" /><updated>2021-01-09T08:57:28-06:00</updated><id>/wide-baseline-stereo-blog/feed.xml</id><title type="html">Wide baseline stereo meets deep learning</title><subtitle>Everything you (didn't) want to know about image matching</subtitle><entry><title type="html">Wide multiple baseline stereo in simple terms</title><link href="/wide-baseline-stereo-blog/2021/01/09/wxbs-in-simple-terms.html" rel="alternate" type="text/html" title="Wide multiple baseline stereo in simple terms" /><published>2021-01-09T00:00:00-06:00</published><updated>2021-01-09T00:00:00-06:00</updated><id>/wide-baseline-stereo-blog/2021/01/09/wxbs-in-simple-terms</id><content type="html" xml:base="/wide-baseline-stereo-blog/2021/01/09/wxbs-in-simple-terms.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-01-09-wxbs-in-simple-terms.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;What-is-the-wide-baseline-stereo?&quot;&gt;What is the wide baseline stereo?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-the-wide-baseline-stereo?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-03-27-intro_files/att_00001.png&quot; alt=&quot;&quot; title=&quot;Two images of the same place, taken from different viewpoint and in different times. The building: Kyiv doll theater.&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Imagine you have a nice photo you took in autumn and would like to take one in summer, from exactly the same spot. How would you achieve that? You come to the place and start to compare what you see on the camera screen and on the printed photo. Specifically, you would probably try to locate the same objects, e.g. &quot;that high lamppost&quot; or &quot;this wall clock&quot;. Then one would estimate how differently they are arranged on the old photo and camera screen. For example, by checking whether the lamppost is occluding the clock on the tower, or not. That would give an idea of how you should move your camera, i.e. relative camera pose.&lt;/p&gt;
&lt;p&gt;Now, what if you do not have that photo as is, but can have only a description of it instead? In that case, it is likely that you would try to list features and objects in the photo. This can be together with descriptions, which are distinguishable. For example, &quot;long staircase on the left side”, &quot;The nice building with a dark roof and two towers&quot; or “top of the lampost”. It would be useful to also describe where these objects and features are pictured in the photo, &quot;The lamp posts are on the left, the closest to the viewer is in front of the left tower with a clock. The clock tower is not the part of the building and stands on its own&quot;.&lt;/p&gt;
&lt;p&gt;Then, when arriving on the spot, you would try to find those objects, match them to the description you have, and try to estimate where you should go. You repeat the procedure until the camera screen shows a similar enough picture to what you have in mind.&lt;/p&gt;
&lt;p&gt;Congratulations! You just have successfully registered two images, which have a significant difference in viewpoint, appearance, and illumination. In the process of doing so, you have solved multiple times the wide multiple-baseline stereo problem (WxBS) -- estimating the relative camera pose from a pair of images, different in the many aspects, yet depicting the same scene.&lt;/p&gt;
&lt;p&gt;Let us write down the steps, which we took.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Identify salient objects and features in the images -- &quot;trees&quot;, &quot;statues&quot;, “tip of the tower”, etc in images.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Describe the objects and features, taking into account their neighborhood: “statue with a blue left ear&quot;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Establish potential correspondences between features in the different images, based on their descriptors.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Estimate, which direction one should move the camera to align the objects and features.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That is it! If you are interested in the formal definition, &lt;a href=&quot;https://ducha-aiki.github.io/wide-baseline-stereo-blog/2020/07/09/wxbs.html&quot;&gt;check here&lt;/a&gt;, and the &lt;a href=&quot;https://ducha-aiki.github.io/wide-baseline-stereo-blog/2020/03/27/intro.html&quot;&gt;history of the WxBS is here&lt;/a&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/wide-baseline-stereo-blog/images/doll-theater.jpg" /><media:content medium="image" url="/wide-baseline-stereo-blog/images/doll-theater.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Lessons learned and future directions</title><link href="/wide-baseline-stereo-blog/2020/11/26/lessons-and-future-directions.html" rel="alternate" type="text/html" title="Lessons learned and future directions" /><published>2020-11-26T00:00:00-06:00</published><updated>2020-11-26T00:00:00-06:00</updated><id>/wide-baseline-stereo-blog/2020/11/26/lessons-and-future-directions</id><content type="html" xml:base="/wide-baseline-stereo-blog/2020/11/26/lessons-and-future-directions.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-11-26-lessons-and-future-directions.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I would like to share some lessons I have learned about wide baseline stereo and propose some research directions, which are worth exploring in the short and longer term.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Lessons-learned&quot;&gt;Lessons learned&lt;a class=&quot;anchor-link&quot; href=&quot;#Lessons-learned&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;h2 id=&quot;1.-Benchmark-=-metrics-+-implementation-+-dataset&quot;&gt;1. Benchmark = metrics + implementation + dataset&lt;a class=&quot;anchor-link&quot; href=&quot;#1.-Benchmark-=-metrics-+-implementation-+-dataset&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In our paper &quot;&lt;a href=&quot;https://arxiv.org/abs/2003.01587&quot;&gt;Image Matching across Wide Baselines: From Paper to Practice&lt;/a&gt;&quot; we focused on the first two parts. Specifically, metrics -- if they are not &quot;downstream&quot; metrics, the improvements in the single component might not translate to the overall system improvements. 
And implementation -- implementing the simplest possible setup is, of course, a valuable tool, but one have to also incorporate the best known practices, e.g. matching, RANSAC tuning as so on.&lt;/p&gt;
&lt;p&gt;I hope, that we have delivered that message to the community. But the last component -- the dataset -- we have, perhaps, overlooked a bit ourself. The problem with dataset limitations, e.g. lack of illumination or seasonal changes is not that one does not properly address. Usually benchmark papers are adressing their limitations quite clearly.
The problem is that researchers (including myself) have tendency to work on improving results, which are easily measurable, therefore the implicitly designing the methods, which solve only some specific problem, encoded in the form of the dataset.&lt;/p&gt;
&lt;h2 id=&quot;2.-Trying-to-work-for-the-&amp;quot;most-general&amp;quot;-case-might-be-detrimental-for-the-practical-applications.&quot;&gt;2. Trying to work for the &quot;most general&quot; case might be detrimental for the practical applications.&lt;a class=&quot;anchor-link&quot; href=&quot;#2.-Trying-to-work-for-the-&amp;quot;most-general&amp;quot;-case-might-be-detrimental-for-the-practical-applications.&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;This is kind of opposite side of the lesson 1. For example, the classical SIFT matching is rotation-invariant, because that was assumed to be the requirement to work in the &quot;real world&quot;. However, the practice of image retrieval and then most of learned local features like R2D2, SuperPoint, DELF and so on, showed that &quot;up-is-up&quot; is a reasonable assumption to built on. The rotational invariance in lots of scenarios hurt more than helps.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;3.-When-borrow-idea-from-classical-paper,-adapt-it&quot;&gt;3. When borrow idea from classical paper, adapt it&lt;a class=&quot;anchor-link&quot; href=&quot;#3.-When-borrow-idea-from-classical-paper,-adapt-it&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://proceedings.neurips.cc/paper/2017/hash/831caa1b600f852b7844499430ecac17-Abstract.html&quot;&gt;HardNet&lt;/a&gt; borrows the idea of the using &lt;a href=&quot;https://medium.com/@ducha.aiki/how-to-match-to-learn-or-not-to-learn-part-2-1ab52ede2022&quot;&gt;second nearest neighbor&lt;/a&gt;(SNN) descriptor from the SIFT. However, using SNN ratio for descriptor learning leads to inferior results. We had to modify it to the triplet margin loss.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;4.-Classic-handcrafted-algorithms-are-not-dead-and-can-be-improved-a-lot&quot;&gt;4. Classic handcrafted algorithms are not dead and can be improved a lot&lt;a class=&quot;anchor-link&quot; href=&quot;#4.-Classic-handcrafted-algorithms-are-not-dead-and-can-be-improved-a-lot&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Modern versions of RANSAC are still necessary for two view matching even when so complex methods as SuperGlue as used. Moreover, they still have quite a lot things for improvement, as proved by my colleague, &lt;a href=&quot;https://scholar.google.hu/citations?user=U9-D8DYAAAAJ&amp;amp;hl=en&quot;&gt;Daniel Barath&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;They also have a benefit, that once you have an idea, you can make a paper out it faster, as you don't need gathering data and training the model.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Future-research-directions&quot;&gt;Future research directions&lt;a class=&quot;anchor-link&quot; href=&quot;#Future-research-directions&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;h2 id=&quot;1.-Application-specific-local-features&quot;&gt;1. Application-specific local features&lt;a class=&quot;anchor-link&quot; href=&quot;#1.-Application-specific-local-features&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;One thing about the current local features is that they are kind of universal. SIFT works reasonably well for lots of applications: 3d reconstruction, SLAM, image retrieval, etc. The learned features, like SuperPoint or R2D2, although are biased towards the data they are trained on, still don't have anything domain specific.&lt;/p&gt;
&lt;p&gt;Let me explain. There are different qualities about the local feature (detectors). It can be more or less robust to nuisance factors like the illumination and camera position. It can be more or less precisely localized. It can be more or less dense and/or evenly distributed over the image.&lt;/p&gt;
&lt;p&gt;For example, in image retrieval, one does not really care about precise localization, the robustness is much more important. For the 3d reconstruction one would like to have a lot of 3d points to get the reasonable reconstruction.
On the other hand, for the SLAM/relocalization application, sparse features would be more advantageous because of smaller memory footprint and computational cost.&lt;/p&gt;
&lt;p&gt;There are, actually, some steps in that direction. Let me name a few.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2007.13172.pdf&quot;&gt;HOW local features&lt;/a&gt; designed for the image retrieval
&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-11-26-lessons-and-future-directions_files/att_00000.png&quot; alt=&quot;&quot; title=&quot;Robust, but poorly localized HOW local features for imare retrieval, from &amp;#39;Learning and aggregating deep local descriptors for instance-level recognition&amp;#39; paper&quot; /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://rpg.ifi.uzh.ch/docs/3DV19_Cieslewski.pdf&quot;&gt;SIPs: Succinct Interest Points from Unsupervised Inlierness Probability Learning&lt;/a&gt; sparse local features for the SLAM.&lt;/p&gt;
&lt;!--![](2020-11-26-lessons-and-future-directions_files/att_00001.png &quot;A small number of keypoints might be enough for estimating the camera pose. Figure from SIPs paper&quot;) --&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1912.00623&quot;&gt;Reinforced Feature Points: Optimizing Feature Detection and Description for a High-Level Task&lt;/a&gt;
&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-11-26-lessons-and-future-directions_files/att_00002.png&quot; alt=&quot;&quot; title=&quot;Optimizing local features for non-differentiable high-level task, Figure from Reinforced Feature Points&quot; /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I believe, that it is only beginning and we are yet to experience AlphaZero moment for the local features.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;2.-Rethinking-overall-wide-baseline-stereo-pipeline,-optimized-for-the-specific-application&quot;&gt;2. Rethinking overall wide baseline stereo pipeline, optimized for the specific application&lt;a class=&quot;anchor-link&quot; href=&quot;#2.-Rethinking-overall-wide-baseline-stereo-pipeline,-optimized-for-the-specific-application&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;It is often perceived, that image matching across the unordered collection of the images is a task with quadratic complexity w.r.t. number of images. Some operations can be done separately, e.g. feature detection, but others, like feature matching and RANSAC cannot. Right?&lt;/p&gt;
&lt;p&gt;Not necessarily. It turns out, that one can avoid running feature matching and RANSAC for more than 90% of image pairs with clever preprocessing, ordering and re-using results from the previous matching.
Moreover, in order to do the whole task faster (matching image collections), one may need to introduce additional steps, which are not necessary, or slowing things down for the two images case.&lt;/p&gt;
&lt;p&gt;That's what we done for the &lt;a href=&quot;https://arxiv.org/abs/2011.11986&quot;&gt;intial pose estimation for the global SfM&lt;/a&gt; in our paper, which reduced the matching runtime from 200 hours to 29.&lt;/p&gt;
&lt;!-- ![](2020-11-26-lessons-and-future-directions_files/att_00003.png &quot;Table from the Efficient Initial Pose-graph Generation for Global SfM&quot;) --&gt;

&lt;p&gt;Another example would be &quot;&lt;a href=&quot;https://arxiv.org/abs/1911.11763&quot;&gt;SuperGlue: Learning Feature Matching with Graph Neural Networks&lt;/a&gt;&quot;, where authors abandoned traditional descriptor matching and instead leveraged all the information for keypoint and descriptors from both images altogether.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;3.-Rethinking-and-improving-the-process-of-training-data-generation-for-the-WBS&quot;&gt;3. Rethinking and improving the process of training data generation for the WBS&lt;a class=&quot;anchor-link&quot; href=&quot;#3.-Rethinking-and-improving-the-process-of-training-data-generation-for-the-WBS&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;So far, all the local feature papers I have seen rely on one of the ground truth source.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;SfM data, obtained with COLMAP with, possibly, a cleaned depth information.&lt;/li&gt;
&lt;li&gt;Affine and color augmentation.&lt;/li&gt;
&lt;li&gt;Synthetic images (e.g. corners for SuperPoint).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are several problems with them.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SfM data&lt;/strong&gt; assumes that the data is matchable by the existing methods, at least for a some extent. That might not always be true for cross-seasonal, medical and other kind of data. It is also not applicable for historical photographies and other types of data. Moreover, SfM data takes quite a time for compute and space to store. I believe that we may do better.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Affine and color augmentation&lt;/strong&gt; can take us only this far -- we actually want our detectors and descriptors to be robust to the changes, which we don't know how to simulate/augment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Synthetic images&lt;/strong&gt; as they are in, say, &lt;a href=&quot;https://carla.org/&quot;&gt;CARLA&lt;/a&gt; simulator lack fine details and photorealism. However, I am optimistic about using neural renderers and learned wide baseline stereo is a GAN-like self-improving loop.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;4.-Matching-with-On-Demand-View-Synthesis-revisited&quot;&gt;4. Matching with On-Demand View Synthesis revisited&lt;a class=&quot;anchor-link&quot; href=&quot;#4.-Matching-with-On-Demand-View-Synthesis-revisited&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;I like the &quot;on-demand&quot; principle a lot and I think we can explore it much more that we are now. 
So far we have either &lt;a href=&quot;https://ducha-aiki.github.io/wide-baseline-stereo-blog/2020/08/06/affine-view-synthesis.html&quot;&gt;affine view synthesis (ASIFT, MODS)&lt;/a&gt;, or &lt;a href=&quot;https://people.ee.ethz.ch/~timofter/publications/Anoosheh-ICRA-2019.pdf&quot;&gt;GAN-based stylizations&lt;/a&gt; for the day-night matching.&lt;/p&gt;
&lt;p&gt;That is why I am glad to see papers like &lt;a href=&quot;https://arxiv.org/abs/2008.09497&quot;&gt;Single-Image Depth Prediction Makes Feature Matching Easier&lt;/a&gt;, which generate normalized views based on depth in order to help the matching.&lt;/p&gt;
&lt;p&gt;Why not go further? Combine viewpoint, illumination, season, sensor synthesis?&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;5.-Moar-inputs!&quot;&gt;5. Moar inputs!&lt;a class=&quot;anchor-link&quot; href=&quot;#5.-Moar-inputs!&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;I have mentioned above that monocular depth may help the feature matching or &lt;a href=&quot;https://ducha-aiki.github.io/wide-baseline-stereo-blog/2020/07/17/affine-correspondences.html#Back-to-wide-baseline-stereo&quot;&gt;camera pose estimation&lt;/a&gt;. However, why stop here?&lt;/p&gt;
&lt;p&gt;Let's use other networks as well, especially given that we will need them on robot or vehicle anyway. Semantic segmentation? Yes, please. Surface normals? Why not? Intrinsic images? Йой, най буде! &lt;sup id=&quot;fnref-1&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-1&quot;&gt;1. Ukrainian, means let is be&lt;a href=&quot;#fnref-1&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-11-26-lessons-and-future-directions_files/att_00004.png&quot; alt=&quot;image.png&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;What-do-you-think-would-be-good-idea-for-the-WBS--research?&quot;&gt;What do you think would be good idea for the WBS  research?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-do-you-think-would-be-good-idea-for-the-WBS--research?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Let me know in comments/twitter. I am also going to update this page from time to time&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/wide-baseline-stereo-blog/images/yoy_cat.jpg" /><media:content medium="image" url="/wide-baseline-stereo-blog/images/yoy_cat.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Benchmarking Image Retrieval for Visual Localization</title><link href="/wide-baseline-stereo-blog/2020/11/25/review-of-retrieval-for-localization.html" rel="alternate" type="text/html" title="Benchmarking Image Retrieval for Visual Localization" /><published>2020-11-25T00:00:00-06:00</published><updated>2020-11-25T00:00:00-06:00</updated><id>/wide-baseline-stereo-blog/2020/11/25/review-of-retrieval-for-localization</id><content type="html" xml:base="/wide-baseline-stereo-blog/2020/11/25/review-of-retrieval-for-localization.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-11-25-review-of-retrieval-for-localization.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I would like to share my thoughts on 3DV 2020 paper &quot;&lt;a href=&quot;https://arxiv.org/pdf/2011.11946.pdf&quot;&gt;Benchmarking Image Retrieval for Visual Localization&lt;/a&gt;&quot; by Pion et.al.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-11-25-review-of-retrieval-for-localization_files/att_00000.png&quot; alt=&quot;&quot; title=&quot;Overview of the benchmark pipeline&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;What-is-the-paper-about?&quot;&gt;What is the paper about?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-the-paper-about?&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;How one would approach visual localization? The most viable way to do it is hierarchical approach, similar to image retrieval with spatial verification.&lt;/p&gt;
&lt;p&gt;You get the query image, retrieve the most similar images to it from some database by some efficient method, e.g. global descriptor search. Then given the top-k images you estimate the pose of the query image by doing two-view matching, or some other method.&lt;/p&gt;
&lt;p&gt;The question is -- how much influence the quality of the image retrieval has? Should you spend more or less time improving it? That is the questions, paper trying to answer.&lt;/p&gt;
&lt;p&gt;Authors design 3 re-localization systems.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&quot;Task 1&quot; system estimates the query image pose as average of the short-list images pose, weighted by the similarity to the query image.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&quot;Task 2a&quot; system performs pairwise two-view matching between short-list images and query triangulates the query image pose using 3d map built from the successully matches images.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&quot;Task 2b&quot; pre-builds the 3D map from the database images offline. At the inference time, local feature 2d-3d matching is done on the shortlist images.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;What-is-benchmarked?&quot;&gt;What is benchmarked?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-benchmarked?&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;The paper compares&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SIFT-based DenseVLAD&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and CNN-based&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1511.07247&quot;&gt;NetVLAD&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1906.07589.pdf&quot;&gt;APGeM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2001.05027&quot;&gt;DELG&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What is important (and adequately mentioned in the paper, although I would prefer the disclamer in the each figure) is that &lt;strong&gt;all CNN-based methods have different architectures AND training data&lt;/strong&gt;. Basically, the paper uses author-released models. Thus one cannot say if APGeM is better or worse than NetVLAD as method, because they were trained on the very different data. However, I also understand that one cannot easily afford to re-implement and re-train everything.&lt;/p&gt;
&lt;p&gt;As the sanity check paper provides the results on the &lt;a href=&quot;http://cmp.felk.cvut.cz/revisitop/&quot;&gt;Revisited Oxford and Paris&lt;/a&gt; image retrieval benchmark.
&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-11-25-review-of-retrieval-for-localization_files/att_00001.png&quot; alt=&quot;&quot; title=&quot;Retrieval metrics on Revised Oxford and Paris&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Summary-of-results&quot;&gt;Summary of results&lt;a class=&quot;anchor-link&quot; href=&quot;#Summary-of-results&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Paper contains a lot of information and I definitely recommend you to read it. Nevertheless, let me try to summarize paper messages and then my take on it.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;For the task1 (similarity-weighted pose) there is no clear winner. (SIFT)-DenseVLAD works the best for the daytime datasets. Probably DenseVLAD is good because it is not invariant and if it can match images, they are really close -&amp;gt; high pose accuracy.  For the night both DeLG and AP-GeM are good. As paper guesses, that it because they are only ones, which were trained on night images as well.
&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-11-25-review-of-retrieval-for-localization_files/att_00002.png&quot; alt=&quot;image.png&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ol&gt;
&lt;li&gt;There is almost no difference between CNN-based methods for the task2a and task2b (retrieval -&amp;gt; local features matching). This indicates that the limit is the mostly in the number of images and local features.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-11-25-review-of-retrieval-for-localization_files/att_00003.png&quot; alt=&quot;image.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-11-25-review-of-retrieval-for-localization_files/att_00004.png&quot; alt=&quot;image.png&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;My-take-away-messages&quot;&gt;My take-away messages&lt;a class=&quot;anchor-link&quot; href=&quot;#My-take-away-messages&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;h3 id=&quot;Image-Relocalization-seems-to-be-is-more-real-world-and-engineering-task,-than-image-retrieval.&quot;&gt;Image Relocalization seems to be is more real-world and engineering task, than image retrieval.&lt;a class=&quot;anchor-link&quot; href=&quot;#Image-Relocalization-seems-to-be-is-more-real-world-and-engineering-task,-than-image-retrieval.&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;And that it why it actually ALREADY WORKS, because if there some weak spot, it is compensated by the system design.  Thhe same conclusion from our &lt;a href=&quot;https://arxiv.org/abs/2003.01587&quot;&gt;IMC paper&lt;/a&gt;, experiment with ground truth -- if you have 1k images for the 3d model, you can use as bad features, as you want. The COLMAP will recover anyway&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-11-25-review-of-retrieval-for-localization_files/att_00009.png&quot; alt=&quot;image.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The retrieval, on the other hand is more interesting to work on, because it is kind of deliberately hard and you can do some fancy stuff, which do not matter in the real world.&lt;/p&gt;
&lt;h3 id=&quot;Task1-(global-descriptor-only)-system-are-quite-useless-now&quot;&gt;Task1 (global descriptor-only) system are quite useless now&lt;a class=&quot;anchor-link&quot; href=&quot;#Task1-(global-descriptor-only)-system-are-quite-useless-now&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Unless we are speaking about the quite dense image representation. I mean, top-accuracy is 35% vs almost 100% for those, which include local features.&lt;/p&gt;
&lt;p&gt;Good news: it has a LOT of space for the improvement to work on.&lt;/p&gt;
&lt;h3 id=&quot;For-the-task-2a-and-2b,-robust-global-descriptors-are-a-way-to-do-the-retrieval,-sorry-VLAD.&quot;&gt;For the task 2a and 2b, robust global descriptors are a way to do the retrieval, sorry VLAD.&lt;a class=&quot;anchor-link&quot; href=&quot;#For-the-task-2a-and-2b,-robust-global-descriptors-are-a-way-to-do-the-retrieval,-sorry-VLAD.&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The precision will come from the local features. Which I like a lot, because VLAD is more complex to train and initalize, I never liked it (nothing personal).&lt;/p&gt;
&lt;h3 id=&quot;For-the-task2a-and-2b-we-need-new-metrics,-e.g.-precisition-@-X-Mb-memory-footprint&quot;&gt;For the task2a and 2b we need new metrics, e.g. precisition @ X Mb memory footprint&lt;a class=&quot;anchor-link&quot; href=&quot;#For-the-task2a-and-2b-we-need-new-metrics,-e.g.-precisition-@-X-Mb-memory-footprint&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Because otherwise, the task is easily solved by the brute force -- either by photo taking, or, at least with image syntesis, see &lt;a href=&quot;https://hal.inria.fr/hal-01616660/document&quot;&gt;24/7 place recognition by view synthesis&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Such steps are already taken in the paper &lt;a href=&quot;https://arxiv.org/abs/2007.13172&quot;&gt;Learning and aggregating deep local descriptors for instance-level recognition&lt;/a&gt;  -- see the table with memory footprint.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-11-25-review-of-retrieval-for-localization_files/att_00006.png&quot; alt=&quot;image.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;That is how one could have an interesting research challenge, also having some grounds in the real-world -- to work in mobile phones. Otherwise, any method would work, if the database is dense enough.&lt;/p&gt;
&lt;h3 id=&quot;Robust-local-features-matter-for-illumination-changes&quot;&gt;Robust local features matter for illumination changes&lt;a class=&quot;anchor-link&quot; href=&quot;#Robust-local-features-matter-for-illumination-changes&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;It is a bit hidden in the Appendix, so go directly to the Figure 9. It clearly shows that localization performance is bounded by SIFT, if it is used for two view matching, making retrieval improvements irrelevant. When R2D2 or D2Net are used for matching instead, the overall results for night-time are much better.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-11-25-review-of-retrieval-for-localization_files/att_00007.png&quot; alt=&quot;image.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;That is in line with my small visual benchmark I did recently.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://twitter.com/ducha_aiki/status/1330495426865344515&quot;&gt;https://twitter.com/ducha_aiki/status/1330495426865344515&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-11-25-review-of-retrieval-for-localization_files/att_00008.png&quot; alt=&quot;image.png&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;That's all, folks! Now please, check the &lt;a href=&quot;https://arxiv.org/abs/2011.11946.pdf&quot;&gt;paper&lt;/a&gt; and the &lt;a href=&quot;https://github.com/naver/kapture-localization&quot;&gt;code&lt;/a&gt; they provided.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/wide-baseline-stereo-blog/images/retrieval-for-loc.png" /><media:content medium="image" url="/wide-baseline-stereo-blog/images/retrieval-for-loc.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Revisiting Brown patch dataset and benchmark</title><link href="/wide-baseline-stereo-blog/2020/09/23/local-descriptors-validation.html" rel="alternate" type="text/html" title="Revisiting Brown patch dataset and benchmark" /><published>2020-09-23T00:00:00-05:00</published><updated>2020-09-23T00:00:00-05:00</updated><id>/wide-baseline-stereo-blog/2020/09/23/local-descriptors-validation</id><content type="html" xml:base="/wide-baseline-stereo-blog/2020/09/23/local-descriptors-validation.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-09-23-local-descriptors-validation.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;In-this-post&quot;&gt;In this post&lt;a class=&quot;anchor-link&quot; href=&quot;#In-this-post&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;Why one needs good development set? What is wrong with existing sets for local patch descriptor learning?&lt;/li&gt;
&lt;li&gt;One should validate in the same way, as it is used in production.&lt;/li&gt;
&lt;li&gt;Brown patch revisited -- implementation details&lt;/li&gt;
&lt;li&gt;Local patch descriptors evaluation results.&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Really-quick-intro-into-local-patch-descriptors&quot;&gt;Really quick intro into local patch descriptors&lt;a class=&quot;anchor-link&quot; href=&quot;#Really-quick-intro-into-local-patch-descriptors&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Local patch descriptor is the thing, which helps you to automatically decide, if two patches in the pair of images correspond to the same point in a real world, or not. It should be robust to illumination, viewpoint and other changes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-09-16-local-descriptors-validation_files/att_00003.png&quot; alt=&quot;&quot; title=&quot;The task of local descriptor, neural network here, is to decide if two patches belong to the same point, or nor. Image taken from SoSNet decriptor blogpost by Vassileios Balntas https://medium.com/scape-technologies/mapping-the-world-part-4-sosnet-to-the-rescue-5383671713e7&quot; /&gt;&lt;/p&gt;
&lt;p&gt;There are lots of ways how to implement a local patch descriptor: engineered and learned. 
Local patch descriptor is the crucial component of the &lt;a href=&quot;https://ducha-aiki.github.io/wide-baseline-stereo-blog/2020/03/27/intro.html&quot;&gt;wide baseline stereo pipeline&lt;/a&gt; and a popular computer vision research topic.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Why-do-you-need-development-set?&quot;&gt;Why do you need development set?&lt;a class=&quot;anchor-link&quot; href=&quot;#Why-do-you-need-development-set?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Good data is crucial for any machine learning problem -- everyone now knows that. 
One needs high quality training set for training a good model. One also needs good test set, to know, what is &lt;em&gt;real&lt;/em&gt; performance. However, there is one more, often forgotten, crucial component -- &lt;strong&gt;validation&lt;/strong&gt; or &lt;strong&gt;development&lt;/strong&gt; set. We use it to decide hyperparameters and validate design choices we make. It should be different from both training and test sets, yet, be good predictor of test set performance.
Moreover, it should allow fast iterations, so be not too small.&lt;/p&gt;
&lt;p&gt;While such set is commonly called &lt;a href=&quot;https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets&quot;&gt;validation set&lt;/a&gt;, I do like Andrew Ng's term &quot;&lt;a href=&quot;https://cs230.stanford.edu/files/C2M1.pdf&quot;&gt;development&lt;/a&gt;&quot; set more - because it helps to &lt;em&gt;develop&lt;/em&gt; your model.&lt;/p&gt;
&lt;h1 id=&quot;Existing-datasets-for-local-patch-descriptors&quot;&gt;Existing datasets for local patch descriptors&lt;a class=&quot;anchor-link&quot; href=&quot;#Existing-datasets-for-local-patch-descriptors&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;So, what are the development set options for local patch descriptors?&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Brown-PhotoTourism.&quot;&gt;Brown PhotoTourism.&lt;a class=&quot;anchor-link&quot; href=&quot;#Brown-PhotoTourism.&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-09-16-local-descriptors-validation_files/brown_patches.png&quot; alt=&quot;&quot; title=&quot;Patches from 3 subsets of Brown Phototourism dataset&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The most commonly and successfully used dataset for local descriptor learning is PhotoTourism, created in 2008. Here is its &lt;a href=&quot;http://matthewalunbrown.com/patchdata/patchdata.html&quot;&gt;description by authors&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;The dataset consists of corresponding patches sampled from 3D reconstructions of the Statue of Liberty (New York), Notre Dame (Paris) and Half Dome (Yosemite).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It also comes with evaluation protocol:patch pairs are labeled as &quot;same&quot; or &quot;different&quot; and the false positive rate at recall of 95% (FPR95) is reported. The variable, used to build ROC curve is descriptor distance between two patches.
Advantages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It contains local patches, extracted for two types of local feature detector -- DoG (SIFT) and Harris corners.&lt;/li&gt;
&lt;li&gt;It is extracted from images, contraining non-planar structures and the geometrical noise present is caused by the local feature detector, not added artificially.  &lt;/li&gt;
&lt;li&gt;Descriptors, trained on the dataset, show very good performance [&lt;a class=&quot;latex_cit&quot; id=&quot;call-IMW2020&quot; href=&quot;#cit-IMW2020&quot;&gt;IMW2020&lt;/a&gt;], therefore the data itself is good.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Disadvantages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;when used as a benchmark, it shows unrealistic results: SIFT is 40x worse than deep learned descriptor. In practice, the difference is much smaller.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;HPatches&quot;&gt;HPatches&lt;a class=&quot;anchor-link&quot; href=&quot;#HPatches&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/hpatches/hpatches-dataset&quot;&gt;HPatches&lt;/a&gt;, where H stands for the &quot;&lt;a href=&quot;https://en.wikipedia.org/wiki/Homography&quot;&gt;homography&lt;/a&gt;&quot; was proposed to overcome the problem of unrealisting metric and, seemingly, too easy data, used in Phototourism dataset.&lt;/p&gt;
&lt;p&gt;It was constructed in a different way than a Phototourism. First, local features were detected in the &quot;reference&quot; image and then reprojected to other images in sequences. Reprojection is prossible, because all the images are photographies of the planes  -- graffity, drawing, print, etc, or are all taken from the same position.
After the reprojection, some amount of geometrical noise -- rotation, translation, scaling, was added to the local features and the patches were extracted.&lt;/p&gt;
&lt;p&gt;This process is illustration on the picture below (both taken from the &lt;a href=&quot;https://github.com/hpatches/hpatches-dataset&quot;&gt;HPatches website&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-09-16-local-descriptors-validation_files/images_hard.png&quot; alt=&quot;&quot; title=&quot;Visualization of the hard patches locations in the target images.&quot; /&gt;
&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-09-16-local-descriptors-validation_files/patches_hard.png&quot; alt=&quot;&quot; title=&quot;Extracted hard patches from the example sequence.&quot; /&gt;&lt;/p&gt;
&lt;p&gt;HPAtches also provide 3 testing protocol, evaluating mean average precision (mAP) for 3 different tasks: patch verification  -- similar to Brown Phototourism, image matching and patch retrieval. The variable, used to build mAP is descriptor distance between two patches.&lt;/p&gt;
&lt;p&gt;Advantages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unlike PhotoTourism patch verification, image matching and patch retrieval tasks are not saturated.&lt;/li&gt;
&lt;li&gt;HPatches contains illumination split, allowing the evaluation of descriptor robustness to illumination changes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Disadvantages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;patches &quot;misregistration&quot; noise is of artificial nature, although paper claims that it has similar statistics&lt;/li&gt;
&lt;li&gt;no non-planar structure&lt;/li&gt;
&lt;li&gt;performance in HPaptches does not really correlate with the downstream performance [&lt;a class=&quot;latex_cit&quot; id=&quot;call-IMW2020&quot; href=&quot;#cit-IMW2020&quot;&gt;IMW2020&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-09-16-local-descriptors-validation_files/hpatches_vs_IMC.png&quot; alt=&quot;image.png&quot; title=&quot;Performance in HPaptches does not really correlate with the downstream performance on the Image Matching Benchmark&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;AMOSPatches&quot;&gt;AMOSPatches&lt;a class=&quot;anchor-link&quot; href=&quot;#AMOSPatches&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/pultarmi/AMOS_patches&quot;&gt;AMOS patches&lt;/a&gt; is &quot;HPatches illumination on steroids, without geometrical noise&quot;. It has the same advantanges and disadvantages, as HPatches and is mostly focused on illumination and weather changes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-09-16-local-descriptors-validation_files/att_00002.png&quot; alt=&quot;&quot; title=&quot;Some images, contributing to AMOSPatches&quot; /&gt;
&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-09-16-local-descriptors-validation_files/amos_patches_small.png&quot; alt=&quot;&quot; title=&quot;Some patches from AMOS Patches dataset&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;PhotoSynth&quot;&gt;PhotoSynth&lt;a class=&quot;anchor-link&quot; href=&quot;#PhotoSynth&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/rmitra/PS-Dataset&quot;&gt;PhotoSynth&lt;/a&gt; can be described and something in the middle between Phototour and HPatches. It contains patches, sampled from planar scenes, as well as from non-planar scenes.&lt;/p&gt;
&lt;p&gt;At first glance, it should be great for the test and training purposes. However, there are several issues with it.
First, pre-trained HardNetPS descriptor, released together with the dataset, works well on HPatches, but poor in practice[&lt;a class=&quot;latex_cit&quot; id=&quot;call-pultar2020improving&quot; href=&quot;#cit-pultar2020improving&quot;&gt;pultar2020improving&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;Second, a couple of colleagues has tried to train the descriptor on top of it, as it was significantly worse than the authors reference model. Moreover, there is no testing/training code protocol available together with dataset.&lt;/p&gt;
&lt;p&gt;So, while PhotoSynth might be a good dataset in principle, it definitely needs more love and work.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-09-16-local-descriptors-validation_files/att_00000.png&quot; alt=&quot;image.png&quot; title=&quot;Images, contributed to PS dataset&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-09-16-local-descriptors-validation_files/att_00001.png&quot; alt=&quot;image.png&quot; title=&quot;Patches, samples from PS dataset&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Designing-the-evaluation-protocol&quot;&gt;Designing the evaluation protocol&lt;a class=&quot;anchor-link&quot; href=&quot;#Designing-the-evaluation-protocol&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Classical local descriptor matching consists of two parts: finding nearest neighbors and filtering unreliable ones based on some criterion. 
I have wrote a &lt;a href=&quot;https://medium.com/@ducha.aiki/how-to-match-to-learn-or-not-to-learn-part-2-1ab52ede2022&quot;&gt;blogpost, describing the matching strategies in details&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The most used in practice criterion is the first to second nearest neighbor distance (Lowe's) ratio threshold for filtering false positive matches. It is shown in the figure below.&lt;/p&gt;
&lt;p&gt;The intuition is simple: if two candidates are too similar, then the match is unreliable and it is better to drop it.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-09-16-local-descriptors-validation_files/att_00004.png&quot; alt=&quot;&quot; title=&quot;Second nearest ratio strategy. Features from img1  -- blue circles -- are matched to features from img2  -- red squares. For each point in img1 we calculate two nearest neighbors and check their distance ratio . If both are too similar, i.e. &amp;gt;0.8, bottom at Figure, then the match is discarded. Only confident matches are kept. Right graph is from SIFT paper, justification of such strategy.&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Somehow, none of the local patch evaluation protocols does not take such filtering criterion in mind, although it greatly influences the overall performance.&lt;/p&gt;
&lt;p&gt;So, let's do the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Take the patches, which are extracted from only two images. &lt;/li&gt;
&lt;li&gt;For the each patch, calculate the descriptor distance to the correct match and to the hardnest (closest) non-match. Calculate the Lowe's ratio between this two.&lt;/li&gt;
&lt;li&gt;Calculate accuracy for each of such triplets. If the correct match has smaller distance, score 1, if not - 0.&lt;/li&gt;
&lt;li&gt;Sort the ratios from smallest to biggest and calculate &lt;a href=&quot;https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval&quot;&gt;mean average precision&lt;/a&gt;#Mean_average_precision) (mAP).&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Brown-PhotoTour-Revisied:-implementation-details&quot;&gt;Brown PhotoTour Revisied: implementation details&lt;a class=&quot;anchor-link&quot; href=&quot;#Brown-PhotoTour-Revisied:-implementation-details&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;We have designed the protocol, now time for data. We could spend several month collecting and cleaning it...or we can just re-use great Brown PhotoTourism dataset. Re-visiting labeling and/or evaluation protocol of the time-tested dataset is a great idea.&lt;/p&gt;
&lt;p&gt;Just couple of examples: &lt;a href=&quot;https://github.com/fastai/imagenette&quot;&gt;ImageNette&lt;/a&gt; created by &lt;a href=&quot;https://twitter.com/jeremyphoward&quot;&gt;Jeremy Howard&lt;/a&gt; from ImageNet, &lt;a href=&quot;http://cmp.felk.cvut.cz/revisitop/&quot;&gt;Revisited Oxford 5k&lt;/a&gt; by &lt;a href=&quot;https://filipradenovic.github.io/&quot;&gt;Filip Radenovic&lt;/a&gt; and so on.&lt;/p&gt;
&lt;p&gt;For the protocol we designed above we need the information about the image id, where the patch was extracted from. Unfortunately, there is no such information in the Brown PhotoTourism, but there is suitable alternative -- the image id, where the reference patch was detected. What does it mean?&lt;/p&gt;
&lt;p&gt;Suppose, we have 4 images and 5 keypoints. All the keypoints present in all images, which gives us 20 patches.
3 keypoints were first detected in Image 1 and 2 in image 2.&lt;br /&gt;
That means that we will have 12 patches labeled image 1 and 8 patches labeled image 2.&lt;/p&gt;
&lt;p&gt;So, we will have results for image 1 and image 2. Let's consider image 1. There are 12 patches, splitted in 3 &quot;classes&quot;, 4 patches in each class.&lt;/p&gt;
&lt;p&gt;Then, for the each of those 12 patches we:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;pick each of the corresponding patched as positives, so 3 positives. $P_1$, $P_2$, $P_3$&lt;/li&gt;
&lt;li&gt;find the closest negative N. &lt;/li&gt;
&lt;li&gt;add triplets (A, $P_1$, N), (A, $P_2$, N), (A, $P_3$, N) to the evaluation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Repeat the same for the image 2. 
That mimics the two-view matching process as close, as possible, given the data available to us.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Installation&quot;&gt;Installation&lt;a class=&quot;anchor-link&quot; href=&quot;#Installation&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;code&gt;pip install brown_phototour_revisited&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&quot;How-to-use&quot;&gt;How to use&lt;a class=&quot;anchor-link&quot; href=&quot;#How-to-use&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;There is a single function, which does everything for you: &lt;code&gt;full_evaluation&lt;/code&gt;. The original Brown benchmark consider evaluation, similar to cross-validation: train descriptor on one subset, evaluate on two others, repeat for all, so 6 evaluations are required. For the handcrafted descriptors, or those, that are trained on 3rd party datasets, only 3 evaluations are necessary.  We are following it here as well.&lt;/p&gt;
&lt;p&gt;However, if you need to run some tests separately, or reuse some functions -- we will cover the usage below.
In the following example we will show how to use &lt;code&gt;full_evaluation&lt;/code&gt; to evaluate SIFT descriptor as implemented in kornia.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# !pip install kornia&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;import torch
import kornia
from IPython.display import clear_output
from brown_phototour_revisited.benchmarking import *
patch_size = 65 

model = kornia.feature.SIFTDescriptor(patch_size, rootsift=True).eval()

descs_out_dir = 'data/descriptors'
download_dataset_to = 'data/dataset'
results_dir = 'data/mAP'

results_dict = {}
results_dict['Kornia RootSIFT'] = full_evaluation(model,
                                'Kornia RootSIFT',
                                path_to_save_dataset = download_dataset_to,
                                path_to_save_descriptors = descs_out_dir,
                                path_to_save_mAP = results_dir,
                                patch_size = patch_size, 
                                device = torch.device('cuda:0'), 
                           distance='euclidean',
                           backend='pytorch-cuda')
clear_output()
print_results_table(results_dict)&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;pre&gt;&lt;code&gt;------------------------------------------------------------------------------
Mean Average Precision wrt Lowe SNN ratio criterion on UBC Phototour Revisited
------------------------------------------------------------------------------
trained on       liberty notredame  liberty yosemite  notredame yosemite
tested  on           yosemite           notredame            liberty
------------------------------------------------------------------------------
Kornia RootSIFT        56.70              47.71               48.09 
------------------------------------------------------------------------------&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Results&quot;&gt;Results&lt;a class=&quot;anchor-link&quot; href=&quot;#Results&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;So, let's check how it goes. The latest results and implementation are in the following notebooks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/ducha-aiki/brown_phototour_revisited/blob/master/examples/evaluate_deep_descriptors.ipynb&quot;&gt;Deep descriptors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/ducha-aiki/brown_phototour_revisited/blob/master/examples/evaluate_non_deep_descriptors.ipynb&quot;&gt;Non-deep descriptors&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The results are the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;------------------------------------------------------------------------------
Mean Average Precision wrt Lowe SNN ratio criterion on UBC Phototour Revisited
------------------------------------------------------------------------------
trained on       liberty notredame  liberty yosemite  notredame yosemite
tested  on           yosemite           notredame            liberty
------------------------------------------------------------------------------
Kornia RootSIFT 32px   58.24              49.07               49.65 
HardNet 32px       70.64  70.31        61.93  59.56        63.06  61.64
SOSNet 32px        70.03  70.19        62.09  59.68        63.16  61.65
TFeat 32px         65.45  65.77        54.99  54.69        56.55  56.24
SoftMargin 32px    69.29  69.20        61.82  58.61        62.37  60.63
HardNetPS 32px         55.56              49.70               49.12 
R2D2_center_grayscal   61.47              53.18               54.98 
R2D2_MeanCenter_gray   62.73              54.10               56.17 
------------------------------------------------------------------------------

------------------------------------------------------------------------------
Mean Average Precision wrt Lowe SNN ratio criterion on UBC Phototour Revisited
------------------------------------------------------------------------------
trained on       liberty notredame  liberty yosemite  notredame yosemite
tested  on           yosemite           notredame            liberty
------------------------------------------------------------------------------
Kornia SIFT 32px       58.47              47.76               48.70 
OpenCV_SIFT 32px       53.16              45.93               46.00 
Kornia RootSIFT 32px   58.24              49.07               49.65 
OpenCV_RootSIFT 32px   53.50              47.16               47.37 
OpenCV_LATCH 65px  -----  -----        -----  37.26        -----  39.08
OpenCV_LUCID 32px      20.37              23.08               27.24 
skimage_BRIEF 65px     52.68              44.82               46.56 
Kornia RootSIFTPCA 3 60.73  60.64        50.80  50.24        52.46  52.02
MKD-concat-lw-32 32p 72.27  71.95        60.88  58.78        60.68  59.10
------------------------------------------------------------------------------

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So far - in agreement with IMC benchmark: SIFT and RootSIFT are good, but not the best, SOSNet and HardNet are the leaders, but within tens of percents, not by orders of magnitude.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-09-16-local-descriptors-validation_files/att_00005.png&quot; alt=&quot;&quot; title=&quot;Image Matching Benchmark results, from https://arxiv.org/abs/2003.01587&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;Disclaimer-1:-don't-trust-this-tables-fully&quot;&gt;Disclaimer 1: don't trust this tables fully&lt;a class=&quot;anchor-link&quot; href=&quot;#Disclaimer-1:-don't-trust-this-tables-fully&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;I haven't (yet!) checked if all the deep descriptors models, trained on Brown, were trained with flip-rotation 90 degrees augmentation. In the code below I assume that they were, however, it might not be true -- and the comparison might not be completely fair. I will do my best to check it, but if you know that I have used wrong weights - please &lt;a href=&quot;https://github.com/ducha-aiki/brown_phototour_revisited/issues&quot;&gt;open an issue&lt;/a&gt;. Thank you.&lt;/p&gt;
&lt;h3 id=&quot;Disclaimer-2:-it-is-not-&amp;quot;benchmark&amp;quot;.&quot;&gt;Disclaimer 2: it is not &quot;benchmark&quot;.&lt;a class=&quot;anchor-link&quot; href=&quot;#Disclaimer-2:-it-is-not-&amp;quot;benchmark&amp;quot;.&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The intended usage of the package is not to test and report the numbers in the paper. Instead think about is as cross-validation tool, helping the development. Thus, one CAN tune hyperparameters based on the benchmark results  instead of doing so on &lt;a href=&quot;https://github.com/hpatches/hpatches-benchmark&quot;&gt;HPatches&lt;/a&gt;. After you have finished tuning, please, evaluate your local descriptors on some downstream task like &lt;a href=&quot;https://github.com/vcg-uvic/image-matching-benchmark&quot;&gt;IMC image matching benchmark&lt;/a&gt; or &lt;a href=&quot;https://www.visuallocalization.net/&quot;&gt;visual localization&lt;/a&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Summary&quot;&gt;Summary&lt;a class=&quot;anchor-link&quot; href=&quot;#Summary&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;It really pays off, to spend time designing a proper evaluation pipeline and gathering the data for it. If you can re-use existing work - great. 
But don't blindly trust anything, even super-popular and widely adopted benchmarks. You need always check if the the protocol and data makes sense for your use-case personally.&lt;/p&gt;
&lt;p&gt;Thanks for the reading, see you soon!&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Citation&quot;&gt;Citation&lt;a class=&quot;anchor-link&quot; href=&quot;#Citation&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;If you use the benchmark/development set in an academic work, please cite it.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@misc{BrownRevisited2020,
  title={UBC PhotoTour Revisied},
  author={Mishkin, Dmytro},
  year={2020},
  url = {https://github.com/ducha-aiki/brown_phototour_revisited}
}&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;References&quot;&gt;References&lt;a class=&quot;anchor-link&quot; href=&quot;#References&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;[&lt;a id=&quot;cit-IMW2020&quot; href=&quot;#call-IMW2020&quot;&gt;IMW2020&lt;/a&gt;] Jin Yuhe, Mishkin Dmytro, Mishchuk Anastasiia &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Image Matching across Wide Baselines: From Paper to Practice&lt;/em&gt;'', arXiv preprint arXiv:2003.01587, vol. , number , pp. ,  2020.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-pultar2020improving&quot; href=&quot;#call-pultar2020improving&quot;&gt;pultar2020improving&lt;/a&gt;] Pultar Milan, ``&lt;em&gt;Improving the HardNet Descriptor&lt;/em&gt;'', arXiv ePrint:2007.09699, vol. , number , pp. ,  2020.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/wide-baseline-stereo-blog/images/brown_phototour_revisited.jpg" /><media:content medium="image" url="/wide-baseline-stereo-blog/images/brown_phototour_revisited.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">How to match images taken from really extreme viewpoints?</title><link href="/wide-baseline-stereo-blog/2020/08/06/affine-view-synthesis.html" rel="alternate" type="text/html" title="How to match images taken from really extreme viewpoints?" /><published>2020-08-06T00:00:00-05:00</published><updated>2020-08-06T00:00:00-05:00</updated><id>/wide-baseline-stereo-blog/2020/08/06/affine-view-synthesis</id><content type="html" xml:base="/wide-baseline-stereo-blog/2020/08/06/affine-view-synthesis.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-08-06-affine-view-synthesis.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;In-this-post&quot;&gt;In this post&lt;a class=&quot;anchor-link&quot; href=&quot;#In-this-post&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;What to do, if you are in a desperate need of matching this particular image pair?&lt;/li&gt;
&lt;li&gt;What are the limitations of the affine-covariant detectors like Hessian-Affine or HesAffNet?&lt;/li&gt;
&lt;li&gt;ASIFT: brute-force affine view synthesis&lt;/li&gt;
&lt;li&gt;Do as little as possible: MODS&lt;/li&gt;
&lt;li&gt;What is the key factor of affine view synthesis? Ablation study&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;How-to-match-images-taken-from-really-extreme-viewpoints?&quot;&gt;How to match images taken from really extreme viewpoints?&lt;a class=&quot;anchor-link&quot; href=&quot;#How-to-match-images-taken-from-really-extreme-viewpoints?&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Standard wide-baseline stereo or 3d reconstruction pipelines work well in the many situations. Even if some image pair is not matched, it is usually not a problem. For example, one could match images from very different viewpoints, if there is a sequence of images in between, as shown in Figure below, from &quot;From Single Image Query to Detailed 3D Reconstruction&quot; paper[&lt;a class=&quot;latex_cit&quot; id=&quot;call-SingleImage3dRec2015&quot; href=&quot;#cit-SingleImage3dRec2015&quot;&gt;SingleImage3dRec2015&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-08-04-affine-view-synthesis_files/att_00000.png&quot; alt=&quot;&quot; title=&quot;One could handle extreme viewpoint changes by proxy images. Figure from From Single Image Query to Detailed 3D Reconstruction&quot; /&gt;&lt;/p&gt;
&lt;p&gt;However, that might not always be possible. For example, the number of pictures is limited because they historical and there is no way how one could go and take more without inventing a time machine.&lt;/p&gt;
&lt;p&gt;What to do? One way would be to use &lt;a href=&quot;https://ducha-aiki.github.io/wide-baseline-stereo-blog/2020/07/17/affine-correspondences.html&quot;&gt;affine features&lt;/a&gt; like Hessian-AffNet[&lt;a class=&quot;latex_cit&quot; id=&quot;call-AffNet2018&quot; href=&quot;#cit-AffNet2018&quot;&gt;AffNet2018&lt;/a&gt;] or MSER[&lt;a class=&quot;latex_cit&quot; id=&quot;call-MSER2002&quot; href=&quot;#cit-MSER2002&quot;&gt;MSER2002&lt;/a&gt;]. However, they help only up to some extent and what if the view, we need to match are more extreme?&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-08-04-affine-view-synthesis_files/MODS-match-historically.png&quot; alt=&quot;&quot; title=&quot;Non-matchable by standard methods pair of historical photographies. Matched only with help of affine view synthesis in the MODS framework. Images from Location recognition over large time lags dataset&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The image pair above is from &quot;Location recognition over large time lags dataset&quot; paper [&lt;a class=&quot;latex_cit&quot; id=&quot;call-LostInPast2015&quot; href=&quot;#cit-LostInPast2015&quot;&gt;LostInPast2015&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;The solution is to simulate real viewpoint change by affine or perspective warps of the current image. This idea was first proposed by Lepetit and Fua in 2006[&lt;a class=&quot;latex_cit&quot; id=&quot;call-AffineTree2006&quot; href=&quot;#cit-AffineTree2006&quot;&gt;AffineTree2006&lt;/a&gt;]. You can think about it as a special version of test-time augmentation, popular nowadays in deep learning. Later affine view synthesis for wide baseline stereo was extended and mathematically justified by Morel &amp;amp; Yu in ASIFT paper[&lt;a class=&quot;latex_cit&quot; id=&quot;call-ASIFT2009&quot; href=&quot;#cit-ASIFT2009&quot;&gt;ASIFT2009&lt;/a&gt;]. They proved that perspective image warps are can be approximated by synthetic affine views.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;What-is-wrong-with-affine-covariant-local-detectors?&quot;&gt;What is wrong with affine-covariant local detectors?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-wrong-with-affine-covariant-local-detectors?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;One could say that the goal of affine-covariant detectors like MSER, Hessian-Affine or Hessian-AffNet is to detect the same region on a planar surface, regardless the camera angle change.
It is true to some extent, as we demostrate on toy example below with Hessian-Affine feature.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-08-04-affine-view-synthesis_files/together_single.png&quot; alt=&quot;&quot; title=&quot;Hessian-Affine detector detects the same support region under synthetic tilt&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The problem arises, when the image content, e.g. 3 blobs on the figure below are situated close to each other, so under the tilt transform the merge into a single blob. So it is not the shape of region, which is detected incorrectly, but the center of the features themselves. For clarity, we omited affine shape estimation on the image below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-08-04-affine-view-synthesis_files/blobs_merging.png&quot; alt=&quot;&quot; title=&quot;When blobs are close to each other, they merge together when picture is taked from a side viewpoint. This results in failure of the blob detector.&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;ASIFT:-brute-force-affine-view-synthesis&quot;&gt;ASIFT: brute-force affine view synthesis&lt;a class=&quot;anchor-link&quot; href=&quot;#ASIFT:-brute-force-affine-view-synthesis&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;So, to solve the problem explained above, Morel &amp;amp; Yu [&lt;a class=&quot;latex_cit&quot; id=&quot;call-ASIFT2009&quot; href=&quot;#cit-ASIFT2009&quot;&gt;ASIFT2009&lt;/a&gt;] proposed to do a lot affine warps of each image, as shown on the Figure below, as match each view against all others, which is $O(n^2)$ complexity, where $n$ is number of views generated.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-08-04-affine-view-synthesis_files/att_00006.png&quot; alt=&quot;&quot; title=&quot;ASIFT algorithm: generate a lot of synthetic views, match all to all. Figure from Fast Affine Invariant Image Matching&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The motivation do doing so it that assuming, original image to be a fronto-parallel one, to cover viewsphere really dense, as shown in the Figure below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-08-04-affine-view-synthesis_files/att_00007.png&quot; alt=&quot;&quot; title=&quot;Viewsphere covering by ASIFT. Figure from Fast Affine Invariant Image Matching&quot; /&gt;&lt;/p&gt;
&lt;p&gt;This leads to impressive performance on a very challenging image pairs, see an example below&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-08-04-affine-view-synthesis_files/att_00009.png&quot; alt=&quot;image.png&quot; title=&quot;ASIFT find a lot of correspondences on challenging pair. Figure from ASIFT: An Algorithm for Fully Affine Invariant Comparison&quot; /&gt;&lt;/p&gt;
&lt;p&gt;In this section I have used great illustrations done by &lt;a href=&quot;https://rdguez-mariano.github.io/&quot;&gt;Mariano Rodríguez&lt;/a&gt; for his paper &quot;Fast Affine Invariant Image Matching&quot; [&lt;a class=&quot;latex_cit&quot; id=&quot;call-FastASIFT2018&quot; href=&quot;#cit-FastASIFT2018&quot;&gt;FastASIFT2018&lt;/a&gt;]. Please, checkout his &lt;a href=&quot;https://rdguez-mariano.github.io/&quot;&gt;blog&lt;/a&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;MODS:-do-as-little-as-possible&quot;&gt;MODS: do as little as possible&lt;a class=&quot;anchor-link&quot; href=&quot;#MODS:-do-as-little-as-possible&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The main drawback of ASIFT algorithm is a huge computational cost: 82 views are generated regardless of the image pair difficulty. To overcome this, we proposed MODS[&lt;a class=&quot;latex_cit&quot; id=&quot;call-MODS2015&quot; href=&quot;#cit-MODS2015&quot;&gt;MODS2015&lt;/a&gt;] algorithm: Matching with On-Demand Synthesis.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-08-04-affine-view-synthesis_files/att_00004.png&quot; alt=&quot;&quot; title=&quot;MODS algorithm: synthetize more views until match&quot; /&gt;&lt;/p&gt;
&lt;p&gt;One starts with the fastest detector-descriptor without view synthesys and then uses more and more computationally expensive methods if needed. Moreover, by using affine-covariant detectors like MSER or Hessian-Affine, one could synthetise significantly less views, saving computations spent on local descriptor and matching.&lt;/p&gt;
&lt;p&gt;This, together with &lt;a href=&quot;https://medium.com/@ducha.aiki/how-to-match-to-learn-or-not-to-learn-part-2-1ab52ede2022&quot;&gt;FGINN matching strategy&lt;/a&gt;, specifically designed for the handling re-detections, MODS is able to match more challenging image pairs in less time than ASIFT.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-08-04-affine-view-synthesis_files/att_00010.png&quot; alt=&quot;&quot; title=&quot;MODS outperforms ASIFT on a several dataset both in terms of speed and quality&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Why-does-affine-synthesis-help?&quot;&gt;Why does affine synthesis help?&lt;a class=&quot;anchor-link&quot; href=&quot;#Why-does-affine-synthesis-help?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Despite that ASIFT and other view-synthesis based approaches are know more than decade, we are not aware of a study, why does affine synthesis helps in practice. Could one get a similar performance without view synthesis?
Specificallly:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;May it be that the most of improvements come from the fact that we have much more features? That is why we fix the number of features for all approaches.&lt;/li&gt;
&lt;li&gt;Some regions from ASIFT, when reprojected to the original image, are quite narrow. Could we get them just by removing edge-like feature filtering, which is done in SIFT, Hessian and other detectors.  Denoted &lt;strong&gt;+edge&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Instead of doing affine view synthesis, one could directly use the same affine parameters to get the affine regions to describe, so the each keypoint would have several associated regions+descriptors. Denoted &lt;strong&gt;+MD&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Using AffNet to directly estimated local affine shape without multiple descriptors. Denoted &lt;strong&gt;+AffNet&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Combine (1), (2) and (3).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So, we did the study on HPatches Sequences dataset, the hardest image pairs (1-6) of viewpoint subset. The metric is similar to one used in the &quot;&lt;a href=&quot;https://arxiv.org/abs/2003.01587&quot;&gt;Image Matching across Wide Baselines: From Paper to Practice&lt;/a&gt;&quot; and CVPR 2020 &lt;a href=&quot;http://cmp.felk.cvut.cz/cvpr2020-ransac-tutorial/&quot;&gt;RANSAC in 2020&lt;/a&gt; - mean average accuracy of the estimated homography.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-08-04-affine-view-synthesis_files/att_00011.png&quot; alt=&quot;&quot; title=&quot; mean average accuracy of the estimated homography - used metric&quot; /&gt;&lt;/p&gt;
&lt;p&gt;We run Hessian detector with RootSIFT descriptor, FLANN matching and LO-RANSAC, as implemented in &lt;a href=&quot;https://github.com/ducha-aiki/mods-light-zmq&quot;&gt;MODS&lt;/a&gt;. Features are sorted according the the detector response and their total number is clipped to 2048 or 8000 to ensure that the improvements do not come from just having more features.&lt;/p&gt;
&lt;p&gt;Note, that we do not study, if view synthesis helps for the regular image pairs - it might actually hurt performance, similarly to &lt;a href=&quot;https://ducha-aiki.github.io/wide-baseline-stereo-blog/2020/07/17/affine-correspondences.html&quot;&gt;affine features&lt;/a&gt;. 
Instead we are focusing on the case, when view synthesis definitely helps: matching obscure views of the mostly planar scenes.&lt;/p&gt;
&lt;h3 id=&quot;8000-feature-budget&quot;&gt;8000 feature budget&lt;a class=&quot;anchor-link&quot; href=&quot;#8000-feature-budget&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Results are in Figure below. Indeed, all of the factors: detecting more edge-like features, having multiple descriptors or better affine shape improve results over the plain Hessian detector, but even all of the combined are not good enough to match performance of the affine view synthesis + plain Hessian detector.&lt;/p&gt;
&lt;p&gt;But the best setup is to use both Hessian-AffNet and view synthesis.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-08-04-affine-view-synthesis_files/8k_budget.png&quot; alt=&quot;&quot; title=&quot;Ablation study of the different benefit from the affine view synthesis: ability to detect elongated blobs, multiple affine decriptors or everything together. Feature budget: 8k&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;2048-feature-budget&quot;&gt;2048 feature budget&lt;a class=&quot;anchor-link&quot; href=&quot;#2048-feature-budget&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The picture is a bit different in a small feature budget: neither multiple-(affine)-descriptors per keypoint, nor allowing edge-like feature help. From other hand, affine view synthesis still improves results of the Hessian. And, again, the best performance is achieved with combination of view synthesis and AffNet shape estimation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-08-04-affine-view-synthesis_files/2k_budget.png&quot; alt=&quot;&quot; title=&quot;Ablation study of the different benefit from the affine view synthesis: ability to detect elongated blobs, multiple affine decriptors or everything together. Feature budget: 8k&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;Summary&quot;&gt;Summary&lt;a class=&quot;anchor-link&quot; href=&quot;#Summary&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Affine view synthesis helps for matching challenging image pairs and its improvement are not just because of more local features used. It can be done effective and efficient -- in the iterative MODS framework.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;References&quot;&gt;References&lt;a class=&quot;anchor-link&quot; href=&quot;#References&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;[&lt;a id=&quot;cit-SingleImage3dRec2015&quot; href=&quot;#call-SingleImage3dRec2015&quot;&gt;SingleImage3dRec2015&lt;/a&gt;] J.L. Schonberger, F. Radenovic, O. Chum &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;From Single Image Query to Detailed 3D Reconstruction&lt;/em&gt;'', Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-AffNet2018&quot; href=&quot;#call-AffNet2018&quot;&gt;AffNet2018&lt;/a&gt;] D. Mishkin, F. Radenovic and J. Matas, ``&lt;em&gt;Repeatability is Not Enough: Learning Affine Regions via Discriminability&lt;/em&gt;'', ECCV,  2018.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-MSER2002&quot; href=&quot;#call-MSER2002&quot;&gt;MSER2002&lt;/a&gt;] J. Matas, O. Chum, M. Urban &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Robust Wide Baseline Stereo from Maximally Stable Extrema Regions&lt;/em&gt;'', BMVC,  2002.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-LostInPast2015&quot; href=&quot;#call-LostInPast2015&quot;&gt;LostInPast2015&lt;/a&gt;] Fernando Basura, Tommasi Tatiana and Tuytelaars Tinne, ``&lt;em&gt;Location recognition over large time lags&lt;/em&gt;'', Computer Vision and Image Understanding, vol. 139, number , pp. ,  2015.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-AffineTree2006&quot; href=&quot;#call-AffineTree2006&quot;&gt;AffineTree2006&lt;/a&gt;] Lepetit Vincent and Fua Pascal, ``&lt;em&gt;Keypoint Recognition Using Randomized Trees&lt;/em&gt;'', IEEE Trans. Pattern Anal. Mach. Intell., vol. 28, number 9, pp. , sep 2006.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-ASIFT2009&quot; href=&quot;#call-ASIFT2009&quot;&gt;ASIFT2009&lt;/a&gt;] Morel Jean-Michel and Yu Guoshen, ``&lt;em&gt;ASIFT: A New Framework for Fully Affine Invariant Image Comparison&lt;/em&gt;'', SIAM J. Img. Sci., vol. 2, number 2, pp. , apr 2009.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-FastASIFT2018&quot; href=&quot;#call-FastASIFT2018&quot;&gt;FastASIFT2018&lt;/a&gt;] Rodríguez Mariano, Delon Julie and Morel Jean-Michel, ``&lt;em&gt;Fast Affine Invariant Image Matching&lt;/em&gt;'', Image Processing On Line, vol. 8, number , pp. ,  2018.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-MODS2015&quot; href=&quot;#call-MODS2015&quot;&gt;MODS2015&lt;/a&gt;] Mishkin Dmytro, Matas Jiri and Perdoch Michal, ``&lt;em&gt;MODS: Fast and robust method for two-view matching &lt;/em&gt;'', Computer Vision and Image Understanding , vol. , number , pp. ,  2015.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/wide-baseline-stereo-blog/images/transition.png" /><media:content medium="image" url="/wide-baseline-stereo-blog/images/transition.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Patch extraction: devil in details</title><link href="/wide-baseline-stereo-blog/2020/07/22/patch-extraction.html" rel="alternate" type="text/html" title="Patch extraction: devil in details" /><published>2020-07-22T00:00:00-05:00</published><updated>2020-07-22T00:00:00-05:00</updated><id>/wide-baseline-stereo-blog/2020/07/22/patch-extraction</id><content type="html" xml:base="/wide-baseline-stereo-blog/2020/07/22/patch-extraction.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-07-22-patch-extraction.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;When working with local features one needs to pay attention to even a smallest details, or the whole process can be ruined. One of such details is how to extract the patch, which will be described by local descriptor such as SIFT or HardNet.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-22-patch-extraction_files/att_00000.png&quot; alt=&quot;&quot; title=&quot;In order to describe the local patch one first has to extract it. Figure from A Few Things One Should Know About Feature Extraction, Description and Matching&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Unfortunately, we cannot just extract patch from the image by cropping the patch and then resizing it. Or can we? 
Let's check. We will use two versions of image: original and 4x smaller one and would like to extract same-looking fixed size patch from both of them. The patch we want to crop is showed by oriented red circle.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-22-patch-extraction_files/both_imgs.png&quot; alt=&quot;&quot; title=&quot;Two versions of the same image: original and 4x smaller&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Aliasing&quot;&gt;Aliasing&lt;a class=&quot;anchor-link&quot; href=&quot;#Aliasing&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;And here what we get by doing a simple crop and resize to 32x32 pixels.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-22-patch-extraction_files/naive_patches.png&quot; alt=&quot;&quot; title=&quot;Patches, which are extracted from images of different sizes look different, if extraction done in a naive way&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Doesn't look good. It is called &quot;&lt;a href=&quot;https://en.wikipedia.org/wiki/Aliasing&quot;&gt;aliasing&lt;/a&gt;&quot; - a problem, which arise when we are trying to downscale big images into small resolution. Specifically: the original image contains finer details, than we could represent in thumbnail, which leads to artifacts. 
&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-22-patch-extraction_files/ep.jpg&quot; alt=&quot;&quot; title=&quot;One does not simply extract patch from the image&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;The-solution:-anti-aliasing&quot;&gt;The solution: anti-aliasing&lt;a class=&quot;anchor-link&quot; href=&quot;#The-solution:-anti-aliasing&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The solution,  which follows out of &lt;a href=&quot;https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem&quot;&gt;sampling theorem&lt;/a&gt; is known: remove the details, which cannot be seens in small image first, then resample image to small size.&lt;/p&gt;
&lt;p&gt;The simplest way to remove the fine details is to blur image with the Gaussian kernel.&lt;/p&gt;
&lt;p&gt;Lets do it and compare the results. 
&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-22-patch-extraction_files/all_patches.png&quot; alt=&quot;&quot; title=&quot;When extracted properly, patches look same&quot; /&gt;&lt;/p&gt;
&lt;p&gt;By the way, you can try for yourself, all the required code is &lt;a href=&quot;https://github.com/kornia/kornia-examples/blob/master/aliased-and-not-aliased-patch-extraction.ipynb&quot;&gt;here, in kornia-examples&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Performance&quot;&gt;Performance&lt;a class=&quot;anchor-link&quot; href=&quot;#Performance&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The problem is solved. Or is it?&lt;/p&gt;
&lt;p&gt;The problem with properly antialiased patch extraction is that it is quite slow for two reasons. First, blurring a whole image is a costly operation. But, the worst part is that the required amount of blur depends on the patch size in the original image, or, in other words, keypoint scale. So for extracting, say 8000 patches, one needs to perform blurring 8000 times. Moreover, if one wants to extract elongated region and warp it to the square patch, the amount of blur in vertical and horizontal directions should be different!&lt;/p&gt;
&lt;p&gt;What can be done? Well, instead of doing blurring 8000 times, one could create so called scale pyramid and then pick the level, which is the closest to optimal one, predicted by theorem.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-22-patch-extraction_files/Image_pyramid.png&quot; alt=&quot;&quot; title=&quot;Image from Wikimedia by wiki-user Cmglee https://en.wikipedia.org/wiki/File:Image_pyramid.svg&quot; /&gt;&lt;/p&gt;
&lt;p&gt;This is exactly, what kornia function &lt;a href=&quot;https://kornia.readthedocs.io/en/latest/feature.html#kornia.feature.extract_patches_from_pyramid&quot;&gt;extract_patches_from_pyramid&lt;/a&gt; does.&lt;/p&gt;
&lt;p&gt;Also - I have a bit cheated with you above: the &quot;anti-aliased&quot; patches were actually extracted using the function above.&lt;/p&gt;
&lt;h2 id=&quot;How-it-impacts-local-descriptor-matching?&quot;&gt;How it impacts local descriptor matching?&lt;a class=&quot;anchor-link&quot; href=&quot;#How-it-impacts-local-descriptor-matching?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Let's do the toy example first - describe four patches we have in the example above with HardNet descriptor and calculate the distance between them.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-22-patch-extraction_files/att_00001.png&quot; alt=&quot;&quot; title=&quot;Descriptor difference between antialiased patches is 0.09 and between naively extracted -- 0.44&quot; /&gt;&lt;/p&gt;
&lt;p&gt;So the descriptor difference between antialiased patches is 0.09 and between naively extracted -- 0.44. 0.09 is not a big deal, but 0.44 is a lot, actually.&lt;/p&gt;
&lt;p&gt;Let's move to the non-toy example from the paper devoted to this topic: &quot;&lt;a href=&quot;http://cmp.felk.cvut.cz/~mishkdmy/lenc-2014-features-cvww.pdf&quot;&gt;A Few Things One Should Know About Feature Extraction, Description and
Matching&lt;/a&gt;&quot;[&lt;a class=&quot;latex_cit&quot; id=&quot;call-PatchExtraction2014&quot; href=&quot;#cit-PatchExtraction2014&quot;&gt;PatchExtraction2014&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;The original data is lost I am too lazy to redo the experiments for the post, so I will just copy-past images with results. Here are abbrevations used in the paper:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;OPE&lt;/strong&gt; -- Optimal Patch Extraction. The most correct and slow way of extracting, including different amount of bluring in different directions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;NBPE&lt;/strong&gt; -- No-Blur Patch Extraction. The most naive way we started with&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;PNBPE&lt;/strong&gt; -- Pyramid, No-Blur Patch Extraction. The one, we described above - sampling patches from scale pyramid.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;PSPE&lt;/strong&gt; Pyramid-Smoothing Patch Extraction. Pick the matching pyramid level and then add anisotropic blur missing.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As you can see, doing things optimally is quite slow. 
&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-22-patch-extraction_files/att_00003.png&quot; alt=&quot;image.png&quot; title=&quot;Time spent on a various stages of local feature extraction&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Now let's see how it influences performance.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-22-patch-extraction_files/att_00005.png&quot; alt=&quot;&quot; title=&quot;Number of matches for MSER and Hessian-Affine + SIFT on Grffity sequence of Oxford-Affine dataset for different patch extraction methods&quot; /&gt;&lt;/p&gt;
&lt;p&gt;It looks like that influence is smaller than we thought. But recall that the experiment above is for SIFT descriptor only.  Doing pyramid helps for the small viewpoint change almost as good, as going fully optimal, but with increasing the viewpoint difference, such approximation degrades. Moreover, it influnces MSER detector much more that than Hessian-Affine.&lt;/p&gt;
&lt;p&gt;How does it work with deep descriptors like HardNet or SoSNet?  That is the question which not answered yet. Drop me a message if you want to do it yourself and we can do the follow-up post together.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;References&quot;&gt;References&lt;a class=&quot;anchor-link&quot; href=&quot;#References&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;[&lt;a id=&quot;cit-PatchExtraction2014&quot; href=&quot;#call-PatchExtraction2014&quot;&gt;PatchExtraction2014&lt;/a&gt;] K. Lenc, J. Matas and D. Mishkin, ``&lt;em&gt;A few things one should know about feature extraction, description and matching&lt;/em&gt;'', Proceedings of the Computer Vision Winter Workshop,  2014.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/wide-baseline-stereo-blog/images/ep.jpg" /><media:content medium="image" url="/wide-baseline-stereo-blog/images/ep.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Local affine features: useful side product</title><link href="/wide-baseline-stereo-blog/2020/07/17/affine-correspondences.html" rel="alternate" type="text/html" title="Local affine features: useful side product" /><published>2020-07-17T00:00:00-05:00</published><updated>2020-07-17T00:00:00-05:00</updated><id>/wide-baseline-stereo-blog/2020/07/17/affine-correspondences</id><content type="html" xml:base="/wide-baseline-stereo-blog/2020/07/17/affine-correspondences.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-07-17-affine-correspondences.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Keypoints-are-not-just-points&quot;&gt;Keypoints are not just points&lt;a class=&quot;anchor-link&quot; href=&quot;#Keypoints-are-not-just-points&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/att_00000.png&quot; alt=&quot;image.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Wide baseline stereo matching often as perceived as establishing (key-)point correspondences between images. While this might be true for the some local features like SuperPoint [&lt;a class=&quot;latex_cit&quot; id=&quot;call-SuperPoint2017&quot; href=&quot;#cit-SuperPoint2017&quot;&gt;SuperPoint2017&lt;/a&gt;], typically it is more than that.&lt;/p&gt;
&lt;p&gt;Specifically, detectors like DoG[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Lowe99&quot; href=&quot;#cit-Lowe99&quot;&gt;Lowe99&lt;/a&gt;], Harris[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Harris88&quot; href=&quot;#cit-Harris88&quot;&gt;Harris88&lt;/a&gt;], Hessian[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Hessian78&quot; href=&quot;#cit-Hessian78&quot;&gt;Hessian78&lt;/a&gt;], KeyNet[&lt;a class=&quot;latex_cit&quot; id=&quot;call-KeyNet2019&quot; href=&quot;#cit-KeyNet2019&quot;&gt;KeyNet2019&lt;/a&gt;], ORB[&lt;a class=&quot;latex_cit&quot; id=&quot;call-ORB2011&quot; href=&quot;#cit-ORB2011&quot;&gt;ORB2011&lt;/a&gt;], and many others rate on scale-space provide at least 3 parameters: x, y, and scale.&lt;/p&gt;
&lt;p&gt;Most of the local descriptors  -- SIFT[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Lowe99&quot; href=&quot;#cit-Lowe99&quot;&gt;Lowe99&lt;/a&gt;], HardNet[&lt;a class=&quot;latex_cit&quot; id=&quot;call-HardNet2017&quot; href=&quot;#cit-HardNet2017&quot;&gt;HardNet2017&lt;/a&gt;] and so on -- are not rotation invariant and those which are - mostly require complex matching function[&lt;a class=&quot;latex_cit&quot; id=&quot;call-RIFT2005&quot; href=&quot;#cit-RIFT2005&quot;&gt;RIFT2005&lt;/a&gt;], [&lt;a class=&quot;latex_cit&quot; id=&quot;call-sGLOH2&quot; href=&quot;#cit-sGLOH2&quot;&gt;sGLOH2&lt;/a&gt;], so the patch orientation has to be estimated anyway, in order to match reliably. This can be done by various methods:
corners center of mass (ORB[&lt;a class=&quot;latex_cit&quot; id=&quot;call-ORB2011&quot; href=&quot;#cit-ORB2011&quot;&gt;ORB2011&lt;/a&gt;], dominant gradient orientation (SIFT)[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Lowe99&quot; href=&quot;#cit-Lowe99&quot;&gt;Lowe99&lt;/a&gt;] or by some learned estimator (OriNets[&lt;a class=&quot;latex_cit&quot; id=&quot;call-OriNet2016&quot; href=&quot;#cit-OriNet2016&quot;&gt;OriNet2016&lt;/a&gt;],[&lt;a class=&quot;latex_cit&quot; id=&quot;call-AffNet2018&quot; href=&quot;#cit-AffNet2018&quot;&gt;AffNet2018&lt;/a&gt;]). Sometimes it is possible to rely on smartphone camera IMU or photographer and assume that images are upright[&lt;a class=&quot;latex_cit&quot; id=&quot;call-PerdochRetrieval2009&quot; href=&quot;#cit-PerdochRetrieval2009&quot;&gt;PerdochRetrieval2009&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;Thus, we can assume that if local descriptors match, this means the local feature scale and orientation also match, at least approximately -- see Figure below. Possible exceptions are cases, when the patch is symmetrical and orientation is ambiguous up to some symmetry group.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/matches_patches.png&quot; alt=&quot;&quot; title=&quot;Selected matching SIFT keypoints and corresponding patches. One can see that not only patch centers correspond to each other, but also other pixels, although less precise. Image pair from Sacre Coeur IMW dataset.&quot; /&gt;&lt;/p&gt;
&lt;p&gt;In addition, one could assume that we observe the patch not from the fronto-parallel position and try to estimate local normal, or, more precisely, affine shape of the feature point, modeling it as an ellipse instead of circle. One could also think of affine shape estimation as finding the camera position, from where the patch is seen in some &quot;canonical&quot; view.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/affinematches_patches.png&quot; alt=&quot;&quot; title=&quot;Selected matching SIFT-AffNet keypoints and corresponding patches. One can see that not only patch centers correspond to each other, but also other pixels, although less precise. Image pair from Sacre Coeur IMW dataset.&quot; /&gt;&lt;/p&gt;
&lt;p&gt;This gives us 3 points correspondences from a single local feature match, see an example in Figure below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/laf-check-illustration.png&quot; alt=&quot;&quot; title=&quot;Local affine correspondences. While centers of both regions A and B are correct point matches, only A is a correct affine correspondence.&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Why is it important and how to use it -- see in current post. How to esimate local affine features robustly -- in the next post.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Benefits-of-local-affine-features&quot;&gt;Benefits of local affine features&lt;a class=&quot;anchor-link&quot; href=&quot;#Benefits-of-local-affine-features&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Making-descriptor-job-easier&quot;&gt;Making descriptor job easier&lt;a class=&quot;anchor-link&quot; href=&quot;#Making-descriptor-job-easier&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The most straightforward benefit of using local affine features is that they increase the repeatability of the detector and potentially reduce appearance changes of a local patch caused by viewpoint difference. This makes possible matching more challenging image pairs.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/test.png&quot; alt=&quot;&quot; title=&quot;Hessian features + HardNet matches + RANSAC inliers, Right: HessAffNet features + HardNet matches + RANSAC inliers. Image pair from Tanks &amp;amp; Temples. Epipolar lines are shown in cyan.&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/att_00009.png&quot; alt=&quot;&quot; title=&quot;Repeatability and the number of correspondences. AffNet compared with the de facto standard Baumberg iteration according to the Mikolajczyk protocol. Left – images with illumination differences, right – with viewpoint and scale changes. SS – patch from the scale-space pyramid at the level of the detection, image – from the original image; 19 and 33 are patch sizes.&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The practice is a little bit more complicated. Our recent benchmark[&lt;a class=&quot;latex_cit&quot; id=&quot;call-IMW2020&quot; href=&quot;#cit-IMW2020&quot;&gt;IMW2020&lt;/a&gt;], which measure the accuracy of the output fundamental matrix, shows that the difference in using affine and similarity-covariant features is quite minor. Specifically, the relative difference between SIFT vs SIFT-Affine features is 0.5% and between Hessian-SIFT and Hessian-AffNet SIFT is 5.1%, see Table below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/att_00011.png&quot; alt=&quot;&quot; title=&quot;Test – Stereo results with 8k features. We report: NF --  Number of Features; NI -- Number of Inliers produced by RANSAC; and mAA@10°.&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Therefore, if the benefit of local features would be to only improve descriptor extraction stage, it would be arguably not worth it. Luckily, there are more benefits, which are more pronounced.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Making-RANSAC-job-easier&quot;&gt;Making RANSAC job easier&lt;a class=&quot;anchor-link&quot; href=&quot;#Making-RANSAC-job-easier&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Let's recall how RANSAC works.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Randomly sample a minimally required number of tentative correspondences to fit the geometrical model of the scene: 4 for homography, 7 for epipolar geometry and estimate the model.&lt;/li&gt;
&lt;li&gt;Calculate &quot;support&quot;: other correspondeces, which are consistent with the model. &lt;/li&gt;
&lt;li&gt;Repeat steps (1), (2) and output the model which is supported with the most of correspondences. If you were lucky and have sampled all-inlier sample, meaning that all correspondences used to estimate the model were correct, you would have a correct model.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Reality is more complicated than I have just described, but the principle is the same. 
The most important part is the sampling and it is sensitive to inlier ratio $\nu$ - the percentage of the correct correspondences in the set. Lets denote the minimal number of correspondences required to estimate the model as &lt;strong&gt;m&lt;/strong&gt;.
To recover the correct model with the confidence &lt;strong&gt;p&lt;/strong&gt; one needs to sample the number of correspondences, which is described by formula:&lt;/p&gt;
\begin{equation}
N = \frac{\log{(1 - p)}}{\log{(1 - \nu^{m})}}
\end{equation}&lt;p&gt;Lets plot the how the number of required samples changes with inlier ratio for confidence equal 99%. Note the log scale on Y axis. Different lines are for different minimal sample size &lt;strong&gt;m&lt;/strong&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/att_00016.png&quot; alt=&quot;&quot; title=&quot;Number of samples to find correct model as a function of inlier ratio.&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;As you can see from the plot above, reducing the minimal sample size required for the model estimation even by 1 saves and order of magnitude of computation. 
In reality the benefit is a smaller, as modern RANSACs like GC-RANSAC[&lt;a class=&quot;latex_cit&quot; id=&quot;call-gcransac2018&quot; href=&quot;#cit-gcransac2018&quot;&gt;gcransac2018&lt;/a&gt;] and MAGSAC[&lt;a class=&quot;latex_cit&quot; id=&quot;call-magsac2019&quot; href=&quot;#cit-magsac2019&quot;&gt;magsac2019&lt;/a&gt;] could estimate the correct model from the sample containing outliers, but it is still huge, especially for low inlier rate cases.&lt;/p&gt;
&lt;h3 id=&quot;Image-retrieval&quot;&gt;Image retrieval&lt;a class=&quot;anchor-link&quot; href=&quot;#Image-retrieval&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The ideal case would be to estimate a model from just a single sample and that is exactly what is done in spatial reranking paper &quot;&lt;strong&gt;Object retrieval with large vocabularies and fast spatial matching&lt;/strong&gt;&quot;  by Philbin et.al [&lt;a class=&quot;latex_cit&quot; id=&quot;call-Philbin07&quot; href=&quot;#cit-Philbin07&quot;&gt;Philbin07&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;Specifically, they are solving a particular object retrieval problem: given an image containing some object, return all the images from the database, which also containg the same object.&lt;/p&gt;
&lt;p&gt;The inital list of images is formed by the descriptor distance and then is reranked.
The authors propose to approximate a perspective change between two images as an affine image transformation, and count number of feature points, which are reprojected inside the second image. This number produces better ranking that the original short-list.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/att_00012.png&quot; alt=&quot;&quot; title=&quot;Figure from  Philbin et.al&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Back-to-wide-baseline-stereo&quot;&gt;Back to wide baseline stereo&lt;a class=&quot;anchor-link&quot; href=&quot;#Back-to-wide-baseline-stereo&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;While working for spatial re-ranking, 3-degrees of freedom camera model is too rough for the wide baseline stereo.
Yet, going from 4 point correspondences (PC) to 2 affine correspondences (AC) for homogaphy and from 7 PC to 3 AC for the fundamental matrix would be huge benefit anyway for the robust model estimation.&lt;/p&gt;
&lt;p&gt;Various variant of RANSAC working for local features were proposed in the last 15 years: Perdoch et.al[&lt;a class=&quot;latex_cit&quot; id=&quot;call-perd2006epipolar&quot; href=&quot;#cit-perd2006epipolar&quot;&gt;perd2006epipolar&lt;/a&gt;], Pritts et.al.[&lt;a class=&quot;latex_cit&quot; id=&quot;call-PrittsRANSAC2013&quot; href=&quot;#cit-PrittsRANSAC2013&quot;&gt;PrittsRANSAC2013&lt;/a&gt;], Barath and Kukelova [&lt;a class=&quot;latex_cit&quot; id=&quot;call-Barath2019ICCV&quot; href=&quot;#cit-Barath2019ICCV&quot;&gt;Barath2019ICCV&lt;/a&gt;], Rodríguez et.al[&lt;a class=&quot;latex_cit&quot; id=&quot;call-RANSACAffine2020&quot; href=&quot;#cit-RANSACAffine2020&quot;&gt;RANSACAffine2020&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;Finally, the systematic study of using is presented by Barath et.al[&lt;a class=&quot;latex_cit&quot; id=&quot;call-barath2020making&quot; href=&quot;#cit-barath2020making&quot;&gt;barath2020making&lt;/a&gt;] in &quot;Making Affine Correspondences Work in Camera Geometry Computation&quot; paper. Authors show that if used naively, affine correspondence lead to worse results, because they are more noisy than point correspondences. 
However, there is a bag of tricks presented in the paper, which allow to solve the noise issue and make the affine RANSAC working in practice, resulting in orders of magnitude faster computation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/att_00013.png&quot; alt=&quot;&quot; title=&quot;Figure from Making Affine Correspondences Work in Camera Geometry Computation&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Moreover, for a special cases like autonomous driving, where the motion is mostly horizonal, one could even use 2 affine correspondes for both motion estimation and consistency check, significantly improving the efficiency of the outliers removal compared to the standard RANSAC loop[&lt;a class=&quot;latex_cit&quot; id=&quot;call-guan2020relative&quot; href=&quot;#cit-guan2020relative&quot;&gt;guan2020relative&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/att_00001.png&quot; alt=&quot;&quot; title=&quot;Empirical cumulative error distributions for KITTI sequence 00. Figure from Relative Pose Estimation for Multi-Camera Systems from Affine Correspondences.&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Besides the special case considerations, additional contraints can also come from running other algorithms, like monocular depth estimation. Such a constraint could reduce the required number of matches from two affine correspondences to a single one for calibrated camera case[&lt;a class=&quot;latex_cit&quot; id=&quot;call-OneACMonoDepth2020&quot; href=&quot;#cit-OneACMonoDepth2020&quot;&gt;OneACMonoDepth2020&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/att_00002.png&quot; alt=&quot;&quot; title=&quot;Figure from Relative Pose from Deep Learned Depth and a Single Affine Correspondence&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/att_00003.png&quot; alt=&quot;&quot; title=&quot;Figure from Relative Pose from Deep Learned Depth and a Single Affine Correspondence&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Application-specific-benefits&quot;&gt;Application-specific benefits&lt;a class=&quot;anchor-link&quot; href=&quot;#Application-specific-benefits&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Besides the wide baseline stereo, local affine features and correspondences have other applications. I will briefly describe some of them here (to be updated).&lt;/p&gt;
&lt;h3 id=&quot;Image-rectification&quot;&gt;Image rectification&lt;a class=&quot;anchor-link&quot; href=&quot;#Image-rectification&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Instead of matching local features between two images one might match them within a single image. Why would someone do it? This allows finding repeated pattern: think about windows, doors and so on. Typically they have the same physical size, therefore the diffrence in local features around them could tell us about the geometry of the scene and lens distortion.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/att_00008.png&quot; alt=&quot;&quot; title=&quot;Repeated patterns detection with MSER andRootSIFT local features. Figure from the Minimal Solvers for Rectifying from Radially-Distorted Conjugate Translations, PAMI 2020 paper.&quot; /&gt;&lt;/p&gt;
&lt;p&gt;This is the idea of the series of works by Pritts and Chum.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/att_00007.png&quot; alt=&quot;&quot; title=&quot;Figure from the &amp;#39;Minimal Solvers for Rectifying from Radially-Distorted Conjugate Translations&amp;#39;, PAMI 2020 paper.&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;Surface-normals-estimation&quot;&gt;Surface normals estimation&lt;a class=&quot;anchor-link&quot; href=&quot;#Surface-normals-estimation&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Ivan Eichhardt and Levente Hajder have a series of works, exploiting the local affine correspondences for surface normals estimation[&lt;a class=&quot;latex_cit&quot; id=&quot;call-SurfaceNormals2019&quot; href=&quot;#cit-SurfaceNormals2019&quot;&gt;SurfaceNormals2019&lt;/a&gt;]&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-07-17-affine-correspondences_files/att_00015.png&quot; alt=&quot;&quot; title=&quot;Estimated surface normals. Figure from Optimal Multi-View Surface Normal Estimation Using Affine Correspondences&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Summary&quot;&gt;Summary&lt;a class=&quot;anchor-link&quot; href=&quot;#Summary&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Despite not being popular right now, treating keypoints as local affine features has a lot of advantages over the traditional treatment the local correspondence as the point correspondences. In the next post I will describe a way of estimating the local feature affine shape and orientation.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;References&quot;&gt;References&lt;a class=&quot;anchor-link&quot; href=&quot;#References&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;[&lt;a id=&quot;cit-SuperPoint2017&quot; href=&quot;#call-SuperPoint2017&quot;&gt;SuperPoint2017&lt;/a&gt;] Detone D., Malisiewicz T. and Rabinovich A., ``&lt;em&gt;Superpoint: Self-Supervised Interest Point Detection and Description&lt;/em&gt;'', CVPRW Deep Learning for Visual SLAM, vol. , number , pp. ,  2018.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Lowe99&quot; href=&quot;#call-Lowe99&quot;&gt;Lowe99&lt;/a&gt;] D. Lowe, ``&lt;em&gt;Object Recognition from Local Scale-Invariant Features&lt;/em&gt;'', ICCV,  1999.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Harris88&quot; href=&quot;#call-Harris88&quot;&gt;Harris88&lt;/a&gt;] C. Harris and M. Stephens, ``&lt;em&gt;A Combined Corner and Edge Detector&lt;/em&gt;'', Fourth Alvey Vision Conference,  1988.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Hessian78&quot; href=&quot;#call-Hessian78&quot;&gt;Hessian78&lt;/a&gt;] P.R. Beaudet, ``&lt;em&gt;Rotationally invariant image operators&lt;/em&gt;'', Proceedings of the 4th International Joint Conference on Pattern Recognition,  1978.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-KeyNet2019&quot; href=&quot;#call-KeyNet2019&quot;&gt;KeyNet2019&lt;/a&gt;] A. Barroso-Laguna, E. Riba, D. Ponsa &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Key.Net: Keypoint Detection by Handcrafted and Learned CNN Filters&lt;/em&gt;'', ICCV,  2019.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-ORB2011&quot; href=&quot;#call-ORB2011&quot;&gt;ORB2011&lt;/a&gt;] E. Rublee, V. Rabaud, K. Konolidge &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;ORB: An Efficient Alternative to SIFT or SURF&lt;/em&gt;'', ICCV,  2011.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-HardNet2017&quot; href=&quot;#call-HardNet2017&quot;&gt;HardNet2017&lt;/a&gt;] A. Mishchuk, D. Mishkin, F. Radenovic &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Working Hard to Know Your Neighbor's Margins: Local Descriptor Learning Loss&lt;/em&gt;'', NeurIPS,  2017.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-RIFT2005&quot; href=&quot;#call-RIFT2005&quot;&gt;RIFT2005&lt;/a&gt;] {Lazebnik} S., {Schmid} C. and {Ponce} J., ``&lt;em&gt;A sparse texture representation using local affine regions&lt;/em&gt;'', IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 27, number 8, pp. 1265-1278,  2005.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-sGLOH2&quot; href=&quot;#call-sGLOH2&quot;&gt;sGLOH2&lt;/a&gt;] {Bellavia} F. and {Colombo} C., ``&lt;em&gt;Rethinking the sGLOH Descriptor&lt;/em&gt;'', IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 40, number 4, pp. 931-944,  2018.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-OriNet2016&quot; href=&quot;#call-OriNet2016&quot;&gt;OriNet2016&lt;/a&gt;] K. M., Y. Verdie, P. Fua &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Learning to Assign Orientations to Feature Points&lt;/em&gt;'', CVPR,  2016.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-AffNet2018&quot; href=&quot;#call-AffNet2018&quot;&gt;AffNet2018&lt;/a&gt;] D. Mishkin, F. Radenovic and J. Matas, ``&lt;em&gt;Repeatability is Not Enough: Learning Affine Regions via Discriminability&lt;/em&gt;'', ECCV,  2018.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-PerdochRetrieval2009&quot; href=&quot;#call-PerdochRetrieval2009&quot;&gt;PerdochRetrieval2009&lt;/a&gt;] M. {Perd'och}, O. {Chum} and J. {Matas}, ``&lt;em&gt;Efficient representation of local geometry for large scale object retrieval&lt;/em&gt;'', CVPR,  2009.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-IMW2020&quot; href=&quot;#call-IMW2020&quot;&gt;IMW2020&lt;/a&gt;] Jin Yuhe, Mishkin Dmytro, Mishchuk Anastasiia &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Image Matching across Wide Baselines: From Paper to Practice&lt;/em&gt;'', arXiv preprint arXiv:2003.01587, vol. , number , pp. ,  2020.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-gcransac2018&quot; href=&quot;#call-gcransac2018&quot;&gt;gcransac2018&lt;/a&gt;] D. Barath and J. Matas, ``&lt;em&gt;Graph-Cut RANSAC&lt;/em&gt;'', The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),  2018.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-magsac2019&quot; href=&quot;#call-magsac2019&quot;&gt;magsac2019&lt;/a&gt;] J.N. Daniel Barath, ``&lt;em&gt;MAGSAC: marginalizing sample consensus&lt;/em&gt;'', CVPR,  2019.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Philbin07&quot; href=&quot;#call-Philbin07&quot;&gt;Philbin07&lt;/a&gt;] J. Philbin, O. Chum, M. Isard &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Object Retrieval with Large Vocabularies and Fast Spatial Matching&lt;/em&gt;'', CVPR,  2007.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-perd2006epipolar&quot; href=&quot;#call-perd2006epipolar&quot;&gt;perd2006epipolar&lt;/a&gt;] M. Perd'och, J. Matas and O. Chum, ``&lt;em&gt;Epipolar geometry from two correspondences&lt;/em&gt;'', ICPR,  2006.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-PrittsRANSAC2013&quot; href=&quot;#call-PrittsRANSAC2013&quot;&gt;PrittsRANSAC2013&lt;/a&gt;] J. {Pritts}, O. {Chum} and J. {Matas}, ``&lt;em&gt;Approximate models for fast and accurate epipolar geometry estimation&lt;/em&gt;'', 2013 28th International Conference on Image and Vision Computing New Zealand (IVCNZ 2013),  2013.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Barath2019ICCV&quot; href=&quot;#call-Barath2019ICCV&quot;&gt;Barath2019ICCV&lt;/a&gt;] D. Barath and Z. Kukelova, ``&lt;em&gt;Homography From Two Orientation- and Scale-Covariant Features&lt;/em&gt;'', ICCV,  2019.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-RANSACAffine2020&quot; href=&quot;#call-RANSACAffine2020&quot;&gt;RANSACAffine2020&lt;/a&gt;] M. {Rodríguez}, G. {Facciolo}, R. G. &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Robust estimation of local affine maps and its applications to image matching&lt;/em&gt;'', 2020 IEEE Winter Conference on Applications of Computer Vision (WACV),  2020.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-barath2020making&quot; href=&quot;#call-barath2020making&quot;&gt;barath2020making&lt;/a&gt;] Barath Daniel, Polic Michal, Förstner Wolfgang &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Making Affine Correspondences Work in Camera Geometry Computation&lt;/em&gt;'', arXiv preprint arXiv:2007.10032, vol. , number , pp. ,  2020.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-guan2020relative&quot; href=&quot;#call-guan2020relative&quot;&gt;guan2020relative&lt;/a&gt;] Guan Banglei, Zhao Ji, Barath Daniel &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Relative Pose Estimation for Multi-Camera Systems from Affine Correspondences&lt;/em&gt;'', arXiv preprint arXiv:2007.10700, vol. , number , pp. ,  2020.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-OneACMonoDepth2020&quot; href=&quot;#call-OneACMonoDepth2020&quot;&gt;OneACMonoDepth2020&lt;/a&gt;] D.B. Ivan Eichhardt, ``&lt;em&gt;Relative Pose from Deep Learned Depth and a Single Affine Correspondence&lt;/em&gt;'', ECCV,  2020.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-SurfaceNormals2019&quot; href=&quot;#call-SurfaceNormals2019&quot;&gt;SurfaceNormals2019&lt;/a&gt;] {Baráth} D., {Eichhardt} I. and {Hajder} L., ``&lt;em&gt;Optimal Multi-View Surface Normal Estimation Using Affine Correspondences&lt;/em&gt;'', IEEE Transactions on Image Processing, vol. 28, number 7, pp. 3301-3311,  2019.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/wide-baseline-stereo-blog/images/affine_matches.png" /><media:content medium="image" url="/wide-baseline-stereo-blog/images/affine_matches.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">WxBS: Wide Multiple Baseline Stereo as a task</title><link href="/wide-baseline-stereo-blog/2020/07/09/wxbs.html" rel="alternate" type="text/html" title="WxBS: Wide Multiple Baseline Stereo as a task" /><published>2020-07-09T00:00:00-05:00</published><updated>2020-07-09T00:00:00-05:00</updated><id>/wide-baseline-stereo-blog/2020/07/09/wxbs</id><content type="html" xml:base="/wide-baseline-stereo-blog/2020/07/09/wxbs.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-07-09-wxbs.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Definition-of-WxBS&quot;&gt;Definition of WxBS&lt;a class=&quot;anchor-link&quot; href=&quot;#Definition-of-WxBS&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Let us denote &lt;strong&gt;observations&lt;/strong&gt; $O_{i}, i=1..n$, each of which belongs to one of the &lt;strong&gt;views&lt;/strong&gt; $V_{j}, j=1..m$, $m \leq n$.&lt;/p&gt;
&lt;p&gt;Observation consist of spatial information and the &quot;descriptor&quot;. View contains the information, which is shared and the same for a group of observations.&lt;/p&gt;
&lt;p&gt;For example, a single observation can be an RGB pixel. Its spatial information is the pixel coordinates and the &quot;descriptor&quot; is RGB value. The view then is the image, with information about the camera pose, camera intrinsics, sensor and the time of the photo. Some of this information can be unknown to the user, i.e. hidden variable.&lt;/p&gt;
&lt;p&gt;Another example could an event camera[&lt;a class=&quot;latex_cit&quot; id=&quot;call-EventCameraSurvey2020&quot; href=&quot;#cit-EventCameraSurvey2020&quot;&gt;EventCameraSurvey2020&lt;/a&gt;]. In that case the observation contains  the pixel coordinates and the descriptor is the sign of the intensity change. The view will contain the information about the sensor, camera pose and the single observation inside it, because every event has an unique timestamp.&lt;/p&gt;
&lt;p&gt;Observations and views can be of different nature and dimentionality. E.g. $V_1$, $V_2$  -- RGB images, $V_3$ -- point cloud from a laser scaner, $V_4$ -- image from a thermal camera, and so on.&lt;/p&gt;
&lt;p&gt;An unordered pair of observations $(O_{i},O_{k})$ forms a &lt;strong&gt;correspondence&lt;/strong&gt; $c_{ik}$ if they are belong to different views $V_{j}$. The group of observations is called multivew correspondence $C_o$, when there is exactly one observation $O_i$ per view $V_j$. Some of observations $O_i$ can be empty $\varnothing$, i.e. not observed in the specific view $V_j$.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;world model&lt;/strong&gt; is the set of contraints on views, observations and correspondences. For example, one the popular models are epipolar geometry and ridid motion assumption.&lt;/p&gt;
&lt;p&gt;The correspondence is called ground truth or veridical if it satisfy the constraints posed be the world model.&lt;/p&gt;
&lt;p&gt;We can now define a wide baseline stereo.&lt;/p&gt;
&lt;p&gt;By &lt;strong&gt;wide baseline stereo&lt;/strong&gt; we understand the process of establishing two or multi-view correspondences  $C_o$ from observations $O_i$ and images $V_{j}$ and recovering the missing information about the views and estimatting the unknown parameters of the world model.&lt;/p&gt;
&lt;p&gt;Most often in the current thesis we will be using the the following world model. The scene consists of 3 dimentional elements, and is rigid and static. The observations are the 2D projections to the camera plane by the projectice pinhole camera. The relationship between observations in different views is either epipolar geometry, or projective transform[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Hartley2004&quot; href=&quot;#cit-Hartley2004&quot;&gt;Hartley2004&lt;/a&gt;]. Any moving object does not satisty the world model and therefore is considered an occlusion. We will call the &quot;baseline&quot; the distance between the camera centers.&lt;/p&gt;
&lt;p&gt;For example, on image below, observations $O_i$ are blue circles and the correspondences $c_{jk}$ are shown as lines. The assumed object $X_i$ is a red circle.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/imgs/WxBS_house.jpeg&quot; alt=&quot;&quot; title=&quot;Example of multimodal wide baseline stereo&quot; /&gt;&lt;/p&gt;
&lt;p&gt;We will call &quot;&lt;strong&gt;wide multiple baseline stereo&lt;/strong&gt;&quot; or &lt;strong&gt;WxBS&lt;/strong&gt; [&lt;a class=&quot;latex_cit&quot; id=&quot;call-Mishkin2015WXBS&quot; href=&quot;#cit-Mishkin2015WXBS&quot;&gt;Mishkin2015WXBS&lt;/a&gt;] if the observations have different nature or the conditions under which observation were made are different.&lt;/p&gt;
&lt;p&gt;The different between &lt;strong&gt;wide baseline stereo&lt;/strong&gt; and &lt;strong&gt;short baseline stereo&lt;/strong&gt;, or, simply &lt;strong&gt;stereo&lt;/strong&gt; is the follwing. In &lt;strong&gt;stereo&lt;/strong&gt; the baseline is small -- less then 1 meter -- and typically known and fixed. The task is to establish correspondences, which can be done by 1D search along the known epipolar lines.&lt;/p&gt;
&lt;p&gt;In contrast, in  &lt;strong&gt;wide baseline stereo&lt;/strong&gt; the baseline is unknown, mostly unconstrained and the viewpoints of the cameras can vary drastically.&lt;/p&gt;
&lt;p&gt;The wide baseline stereo, which also outputs the estimation of the latent objects, e.g. in form of 3d point world coordinates we would call &lt;strong&gt;rigid structure-from-motion&lt;/strong&gt; (rigid SfM) or &lt;strong&gt;3D reconstruction&lt;/strong&gt;. We do not consider object shape approximation with voxels, meshes, etc in the current thesis. Nor we consider the recovery of scene albedo, illumination, and other appearance properties.&lt;/p&gt;
&lt;p&gt;While the difference between &lt;strong&gt;SfM&lt;/strong&gt; and &lt;strong&gt;WBS&lt;/strong&gt; is often blurred and the terms are used interchangeably, we would  consider WBS as a part of SfM pipeline prior to recovering 3d point cloud.&lt;/p&gt;
&lt;p&gt;Other correspondence problems, as tracking, optical flow or establishing semantic correspondences could be defined using the terminilogy we established.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/imgs/wxbs_problems.png&quot; alt=&quot;&quot; title=&quot;Example of  WxBS problems&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;References&quot;&gt;References&lt;a class=&quot;anchor-link&quot; href=&quot;#References&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;(&lt;a id=&quot;cit-EventCameraSurvey2020&quot; href=&quot;#call-EventCameraSurvey2020&quot;&gt;Gallego, Delbruck &lt;em&gt;et al.&lt;/em&gt;, 2020&lt;/a&gt;) Gallego Guillermo, Delbruck Tobi, Orchard Garrick Michael &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Event-based Vision: A Survey&lt;/em&gt;'', IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. , number , pp. ,  2020.&lt;/p&gt;
&lt;p&gt;(&lt;a id=&quot;cit-Hartley2004&quot; href=&quot;#call-Hartley2004&quot;&gt;Hartley and Zisserman, 2004&lt;/a&gt;) R.~I. Hartley and A. Zisserman, ``&lt;em&gt;Multiple View Geometry in Computer Vision&lt;/em&gt;'',  2004.&lt;/p&gt;
&lt;p&gt;(&lt;a id=&quot;cit-Mishkin2015WXBS&quot; href=&quot;#call-Mishkin2015WXBS&quot;&gt;Mishkin, Matas &lt;em&gt;et al.&lt;/em&gt;, 2015&lt;/a&gt;) D. Mishkin, J. Matas, M. Perdoch &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;WxBS: Wide Baseline Stereo Generalizations&lt;/em&gt;'', BMVC,  2015.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/wide-baseline-stereo-blog/images/wxbs_problems400.png" /><media:content medium="image" url="/wide-baseline-stereo-blog/images/wxbs_problems400.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The Role of Wide Baseline Stereo in the Deep Learning World</title><link href="/wide-baseline-stereo-blog/2020/03/27/intro.html" rel="alternate" type="text/html" title="The Role of Wide Baseline Stereo in the Deep Learning World" /><published>2020-03-27T00:00:00-05:00</published><updated>2020-03-27T00:00:00-05:00</updated><id>/wide-baseline-stereo-blog/2020/03/27/intro</id><content type="html" xml:base="/wide-baseline-stereo-blog/2020/03/27/intro.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-03-27-intro.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Rise-of-Wide-Baseline-Stereo&quot;&gt;Rise of Wide Baseline Stereo&lt;a class=&quot;anchor-link&quot; href=&quot;#Rise-of-Wide-Baseline-Stereo&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The wide baseline stereo (WBS) is a process of establishing correspondences between pixels and/or regions between images depicting the same object or scene and estimation geometric relationship between the cameras, which produced that images. It is the building block of many popular computer vision application, where spatial localization or 3D world understanding is required.&lt;/p&gt;
&lt;p&gt;If the wide baseline stereo is a new concept for you, I recommend checking the &lt;a href=&quot;https://ducha-aiki.github.io/wide-baseline-stereo-blog/2021/01/09/wxbs-in-simple-terms.html&quot;&gt;examplanation in simple terms&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/00_intro_files/match_doll.png&quot; alt=&quot;&quot; title=&quot;Correspondences between two views found by wide baseline stereo algorithm. Photo and doll created by Olha Mishkina&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Where does wide baseline stereo come from?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As it often happens, the new arised from the older problem -- stereo matching, or as we will call it -- narrow or short baseline stereo. In the narrow baseline stereo images are taken from the nearby positions and not differ much in the orientation either. One could find correspondence for the point $(x,y)$ from the image $I_1$ in the image $I_2$ by simply searching in some small window around $(x,y)$[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Hannah1974ComputerMO&quot; href=&quot;#cit-Hannah1974ComputerMO&quot;&gt;Hannah1974ComputerMO&lt;/a&gt;,&lt;a class=&quot;latex_cit&quot; id=&quot;call-Moravec1980&quot; href=&quot;#cit-Moravec1980&quot;&gt;Moravec1980&lt;/a&gt;] or, assuming that camera pair is calibrated -- by searching along the epipolar line[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Hartley2004&quot; href=&quot;#cit-Hartley2004&quot;&gt;Hartley2004&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-03-27-intro_files/att_00003.png&quot; alt=&quot;&quot; title=&quot;Correspondence search in narrow baseline stereo, from Moravec 1980 PhD thesis.&quot; /&gt;&lt;/p&gt;
&lt;!--- ![Wide baseline stereo model. &quot;Baseline&quot; is the distance between cameras. Image by Arne Nordmann (WikiMedia)](00_intro_files/Epipolar_geometry.svg) 
--&gt;

&lt;p&gt;One of the first succesful approaches to the wide baseline stereo problem was proposed by Schmid and Mohr[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Schmid1995&quot; href=&quot;#cit-Schmid1995&quot;&gt;Schmid1995&lt;/a&gt;] in 1995. One of the stepping stones was the corner detector by Harris and Stevens [&lt;a class=&quot;latex_cit&quot; id=&quot;call-Harris88&quot; href=&quot;#cit-Harris88&quot;&gt;Harris88&lt;/a&gt;], initially developed for the application of tracking.&lt;/p&gt;
&lt;p&gt;It was later extended by Beardsley, Torr and Zisserman[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Beardsley96&quot; href=&quot;#cit-Beardsley96&quot;&gt;Beardsley96&lt;/a&gt;] by adding RANSAC robust geometry estimation and later refined by Pritchett and Zisserman [&lt;a class=&quot;latex_cit&quot; id=&quot;call-Pritchett1998&quot; href=&quot;#cit-Pritchett1998&quot;&gt;Pritchett1998&lt;/a&gt;,&lt;a class=&quot;latex_cit&quot; id=&quot;call-Pritchett1998b&quot; href=&quot;#cit-Pritchett1998b&quot;&gt;Pritchett1998b&lt;/a&gt;] in 1998. The general pipeline remains mostly the same until now [&lt;a class=&quot;latex_cit&quot; id=&quot;call-WBSTorr99&quot; href=&quot;#cit-WBSTorr99&quot;&gt;WBSTorr99&lt;/a&gt;,&lt;a class=&quot;latex_cit&quot; id=&quot;call-CsurkaReview2018&quot; href=&quot;#cit-CsurkaReview2018&quot;&gt;CsurkaReview2018&lt;/a&gt;]. The currently adopted version of the wide baseline stereo algorithm is shown below.&lt;/p&gt;
&lt;!--- 
![image.png](/wide-baseline-stereo-blog/images/copied_from_nb/00_intro_files/att_00002.png)
--&gt;


&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/00_intro_files/matching-filtering.png&quot; alt=&quot;&quot; title=&quot;Commonly used wide baseline stereo pipeline&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Let's write down the WBS algorithm:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Compute interest points/regions in all images independently&lt;/li&gt;
&lt;li&gt;For each interest point/region compute a descriptor of their neigborhood (local patch).&lt;/li&gt;
&lt;li&gt;Establish tentative correspondences between interest points based on their descriptors.&lt;/li&gt;
&lt;li&gt;Robustly estimate geometric relation between two images based on tentative correspondences with RANSAC.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The reason of steps 1 and 2 done on the both images separately is that in general wide baseline stereo is not limited to pairs of images, but rather to a collections of them. If all the steps are done pairwise, then the computational complexity is $O(n^2)$. The more steps done seperately - the more efficient algorithm is.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Quick-expansion&quot;&gt;Quick expansion&lt;a class=&quot;anchor-link&quot; href=&quot;#Quick-expansion&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;This algorithm significantly changed computer vision landscape for next forteen years.&lt;/p&gt;
&lt;p&gt;Soon after introducing the algorithm, there it become clear that its quality significantly depends on quality of each component, i.e. local feature detector, descriptor, and geometry estimation. Pleora of new detectors and descriptors were proposed, with the most cited computer vision paper ever SIFT local feature[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Lowe99&quot; href=&quot;#cit-Lowe99&quot;&gt;Lowe99&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;It is worth noting, that SIFT became popular only after Mikolajczyk benchmark paper [&lt;a class=&quot;latex_cit&quot; id=&quot;call-MikoDescEval2003&quot; href=&quot;#cit-MikoDescEval2003&quot;&gt;MikoDescEval2003&lt;/a&gt;,&lt;a class=&quot;latex_cit&quot; id=&quot;call-Mikolajczyk05&quot; href=&quot;#cit-Mikolajczyk05&quot;&gt;Mikolajczyk05&lt;/a&gt;], showed it superiority to the rest of alternatives.&lt;/p&gt;
&lt;p&gt;Robust geometry estimation was also a hot topic: a lot of improvements over vanilla RANSAC were proposed: LO-RANSAC[&lt;a class=&quot;latex_cit&quot; id=&quot;call-LOransac2003&quot; href=&quot;#cit-LOransac2003&quot;&gt;LOransac2003&lt;/a&gt;], DEGENSAC[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Degensac2005&quot; href=&quot;#cit-Degensac2005&quot;&gt;Degensac2005&lt;/a&gt;], MLESAC[&lt;a class=&quot;latex_cit&quot; id=&quot;call-MLESAC00&quot; href=&quot;#cit-MLESAC00&quot;&gt;MLESAC00&lt;/a&gt;]&lt;/p&gt;
&lt;p&gt;Success of wide baseline stereo with SIFT features led to aplication of its components to other computer vision tasks, which were reformulated through wide baseline stereo lens:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Scalable image search&lt;/strong&gt;. Sivic and Zisserman in famous &quot;Video Google&quot; paper[&lt;a class=&quot;latex_cit&quot; id=&quot;call-VideoGoogle2003&quot; href=&quot;#cit-VideoGoogle2003&quot;&gt;VideoGoogle2003&lt;/a&gt;] proposed to treat local features as &quot;visual words&quot; and use ideas from text processing for searching in image collections.  Later even more WBS elements were re-introduced to image search, most notable -- &lt;strong&gt;spatial verification&lt;/strong&gt;[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Philbin07&quot; href=&quot;#cit-Philbin07&quot;&gt;Philbin07&lt;/a&gt;]: simplified RANSAC procedure to verify if visual word matches were spatially consistent.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/00_intro_files/att_00004.png&quot; alt=&quot;&quot; title=&quot;Bag of words image search. Image credit: Filip Radenovic http://cmp.felk.cvut.cz/~radenfil/publications/Radenovic-CMPcolloq-2015.11.12.pdf&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Image classification&lt;/strong&gt; was performed by placing some classifier (SVM, random forest, etc) on top of some encoding of the SIFT-like descriptors, extracted sparsely[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Fergus03&quot; href=&quot;#cit-Fergus03&quot;&gt;Fergus03&lt;/a&gt;,&lt;a class=&quot;latex_cit&quot; id=&quot;call-CsurkaBoK2004&quot; href=&quot;#cit-CsurkaBoK2004&quot;&gt;CsurkaBoK2004&lt;/a&gt;] or densely[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Lazebnik06&quot; href=&quot;#cit-Lazebnik06&quot;&gt;Lazebnik06&lt;/a&gt;]. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/00_intro_files/att_00005.png&quot; alt=&quot;&quot; title=&quot;Bag of local features representation for classification from Fergus03&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Object detection&lt;/strong&gt; was formulated as relaxed wide baseline stereo problem[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Chum2007Exemplar&quot; href=&quot;#cit-Chum2007Exemplar&quot;&gt;Chum2007Exemplar&lt;/a&gt;] or as classification of SIFT-like features inside a sliding window [&lt;a class=&quot;latex_cit&quot; id=&quot;call-HoG2005&quot; href=&quot;#cit-HoG2005&quot;&gt;HoG2005&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/00_intro_files/att_00003.png&quot; alt=&quot;&quot; title=&quot;Exemplar-representation of the classes using local features, cite{Chum2007Exemplar}&quot; /&gt;&lt;/p&gt;
&lt;!--- 
![HoG-based pedestrian detection algorithm](/wide-baseline-stereo-blog/images/copied_from_nb/00_intro_files/att_00006.png)
![Histogram of gradient visualization](/wide-baseline-stereo-blog/images/copied_from_nb/00_intro_files/att_00007.png)
--&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Semantic segmentation&lt;/strong&gt; was performed by classicication of local region descriptors, typically, SIFT and color features and postprocessing afterwards[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Superparsing2010&quot; href=&quot;#cit-Superparsing2010&quot;&gt;Superparsing2010&lt;/a&gt;]. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of course,wide  baseline stereo was also used for its direct applications:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;3D reconstruction&lt;/strong&gt; was based on camera poses and 3D points, estimated with help of SIFT features [&lt;a class=&quot;latex_cit&quot; id=&quot;call-PhotoTourism2006&quot; href=&quot;#cit-PhotoTourism2006&quot;&gt;PhotoTourism2006&lt;/a&gt;,&lt;a class=&quot;latex_cit&quot; id=&quot;call-RomeInDay2009&quot; href=&quot;#cit-RomeInDay2009&quot;&gt;RomeInDay2009&lt;/a&gt;,&lt;a class=&quot;latex_cit&quot; id=&quot;call-COLMAP2016&quot; href=&quot;#cit-COLMAP2016&quot;&gt;COLMAP2016&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/00_intro_files/att_00008.png&quot; alt=&quot;&quot; title=&quot;SfM pipeline from COLMAP&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;SLAM(Simultaneous localization and mapping)&lt;/strong&gt; [&lt;a class=&quot;latex_cit&quot; id=&quot;call-Se02&quot; href=&quot;#cit-Se02&quot;&gt;Se02&lt;/a&gt;,&lt;a class=&quot;latex_cit&quot; id=&quot;call-PTAM2007&quot; href=&quot;#cit-PTAM2007&quot;&gt;PTAM2007&lt;/a&gt;,&lt;a class=&quot;latex_cit&quot; id=&quot;call-Mur15&quot; href=&quot;#cit-Mur15&quot;&gt;Mur15&lt;/a&gt;] were based on fast version of local feature detectors and descriptors.&lt;/p&gt;
&lt;!--- 
![ORBSLAM pipeline](/wide-baseline-stereo-blog/images/copied_from_nb/00_intro_files/att_00009.png)
--&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Panorama stiching&lt;/strong&gt; [&lt;a class=&quot;latex_cit&quot; id=&quot;call-Brown07&quot; href=&quot;#cit-Brown07&quot;&gt;Brown07&lt;/a&gt;] and, more generally, &lt;strong&gt;feature-based image registration&lt;/strong&gt;[&lt;a class=&quot;latex_cit&quot; id=&quot;call-DualBootstrap2003&quot; href=&quot;#cit-DualBootstrap2003&quot;&gt;DualBootstrap2003&lt;/a&gt;] were initalized with a geometry obtained by WBS and then further optimized&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Deep-Learning-Invasion:-retreal-to-the-geometrical-fortress&quot;&gt;Deep Learning Invasion: retreal to the geometrical fortress&lt;a class=&quot;anchor-link&quot; href=&quot;#Deep-Learning-Invasion:-retreal-to-the-geometrical-fortress&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In 2012 deep learning-based AlexNet[&lt;a class=&quot;latex_cit&quot; id=&quot;call-AlexNet2012&quot; href=&quot;#cit-AlexNet2012&quot;&gt;AlexNet2012&lt;/a&gt;] approach beat all the methods in image classification. Soon after, Razavian et.al[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Astounding2014&quot; href=&quot;#cit-Astounding2014&quot;&gt;Astounding2014&lt;/a&gt;] have shown that convolutional neural networks (CNNs) pre-trained on the Imagenet outperform more complex traditional solutions in image and scene classification, object detection and image search.
Deep learning solutions, be it pretrained or end-to-end learned networks quickly become the default solution for the most of computer vision problems.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/00_intro_files/att_00010.png&quot; alt=&quot;&quot; title=&quot;CNN representation beats complex traditional pipelines. Reds are CNN-based and greens are the handcrafted. From Astounding2014&quot; /&gt;&lt;/p&gt;
&lt;p&gt;However, there was still an area, where deep learned solutions failed, sometimes spectacularly: geometry-related tasks. Wide baseline stereo[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Melekhov2017relativePoseCnn&quot; href=&quot;#cit-Melekhov2017relativePoseCnn&quot;&gt;Melekhov2017relativePoseCnn&lt;/a&gt;], visual localization[&lt;a class=&quot;latex_cit&quot; id=&quot;call-PoseNet2015&quot; href=&quot;#cit-PoseNet2015&quot;&gt;PoseNet2015&lt;/a&gt;]}, SLAM are still areas, where the classical wide baseline stereo dominates[&lt;a class=&quot;latex_cit&quot; id=&quot;call-sattler2019understanding&quot; href=&quot;#cit-sattler2019understanding&quot;&gt;sattler2019understanding&lt;/a&gt;,&lt;a class=&quot;latex_cit&quot; id=&quot;call-zhou2019learn&quot; href=&quot;#cit-zhou2019learn&quot;&gt;zhou2019learn&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;The full reasons why convolution pipelines are failing for geometrical tasks are yet to understand, but the current hypothesis are the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CNN-based pose predictions predictions are roughly equivalent to retrieval of most similar image from the training set and outputing its pose.[&lt;a class=&quot;latex_cit&quot; id=&quot;call-sattler2019understanding&quot; href=&quot;#cit-sattler2019understanding&quot;&gt;sattler2019understanding&lt;/a&gt;] This phenomenum is also observed in related area: single-view 3D reconstruction[&lt;a class=&quot;latex_cit&quot; id=&quot;call-Tatarchenko2019&quot; href=&quot;#cit-Tatarchenko2019&quot;&gt;Tatarchenko2019&lt;/a&gt;].&lt;/li&gt;
&lt;li&gt;Geometric and arithmetic operations are hard to represent via vanilla neural networks (i.e. matrix multiplication with non-linearity) and they may require specialized building blocks, resembling operations of algorithmic or geometric methods, e.g. spatial transformers[&lt;a class=&quot;latex_cit&quot; id=&quot;call-STN2015&quot; href=&quot;#cit-STN2015&quot;&gt;STN2015&lt;/a&gt;] and arithmetic units[&lt;a class=&quot;latex_cit&quot; id=&quot;call-NALU2018&quot; href=&quot;#cit-NALU2018&quot;&gt;NALU2018&lt;/a&gt;,&lt;a class=&quot;latex_cit&quot; id=&quot;call-NAU2020&quot; href=&quot;#cit-NAU2020&quot;&gt;NAU2020&lt;/a&gt;]. Even with special structure such networks require &quot;careful initialization, restricting parameter space, and regularizing for sparsity&quot;[&lt;a class=&quot;latex_cit&quot; id=&quot;call-NAU2020&quot; href=&quot;#cit-NAU2020&quot;&gt;NAU2020&lt;/a&gt;].&lt;/li&gt;
&lt;li&gt;Vanilla CNNs are not covariant to even simple geometric transformation like translation [&lt;a class=&quot;latex_cit&quot; id=&quot;call-MakeCNNShiftInvariant2019&quot; href=&quot;#cit-MakeCNNShiftInvariant2019&quot;&gt;MakeCNNShiftInvariant2019&lt;/a&gt;], scaling and especially rotation [&lt;a class=&quot;latex_cit&quot; id=&quot;call-GroupEqCNN2016&quot; href=&quot;#cit-GroupEqCNN2016&quot;&gt;GroupEqCNN2016&lt;/a&gt;]. Unlike them, WBS baseline is grounded on scale-space theory [&lt;a class=&quot;latex_cit&quot; id=&quot;call-lindeberg2013scale&quot; href=&quot;#cit-lindeberg2013scale&quot;&gt;lindeberg2013scale&lt;/a&gt;] and local patches are geometrically normalilzed before description. &lt;/li&gt;
&lt;li&gt;Predictions of the CNNs can be altered by change in a small localized area [&lt;a class=&quot;latex_cit&quot; id=&quot;call-AdvPatch2017&quot; href=&quot;#cit-AdvPatch2017&quot;&gt;AdvPatch2017&lt;/a&gt;] or even single pixel [&lt;a class=&quot;latex_cit&quot; id=&quot;call-OnePixelAttack2019&quot; href=&quot;#cit-OnePixelAttack2019&quot;&gt;OnePixelAttack2019&lt;/a&gt;], while the wide baseline stereo methods require the consensus of different independent regions. &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Today:-assimilation-and-merging&quot;&gt;Today: assimilation and merging&lt;a class=&quot;anchor-link&quot; href=&quot;#Today:-assimilation-and-merging&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;h3 id=&quot;Wide-baseline-stereo-as-a-task:-formulate-differentiably-and-learn-modules&quot;&gt;Wide baseline stereo as a task: formulate differentiably and learn modules&lt;a class=&quot;anchor-link&quot; href=&quot;#Wide-baseline-stereo-as-a-task:-formulate-differentiably-and-learn-modules&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Wide baseline stereo as a task is solved today typically by using learned components as a replacement of specific blocks in WBS algorithm[&lt;a class=&quot;latex_cit&quot; id=&quot;call-jin2020image&quot; href=&quot;#cit-jin2020image&quot;&gt;jin2020image&lt;/a&gt;] ,e.g. local descriptor like HardNet[&lt;a class=&quot;latex_cit&quot; id=&quot;call-HardNet2017&quot; href=&quot;#cit-HardNet2017&quot;&gt;HardNet2017&lt;/a&gt;], detectors like KeyNet[&lt;a class=&quot;latex_cit&quot; id=&quot;call-KeyNet2019&quot; href=&quot;#cit-KeyNet2019&quot;&gt;KeyNet2019&lt;/a&gt;], joint detector-descriptor[&lt;a class=&quot;latex_cit&quot; id=&quot;call-SuperPoint2017&quot; href=&quot;#cit-SuperPoint2017&quot;&gt;SuperPoint2017&lt;/a&gt;] matching and filtering like SuperGlue[&lt;a class=&quot;latex_cit&quot; id=&quot;call-sarlin2019superglue&quot; href=&quot;#cit-sarlin2019superglue&quot;&gt;sarlin2019superglue&lt;/a&gt;], etc. 
There are also attempts to formulate the whole downstream task pipeline like SLAM[&lt;a class=&quot;latex_cit&quot; id=&quot;call-gradslam2020&quot; href=&quot;#cit-gradslam2020&quot;&gt;gradslam2020&lt;/a&gt;] in a differentiable way, combining advantages of structured and learning-based approaches.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/00_intro_files/att_00011.png&quot; alt=&quot;&quot; title=&quot;SuperGlue: separate matching module for handcrafter and learned features&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/00_intro_files/gradslam.png&quot; alt=&quot;&quot; title=&quot;gradSLAM: differentiable formulation of SLAM pipeline&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;Wide-baseline-stereo-as-a-idea:-consensus-of-local-independent-predictions&quot;&gt;Wide baseline stereo as a idea: consensus of local independent predictions&lt;a class=&quot;anchor-link&quot; href=&quot;#Wide-baseline-stereo-as-a-idea:-consensus-of-local-independent-predictions&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;On the other hand, as an algorithm, wide baseline stereo is summarized into two main ideas&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Image should be represented as set of local parts, robust to occlusion, and not influencing each other.&lt;/li&gt;
&lt;li&gt;Decision should be based on spatial consensus of local feature correspondences.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;One of modern revisit of wide baseline stereo ideas is Capsule Networks[&lt;a class=&quot;latex_cit&quot; id=&quot;call-CapsNet2011&quot; href=&quot;#cit-CapsNet2011&quot;&gt;CapsNet2011&lt;/a&gt;,&lt;a class=&quot;latex_cit&quot; id=&quot;call-CapsNet2017&quot; href=&quot;#cit-CapsNet2017&quot;&gt;CapsNet2017&lt;/a&gt;]. Unlike CNNs, they encode not only intensity of feature responce, but also its location and require a geometric agreement between object parts for outputing a confident prediction.&lt;/p&gt;
&lt;p&gt;Similar ideas are now explored for ensuring adversarial robustness of CNNs[&lt;a class=&quot;latex_cit&quot; id=&quot;call-li2020extreme&quot; href=&quot;#cit-li2020extreme&quot;&gt;li2020extreme&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;Another way of using &quot;consensus of local independent predictions&quot; is used in &lt;a href=&quot;https://arxiv.org/abs/2007.11498&quot;&gt;Cross-transformers&lt;/a&gt; paper: spatial attention helps to select relevant feature for few-shot learning, see Figure below.&lt;/p&gt;
&lt;p&gt;While wide baseline stereo is far from the mainstream now, it continues to play an important role in computer vision.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/2020-03-27-intro_files/att_00000.png&quot; alt=&quot;&quot; title=&quot;Cross-transformers: spatial attention helps to select relevant feature for few-shot learning&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/wide-baseline-stereo-blog/images/copied_from_nb/00_intro_files/capsules.png&quot; alt=&quot;&quot; title=&quot;Capsule networks: revisiting the WBS idea. Each feature response is accompanied with its pose. Poses should be in agreement, otherwise object would not be recognized. Image by Aurélien Géron https://www.oreilly.com/content/introducing-capsule-networks/&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;References&quot;&gt;References&lt;a class=&quot;anchor-link&quot; href=&quot;#References&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;[&lt;a id=&quot;cit-Hannah1974ComputerMO&quot; href=&quot;#call-Hannah1974ComputerMO&quot;&gt;Hannah1974ComputerMO&lt;/a&gt;] M. J., ``&lt;em&gt;Computer matching of areas in stereo images.&lt;/em&gt;'',  1974.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Moravec1980&quot; href=&quot;#call-Moravec1980&quot;&gt;Moravec1980&lt;/a&gt;] Hans Peter Moravec, ``&lt;em&gt;Obstacle Avoidance and Navigation in the Real World by a Seeing Robot Rover&lt;/em&gt;'',  1980.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Hartley2004&quot; href=&quot;#call-Hartley2004&quot;&gt;Hartley2004&lt;/a&gt;] R.~I. Hartley and A. Zisserman, ``&lt;em&gt;Multiple View Geometry in Computer Vision&lt;/em&gt;'',  2004.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Schmid1995&quot; href=&quot;#call-Schmid1995&quot;&gt;Schmid1995&lt;/a&gt;] Schmid Cordelia and Mohr Roger, ``&lt;em&gt;Matching by local invariants&lt;/em&gt;'', , vol. , number , pp. ,  1995.  &lt;a href=&quot;https://hal.inria.fr/file/index/docid/74046/filename/RR-2644.pdf&quot;&gt;online&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Harris88&quot; href=&quot;#call-Harris88&quot;&gt;Harris88&lt;/a&gt;] C. Harris and M. Stephens, ``&lt;em&gt;A Combined Corner and Edge Detector&lt;/em&gt;'', Fourth Alvey Vision Conference,  1988.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Beardsley96&quot; href=&quot;#call-Beardsley96&quot;&gt;Beardsley96&lt;/a&gt;] P. Beardsley, P. Torr and A. Zisserman, ``&lt;em&gt;3D model acquisition from extended image sequences&lt;/em&gt;'', ECCV,  1996.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Pritchett1998&quot; href=&quot;#call-Pritchett1998&quot;&gt;Pritchett1998&lt;/a&gt;] P. Pritchett and A. Zisserman, ``&lt;em&gt;Wide baseline stereo matching&lt;/em&gt;'', ICCV,  1998.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Pritchett1998b&quot; href=&quot;#call-Pritchett1998b&quot;&gt;Pritchett1998b&lt;/a&gt;] P. Pritchett and A. Zisserman, ``&lt;em&gt;&quot;Matching and Reconstruction from Widely Separated Views&quot;&lt;/em&gt;'', 3D Structure from Multiple Images of Large-Scale Environments,  1998.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-WBSTorr99&quot; href=&quot;#call-WBSTorr99&quot;&gt;WBSTorr99&lt;/a&gt;] P. Torr and A. Zisserman, ``&lt;em&gt;Feature Based Methods for Structure and Motion Estimation&lt;/em&gt;'', Workshop on Vision Algorithms,  1999.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-CsurkaReview2018&quot; href=&quot;#call-CsurkaReview2018&quot;&gt;CsurkaReview2018&lt;/a&gt;] {Csurka} Gabriela, {Dance} Christopher R. and {Humenberger} Martin, ``&lt;em&gt;From handcrafted to deep local features&lt;/em&gt;'', arXiv e-prints, vol. , number , pp. ,  2018.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Lowe99&quot; href=&quot;#call-Lowe99&quot;&gt;Lowe99&lt;/a&gt;] D. Lowe, ``&lt;em&gt;Object Recognition from Local Scale-Invariant Features&lt;/em&gt;'', ICCV,  1999.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-MikoDescEval2003&quot; href=&quot;#call-MikoDescEval2003&quot;&gt;MikoDescEval2003&lt;/a&gt;] K. Mikolajczyk and C. Schmid, ``&lt;em&gt;A Performance Evaluation of Local Descriptors&lt;/em&gt;'', CVPR, June 2003.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Mikolajczyk05&quot; href=&quot;#call-Mikolajczyk05&quot;&gt;Mikolajczyk05&lt;/a&gt;] Mikolajczyk K., Tuytelaars T., Schmid C. &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;A Comparison of Affine Region Detectors&lt;/em&gt;'', IJCV, vol. 65, number 1/2, pp. 43--72,  2005.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-LOransac2003&quot; href=&quot;#call-LOransac2003&quot;&gt;LOransac2003&lt;/a&gt;] O. Chum, J. Matas and J. Kittler, ``&lt;em&gt;Locally Optimized RANSAC&lt;/em&gt;'', Pattern Recognition,  2003.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Degensac2005&quot; href=&quot;#call-Degensac2005&quot;&gt;Degensac2005&lt;/a&gt;] O. Chum, T. Werner and J. Matas, ``&lt;em&gt;Two-View Geometry Estimation Unaffected by a Dominant Plane&lt;/em&gt;'', CVPR,  2005.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-MLESAC00&quot; href=&quot;#call-MLESAC00&quot;&gt;MLESAC00&lt;/a&gt;] Torr P.H.S. and Zisserman A., ``&lt;em&gt;MLESAC: A New Robust Estimator with Application to Estimating Image Geometry&lt;/em&gt;'', CVIU, vol. 78, number , pp. 138--156,  2000.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-VideoGoogle2003&quot; href=&quot;#call-VideoGoogle2003&quot;&gt;VideoGoogle2003&lt;/a&gt;] J. Sivic and A. Zisserman, ``&lt;em&gt;Video Google: A Text Retrieval Approach to Object Matching in Videos&lt;/em&gt;'', ICCV,  2003.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Philbin07&quot; href=&quot;#call-Philbin07&quot;&gt;Philbin07&lt;/a&gt;] J. Philbin, O. Chum, M. Isard &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Object Retrieval with Large Vocabularies and Fast Spatial Matching&lt;/em&gt;'', CVPR,  2007.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Fergus03&quot; href=&quot;#call-Fergus03&quot;&gt;Fergus03&lt;/a&gt;] R. Fergus, P. Perona and A. Zisserman, ``&lt;em&gt;Object Class Recognition by Unsupervised Scale-Invariant Learning&lt;/em&gt;'', CVPR,  2003.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-CsurkaBoK2004&quot; href=&quot;#call-CsurkaBoK2004&quot;&gt;CsurkaBoK2004&lt;/a&gt;] C.D. G. Csurka, J. Willamowski, L. Fan &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Visual Categorization with Bags of Keypoints&lt;/em&gt;'', ECCV,  2004.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Lazebnik06&quot; href=&quot;#call-Lazebnik06&quot;&gt;Lazebnik06&lt;/a&gt;] S. Lazebnik, C. Schmid and J. Ponce, ``&lt;em&gt;Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories&lt;/em&gt;'', CVPR,  2006.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Chum2007Exemplar&quot; href=&quot;#call-Chum2007Exemplar&quot;&gt;Chum2007Exemplar&lt;/a&gt;] O. {Chum} and A. {Zisserman}, ``&lt;em&gt;An Exemplar Model for Learning Object Classes&lt;/em&gt;'', CVPR,  2007.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-HoG2005&quot; href=&quot;#call-HoG2005&quot;&gt;HoG2005&lt;/a&gt;] N. {Dalal} and B. {Triggs}, ``&lt;em&gt;Histograms of oriented gradients for human detection&lt;/em&gt;'', CVPR,  2005.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Superparsing2010&quot; href=&quot;#call-Superparsing2010&quot;&gt;Superparsing2010&lt;/a&gt;] J. Tighe and S. Lazebnik, ``&lt;em&gt;SuperParsing: Scalable Nonparametric Image Parsing with Superpixels&lt;/em&gt;'', ECCV,  2010.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-PhotoTourism2006&quot; href=&quot;#call-PhotoTourism2006&quot;&gt;PhotoTourism2006&lt;/a&gt;] Snavely Noah, Seitz Steven M. and Szeliski Richard, ``&lt;em&gt;Photo Tourism: Exploring Photo Collections in 3D&lt;/em&gt;'', ToG, vol. 25, number 3, pp. 835–846,  2006.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-RomeInDay2009&quot; href=&quot;#call-RomeInDay2009&quot;&gt;RomeInDay2009&lt;/a&gt;] Agarwal Sameer, Furukawa Yasutaka, Snavely Noah &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Building Rome in a day&lt;/em&gt;'', Communications of the ACM, vol. 54, number , pp. 105--112,  2011.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-COLMAP2016&quot; href=&quot;#call-COLMAP2016&quot;&gt;COLMAP2016&lt;/a&gt;] J. Sch\&quot;{o}nberger and J. Frahm, ``&lt;em&gt;Structure-From-Motion Revisited&lt;/em&gt;'', CVPR,  2016.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Se02&quot; href=&quot;#call-Se02&quot;&gt;Se02&lt;/a&gt;] Se S., G. D. and Little J., ``&lt;em&gt;Mobile Robot Localization and Mapping with Uncertainty Using Scale-Invariant Visual Landmarks&lt;/em&gt;'', IJRR, vol. 22, number 8, pp. 735--758,  2002.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-PTAM2007&quot; href=&quot;#call-PTAM2007&quot;&gt;PTAM2007&lt;/a&gt;] G. {Klein} and D. {Murray}, ``&lt;em&gt;Parallel Tracking and Mapping for Small AR Workspaces&lt;/em&gt;'', IEEE and ACM International Symposium on Mixed and Augmented Reality,  2007.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Mur15&quot; href=&quot;#call-Mur15&quot;&gt;Mur15&lt;/a&gt;] Mur-Artal R., Montiel J. and Tard{\'o}s J., ``&lt;em&gt;ORB-Slam: A Versatile and Accurate Monocular Slam System&lt;/em&gt;'', IEEE Transactions on Robotics, vol. 31, number 5, pp. 1147--1163,  2015.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Brown07&quot; href=&quot;#call-Brown07&quot;&gt;Brown07&lt;/a&gt;] Brown M. and Lowe D., ``&lt;em&gt;Automatic Panoramic Image Stitching Using Invariant Features&lt;/em&gt;'', IJCV, vol. 74, number , pp. 59--73,  2007.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-DualBootstrap2003&quot; href=&quot;#call-DualBootstrap2003&quot;&gt;DualBootstrap2003&lt;/a&gt;] V. C., Tsai} {Chia-Ling and {Roysam} B., ``&lt;em&gt;The dual-bootstrap iterative closest point algorithm with application to retinal image registration&lt;/em&gt;'', IEEE Transactions on Medical Imaging, vol. 22, number 11, pp. 1379-1394,  2003.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-AlexNet2012&quot; href=&quot;#call-AlexNet2012&quot;&gt;AlexNet2012&lt;/a&gt;] Alex Krizhevsky, Ilya Sutskever and Geoffrey E., ``&lt;em&gt;ImageNet Classification with Deep Convolutional Neural Networks&lt;/em&gt;'',  2012.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Astounding2014&quot; href=&quot;#call-Astounding2014&quot;&gt;Astounding2014&lt;/a&gt;] A. S., H. {Azizpour}, J. {Sullivan} &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;CNN Features Off-the-Shelf: An Astounding Baseline for Recognition&lt;/em&gt;'', CVPRW,  2014.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Melekhov2017relativePoseCnn&quot; href=&quot;#call-Melekhov2017relativePoseCnn&quot;&gt;Melekhov2017relativePoseCnn&lt;/a&gt;] I. Melekhov, J. Ylioinas, J. Kannala &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Relative Camera Pose Estimation Using Convolutional Neural Networks&lt;/em&gt;'', ,  2017.  &lt;a href=&quot;https://arxiv.org/abs/1702.01381&quot;&gt;online&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-PoseNet2015&quot; href=&quot;#call-PoseNet2015&quot;&gt;PoseNet2015&lt;/a&gt;] A. Kendall, M. Grimes and R. Cipolla, ``&lt;em&gt;PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization&lt;/em&gt;'', ICCV,  2015.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-sattler2019understanding&quot; href=&quot;#call-sattler2019understanding&quot;&gt;sattler2019understanding&lt;/a&gt;] T. Sattler, Q. Zhou, M. Pollefeys &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Understanding the limitations of cnn-based absolute camera pose regression&lt;/em&gt;'', CVPR,  2019.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-zhou2019learn&quot; href=&quot;#call-zhou2019learn&quot;&gt;zhou2019learn&lt;/a&gt;] Q. Zhou, T. Sattler, M. Pollefeys &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;To Learn or Not to Learn: Visual Localization from Essential Matrices&lt;/em&gt;'', ICRA,  2020.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-Tatarchenko2019&quot; href=&quot;#call-Tatarchenko2019&quot;&gt;Tatarchenko2019&lt;/a&gt;] M. Tatarchenko, S.R. Richter, R. Ranftl &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;What Do Single-View 3D Reconstruction Networks Learn?&lt;/em&gt;'', CVPR,  2019.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-STN2015&quot; href=&quot;#call-STN2015&quot;&gt;STN2015&lt;/a&gt;] M. Jaderberg, K. Simonyan and A. Zisserman, ``&lt;em&gt;Spatial transformer networks&lt;/em&gt;'', NeurIPS,  2015.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-NALU2018&quot; href=&quot;#call-NALU2018&quot;&gt;NALU2018&lt;/a&gt;] A. Trask, F. Hill, S.E. Reed &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Neural arithmetic logic units&lt;/em&gt;'', NeurIPS,  2018.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-NAU2020&quot; href=&quot;#call-NAU2020&quot;&gt;NAU2020&lt;/a&gt;] A. Madsen and A. Rosenberg, ``&lt;em&gt;Neural Arithmetic Units&lt;/em&gt;'', ICLR,  2020.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-MakeCNNShiftInvariant2019&quot; href=&quot;#call-MakeCNNShiftInvariant2019&quot;&gt;MakeCNNShiftInvariant2019&lt;/a&gt;] R. Zhang, ``&lt;em&gt;Making convolutional networks shift-invariant again&lt;/em&gt;'', ICML,  2019.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-GroupEqCNN2016&quot; href=&quot;#call-GroupEqCNN2016&quot;&gt;GroupEqCNN2016&lt;/a&gt;] T. Cohen and M. Welling, ``&lt;em&gt;Group equivariant convolutional networks&lt;/em&gt;'', ICML,  2016.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-lindeberg2013scale&quot; href=&quot;#call-lindeberg2013scale&quot;&gt;lindeberg2013scale&lt;/a&gt;] Lindeberg Tony, ``&lt;em&gt;Scale-space theory in computer vision&lt;/em&gt;'', , vol. 256, number , pp. ,  2013.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-AdvPatch2017&quot; href=&quot;#call-AdvPatch2017&quot;&gt;AdvPatch2017&lt;/a&gt;] T. Brown, D. Mane, A. Roy &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Adversarial patch&lt;/em&gt;'', NeurIPSW,  2017.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-OnePixelAttack2019&quot; href=&quot;#call-OnePixelAttack2019&quot;&gt;OnePixelAttack2019&lt;/a&gt;] Su Jiawei, Vargas Danilo Vasconcellos and Sakurai Kouichi, ``&lt;em&gt;One pixel attack for fooling deep neural networks&lt;/em&gt;'', IEEE Transactions on Evolutionary Computation, vol. 23, number 5, pp. 828--841,  2019.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-jin2020image&quot; href=&quot;#call-jin2020image&quot;&gt;jin2020image&lt;/a&gt;] Jin Yuhe, Mishkin Dmytro, Mishchuk Anastasiia &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Image Matching across Wide Baselines: From Paper to Practice&lt;/em&gt;'', arXiv preprint arXiv:2003.01587, vol. , number , pp. ,  2020.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-HardNet2017&quot; href=&quot;#call-HardNet2017&quot;&gt;HardNet2017&lt;/a&gt;] A. Mishchuk, D. Mishkin, F. Radenovic &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Working Hard to Know Your Neighbor's Margins: Local Descriptor Learning Loss&lt;/em&gt;'', NeurIPS,  2017.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-KeyNet2019&quot; href=&quot;#call-KeyNet2019&quot;&gt;KeyNet2019&lt;/a&gt;] A. Barroso-Laguna, E. Riba, D. Ponsa &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;Key.Net: Keypoint Detection by Handcrafted and Learned CNN Filters&lt;/em&gt;'', ICCV,  2019.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-SuperPoint2017&quot; href=&quot;#call-SuperPoint2017&quot;&gt;SuperPoint2017&lt;/a&gt;] Detone D., Malisiewicz T. and Rabinovich A., ``&lt;em&gt;Superpoint: Self-Supervised Interest Point Detection and Description&lt;/em&gt;'', CVPRW Deep Learning for Visual SLAM, vol. , number , pp. ,  2018.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-sarlin2019superglue&quot; href=&quot;#call-sarlin2019superglue&quot;&gt;sarlin2019superglue&lt;/a&gt;] P. Sarlin, D. DeTone, T. Malisiewicz &lt;em&gt;et al.&lt;/em&gt;, ``&lt;em&gt;SuperGlue: Learning Feature Matching with Graph Neural Networks&lt;/em&gt;'', CVPR,  2020.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-gradslam2020&quot; href=&quot;#call-gradslam2020&quot;&gt;gradslam2020&lt;/a&gt;] J. Krishna Murthy, G. Iyer and L. Paull, ``&lt;em&gt;gradSLAM: Dense SLAM meets Automatic Differentiation &lt;/em&gt;'', ICRA,  2020 .&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-CapsNet2011&quot; href=&quot;#call-CapsNet2011&quot;&gt;CapsNet2011&lt;/a&gt;] G.E. Hinton, A. Krizhevsky and S.D. Wang, ``&lt;em&gt;Transforming auto-encoders&lt;/em&gt;'', ICANN,  2011.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-CapsNet2017&quot; href=&quot;#call-CapsNet2017&quot;&gt;CapsNet2017&lt;/a&gt;] S. Sabour, N. Frosst and G.E. Hinton, ``&lt;em&gt;Dynamic routing between capsules&lt;/em&gt;'', NeurIPS,  2017.&lt;/p&gt;
&lt;p&gt;[&lt;a id=&quot;cit-li2020extreme&quot; href=&quot;#call-li2020extreme&quot;&gt;li2020extreme&lt;/a&gt;] Li Jianguo, Sun Mingjie and Zhang Changshui, ``&lt;em&gt;Extreme Values are Accurate and Robust in Deep Networks&lt;/em&gt;'', , vol. , number , pp. ,  2020.  &lt;a href=&quot;https://openreview.net/forum?id=H1gHb1rFwr&quot;&gt;online&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/wide-baseline-stereo-blog/images/doll_wbs_300.png" /><media:content medium="image" url="/wide-baseline-stereo-blog/images/doll_wbs_300.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>