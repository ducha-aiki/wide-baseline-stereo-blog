<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Evaluating OpenCV new RANSACs</h1><p class="page-description">They become much better now</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-05-17T00:00:00-05:00" itemprop="datePublished">
        May 17, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/ducha-aiki/wide-baseline-stereo-blog/tree/master/_notebooks/2021-05-17-OpenCV-New-RANSACs.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/wide-baseline-stereo-blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/ducha-aiki/wide-baseline-stereo-blog/master?filepath=_notebooks%2F2021-05-17-OpenCV-New-RANSACs.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/wide-baseline-stereo-blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/ducha-aiki/wide-baseline-stereo-blog/blob/master/_notebooks/2021-05-17-OpenCV-New-RANSACs.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/wide-baseline-stereo-blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
          <div class="px-2">
  <a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Fducha-aiki%2Fwide-baseline-stereo-blog%2Fblob%2Fmaster%2F_notebooks%2F2021-05-17-OpenCV-New-RANSACs.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/wide-baseline-stereo-blog/assets/badges/deepnote.svg" alt="Launch in Deepnote"/>
  </a>
</div>

        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-05-17-OpenCV-New-RANSACs.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="OpenCV-RANSAC-is-dead.-Long-live-the-OpenCV-USAC!">OpenCV RANSAC is dead. Long live the OpenCV USAC!<a class="anchor-link" href="#OpenCV-RANSAC-is-dead.-Long-live-the-OpenCV-USAC!"> </a></h2><p>Year ago we published a paper "<a href="https://arxiv.org/abs/2003.01587">Image Matching across Wide Baselines: From Paper to Practice</a>", which, among other messages, has shown that OpenCV RANSAC for fundamental matrix estimation is terrible: it was super inaccurate and slow. 
Since then my colleague <a href="https://www.linkedin.com/in/maksym-ivashechkin-770186185">Maksym Ivashechkin</a> has spent a summer 2020 improving OpenCV RANSACs. His work was released as a part of <a href="https://docs.opencv.org/4.5.0/d1/df1/md__build_master-contrib_docs-lin64_opencv_doc_tutorials_calib3d_usac.html">OpenCV 4.5.0 release</a>.</p>
<p>Now it is time to benchmark them. Let's go!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Evaluation-methodology">Evaluation methodology<a class="anchor-link" href="#Evaluation-methodology"> </a></h2><p>The benchmark is done on the validation subset of the <a href="https://www.cs.ubc.ca/research/image-matching-challenge/2021/data/">Image Matching Challenge 2021</a> datasets. We have detected RootSIFT features, matched them with optimal mutual SNN ratio test and feed into the tested RANSACs. The resulting fundamental matrixes were transformed into relative poses and compared to the ground truth poses. 
You can check details in the paper "<a href="https://arxiv.org/abs/2003.01587">Image Matching across Wide Baselines: From Paper to Practice</a>".</p>
<p>For all RANSACs we first determine the optimal inlier threshold by the grid search, whereas number of iterations (<code>max_iter</code>) was set to a reasonable 100k. Then, after fixing this optimal threshold, we vary number of iterations from 10 to 10M. This gives us an accuracy-time curve.</p>
<h2 id="Methods-evaluated">Methods evaluated<a class="anchor-link" href="#Methods-evaluated"> </a></h2><p>Non-OpenCV methods:</p>
<ul>
<li><a href="https://pypi.org/project/pydegensac/">DEGENSAC</a> - from <code>pydegensac</code> package, based on the original implementation of the method, proposed in CVPR 2005 paper "Two-View Geometry Estimation Unaffected by a Dominant Plane". It is the default choise for the Image Matching Challenge 2020 and 2021.</li>
<li><a href="https://pypi.org/project/pydegensac/">PyRANSAC</a> - also from <code>pydegensac</code> package, with flag <code>enable_degeneracy_check=False</code>, which is equivalent to a vanilla LO-RANSAC implementation.</li>
</ul>
<p>OpenCV methods, named after the flag, one needs to pass into <code>cv2.findFundamentalMatrix</code> function:</p>
<ul>
<li>USAC_DEFAULT – <a href="https://cmp.felk.cvut.cz/~chum/papers/chum-DAGM03.pdf">LO-RANSAC</a> + degeneracy tests</li>
<li>USAC_FAST – <a href="https://cmp.felk.cvut.cz/~chum/papers/chum-DAGM03.pdf">LO-RANSAC</a> + degeneracy tests. Fewer iterations in local optimization step than USAC_DEFAULT. Uses RANSAC score to maximize number of inliers and terminate earlier.</li>
<li>USAC_ACCURATE. Implements <a href="https://cmp.felk.cvut.cz/~matas/papers/barath-2018-gc_ransac-cvpr.pdf">Graph-Cut RANSAC</a> + degeneracy tests.</li>
<li>USAC_MAGSAC – <a href="https://arxiv.org/abs/1912.05909">MAGSAC++</a> implementation +  degeneracy tests.</li>
<li>RANSAC -- OpenCV RANSAC implementation from the previous versions of the library, <em>no</em> degeneracy tests</li>
</ul>
<p>All OpenCV USAC methods also use SPRT-test for speeding-up the evaluation.</p>
<h2 id="Results">Results<a class="anchor-link" href="#Results"> </a></h2><p>Here are results for all 3 datasets. The lefter and upper is curve, the better. Dashed vertical line marks 1/25 sec ("realtime") and 0.5 sec (challenge limit) time budget. Legend shows the method name and the optimal inlier threshold for the datasets: Phototourism, GoogleUrban and PragueParks respectively.</p>
<p><img src="/wide-baseline-stereo-blog/images/copied_from_nb/2021-05-17-OpenCV-New-RANSACs_files/att_00004.png" alt="" title="Time-accuracy trade-off on all 3 datasets" /></p>
<ol>
<li><p>The first and main message -- <strong>all new flags are much better than the old OpenCV implementation (green curve, worst results), which still a default option</strong>.</p>
</li>
<li><p>10k iterations and USAC_ACCURATE (red curve) gives you great results within 0.01 sec</p>
</li>
<li><p>All OpenCV advanced USACs are better than for the small/medium time budget (&lt; 0.1 sec per image) than pydegensac (blue curve).</p>
</li>
<li><p>The best methods for the higher budget are OpenCV USAC_MAGSAC and DEGENSAC from the pydegensac package.</p>
</li>
<li><p>There is no point is using flag "USAC_FAST" it is always better to use USAC_DEFAULT, USAC_ACCURATE or USAC_MAGSAC.</p>
</li>
<li><p>USAC_MAGSAC is the only method, which optimal threshold is <em>the same across all datasets</em>. This is a valuable property for practice, as it requires the least tuning.</p>
</li>
</ol>
<p>If you are interesting in results for an individual datasets, here they are.</p>
<h3 id="Phototourism">Phototourism<a class="anchor-link" href="#Phototourism"> </a></h3><p><img src="/wide-baseline-stereo-blog/images/copied_from_nb/2021-05-17-OpenCV-New-RANSACs_files/att_00005.png" alt="image.png" title="Time-accuracy trade-off on the Phototourism dataset" /></p>
<h3 id="GoogleUrban">GoogleUrban<a class="anchor-link" href="#GoogleUrban"> </a></h3><p><img src="/wide-baseline-stereo-blog/images/copied_from_nb/2021-05-17-OpenCV-New-RANSACs_files/att_00007.png" alt="" title="Time-accuracy trade-off on the GoogleUrban dataset" /></p>
<h3 id="PragueParks">PragueParks<a class="anchor-link" href="#PragueParks"> </a></h3><p><img src="/wide-baseline-stereo-blog/images/copied_from_nb/2021-05-17-OpenCV-New-RANSACs_files/att_00008.png" alt="" title="Time-accuracy trade-off on the PragueParks dataset" /></p>
<h2 id="Why-do-I-tune-and-evaluate-on-the-same-set?">Why do I tune and evaluate on the same set?<a class="anchor-link" href="#Why-do-I-tune-and-evaluate-on-the-same-set?"> </a></h2><p>It is true, that tuning and evaluation of the method on the same dataset does not make any sense. However, let me defend my choice. Here are the arguments:</p>
<ol>
<li><p>I do not want to compromise an integrity of the test set, which is the basis of the on-going competition <a href="https://www.cs.ubc.ca/research/image-matching-challenge/current/">Image Matching Challenge 2021</a> with prize money. That is why I do not want to leak information from the abovementioned test set and this is my primarly optimization objective. I also cannot tune the threshold on the "training subset", as both GoogleUrban and PragueParks do not have such.</p>
</li>
<li><p>I am interested more in the rough speed-accuracy trade-off than the precise rankings of the methods. It is quite likely, that those methods, which have an small acuracy gap on the validation set, would switch on the test set -- as it happened with DEGENSAC and MAGSAC in our original paper. However, it is very unlikely, that method, which performs poorly on the validation set would magically outperform everyone on the test set. Again, see PyRANSAC vs DEGENSAC in the original paper.</p>
</li>
<li><p>I clearly state this fact as a limitation and do not publish a paper ;)</p>
</li>
</ol>
<h2 id="Conclusion">Conclusion<a class="anchor-link" href="#Conclusion"> </a></h2><p>New OpenCV RANSACs are fast and have comparable accuracy, you can safely pick one of them. However, if you are using pydegensac and have &gt; 0.1 sec time budget, there is no need to switch.</p>
<p>Use proper RANSACs and be happy :)</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="ducha-aiki/wide-baseline-stereo-blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/wide-baseline-stereo-blog/2021/05/17/OpenCV-New-RANSACs.html" hidden></a>
</article>
