<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Revisiting Brown patch dataset and benchmark | Wide baseline stereo meets deep learning</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Revisiting Brown patch dataset and benchmark" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="How to create useful development set" />
<meta property="og:description" content="How to create useful development set" />
<meta property="og:site_name" content="Wide baseline stereo meets deep learning" />
<meta property="og:image" content="/wide-baseline-stereo-blog/images/brown_phototour_revisited.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-09-23T00:00:00-05:00" />
<script type="application/ld+json">
{"datePublished":"2020-09-23T00:00:00-05:00","dateModified":"2020-09-23T00:00:00-05:00","image":"/wide-baseline-stereo-blog/images/brown_phototour_revisited.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"/wide-baseline-stereo-blog/2020/09/23/local-descriptors-validation.html"},"description":"How to create useful development set","@type":"BlogPosting","url":"/wide-baseline-stereo-blog/2020/09/23/local-descriptors-validation.html","headline":"Revisiting Brown patch dataset and benchmark","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/wide-baseline-stereo-blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="/wide-baseline-stereo-blog/feed.xml" title="Wide baseline stereo meets deep learning" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-174193910-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/wide-baseline-stereo-blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Revisiting Brown patch dataset and benchmark | Wide baseline stereo meets deep learning</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Revisiting Brown patch dataset and benchmark" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="How to create useful development set" />
<meta property="og:description" content="How to create useful development set" />
<meta property="og:site_name" content="Wide baseline stereo meets deep learning" />
<meta property="og:image" content="/wide-baseline-stereo-blog/images/brown_phototour_revisited.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-09-23T00:00:00-05:00" />
<script type="application/ld+json">
{"datePublished":"2020-09-23T00:00:00-05:00","dateModified":"2020-09-23T00:00:00-05:00","image":"/wide-baseline-stereo-blog/images/brown_phototour_revisited.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"/wide-baseline-stereo-blog/2020/09/23/local-descriptors-validation.html"},"description":"How to create useful development set","@type":"BlogPosting","url":"/wide-baseline-stereo-blog/2020/09/23/local-descriptors-validation.html","headline":"Revisiting Brown patch dataset and benchmark","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="/wide-baseline-stereo-blog/feed.xml" title="Wide baseline stereo meets deep learning" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-174193910-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>


    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/wide-baseline-stereo-blog/">Wide baseline stereo meets deep learning</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/wide-baseline-stereo-blog/about/">About Me</a><a class="page-link" href="/wide-baseline-stereo-blog/search/">Search</a><a class="page-link" href="/wide-baseline-stereo-blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Revisiting Brown patch dataset and benchmark</h1><p class="page-description">How to create useful development set</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-09-23T00:00:00-05:00" itemprop="datePublished">
        Sep 23, 2020
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      10 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/ducha-aiki/wide-baseline-stereo-blog/tree/master/_notebooks/2020-09-23-local-descriptors-validation.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/wide-baseline-stereo-blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/ducha-aiki/wide-baseline-stereo-blog/master?filepath=_notebooks%2F2020-09-23-local-descriptors-validation.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/wide-baseline-stereo-blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/ducha-aiki/wide-baseline-stereo-blog/blob/master/_notebooks/2020-09-23-local-descriptors-validation.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/wide-baseline-stereo-blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-09-23-local-descriptors-validation.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="In-this-post">In this post<a class="anchor-link" href="#In-this-post"> </a></h3><ol>
<li>Why one needs good development set? What is wrong with existing sets for local patch descriptor learning?</li>
<li>One should validate in the same way, as it is used in production.</li>
<li>Brown patch revisited -- implementation details</li>
<li>Local patch descriptors evaluation results.</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Really-quick-intro-into-local-patch-descriptors">Really quick intro into local patch descriptors<a class="anchor-link" href="#Really-quick-intro-into-local-patch-descriptors"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Local patch descriptor is the thing, which helps you to automatically decide, if two patches in the pair of images correspond to the same point in a real world, or not. It should be robust to illumination, viewpoint and other changes.</p>
<p><img src="/wide-baseline-stereo-blog/images/copied_from_nb/2020-09-16-local-descriptors-validation_files/att_00003.png" alt="" title="The task of local descriptor, neural network here, is to decide if two patches belong to the same point, or nor. Image taken from SoSNet decriptor blogpost by Vassileios Balntas https://medium.com/scape-technologies/mapping-the-world-part-4-sosnet-to-the-rescue-5383671713e7" /></p>
<p>There are lots of ways how to implement a local patch descriptor: engineered and learned. 
Local patch descriptor is the crucial component of the <a href="https://ducha-aiki.github.io/wide-baseline-stereo-blog/2020/03/27/intro.html">wide baseline stereo pipeline</a> and a popular computer vision research topic.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Why-do-you-need-development-set?">Why do you need development set?<a class="anchor-link" href="#Why-do-you-need-development-set?"> </a></h2><p>Good data is crucial for any machine learning problem -- everyone now knows that. 
One needs high quality training set for training a good model. One also needs good test set, to know, what is <em>real</em> performance. However, there is one more, often forgotten, crucial component -- <strong>validation</strong> or <strong>development</strong> set. We use it to decide hyperparameters and validate design choices we make. It should be different from both training and test sets, yet, be good predictor of test set performance.
Moreover, it should allow fast iterations, so be not too small.</p>
<p>While such set is commonly called <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">validation set</a>, I do like Andrew Ng's term "<a href="https://cs230.stanford.edu/files/C2M1.pdf">development</a>" set more - because it helps to <em>develop</em> your model.</p>
<h1 id="Existing-datasets-for-local-patch-descriptors">Existing datasets for local patch descriptors<a class="anchor-link" href="#Existing-datasets-for-local-patch-descriptors"> </a></h1><p>So, what are the development set options for local patch descriptors?</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Brown-PhotoTourism.">Brown PhotoTourism.<a class="anchor-link" href="#Brown-PhotoTourism."> </a></h3><p><img src="/wide-baseline-stereo-blog/images/copied_from_nb/2020-09-16-local-descriptors-validation_files/brown_patches.png" alt="" title="Patches from 3 subsets of Brown Phototourism dataset" /></p>
<p>The most commonly and successfully used dataset for local descriptor learning is PhotoTourism, created in 2008. Here is its <a href="http://matthewalunbrown.com/patchdata/patchdata.html">description by authors</a>:</p>
<blockquote><p>The dataset consists of corresponding patches sampled from 3D reconstructions of the Statue of Liberty (New York), Notre Dame (Paris) and Half Dome (Yosemite).</p>
</blockquote>
<p>It also comes with evaluation protocol:patch pairs are labeled as "same" or "different" and the false positive rate at recall of 95% (FPR95) is reported. The variable, used to build ROC curve is descriptor distance between two patches.
Advantages:</p>
<ul>
<li>It contains local patches, extracted for two types of local feature detector -- DoG (SIFT) and Harris corners.</li>
<li>It is extracted from images, contraining non-planar structures and the geometrical noise present is caused by the local feature detector, not added artificially.  </li>
<li>Descriptors, trained on the dataset, show very good performance [<a class="latex_cit" id="call-IMW2020" href="#cit-IMW2020">IMW2020</a>], therefore the data itself is good.</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>when used as a benchmark, it shows unrealistic results: SIFT is 40x worse than deep learned descriptor. In practice, the difference is much smaller.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="HPatches">HPatches<a class="anchor-link" href="#HPatches"> </a></h3><p><a href="https://github.com/hpatches/hpatches-dataset">HPatches</a>, where H stands for the "<a href="https://en.wikipedia.org/wiki/Homography">homography</a>" was proposed to overcome the problem of unrealisting metric and, seemingly, too easy data, used in Phototourism dataset.</p>
<p>It was constructed in a different way than a Phototourism. First, local features were detected in the "reference" image and then reprojected to other images in sequences. Reprojection is prossible, because all the images are photographies of the planes  -- graffity, drawing, print, etc, or are all taken from the same position.
After the reprojection, some amount of geometrical noise -- rotation, translation, scaling, was added to the local features and the patches were extracted.</p>
<p>This process is illustration on the picture below (both taken from the <a href="https://github.com/hpatches/hpatches-dataset">HPatches website</a>).</p>
<p><img src="/wide-baseline-stereo-blog/images/copied_from_nb/2020-09-16-local-descriptors-validation_files/images_hard.png" alt="" title="Visualization of the hard patches locations in the target images." />
<img src="/wide-baseline-stereo-blog/images/copied_from_nb/2020-09-16-local-descriptors-validation_files/patches_hard.png" alt="" title="Extracted hard patches from the example sequence." /></p>
<p>HPAtches also provide 3 testing protocol, evaluating mean average precision (mAP) for 3 different tasks: patch verification  -- similar to Brown Phototourism, image matching and patch retrieval. The variable, used to build mAP is descriptor distance between two patches.</p>
<p>Advantages:</p>
<ul>
<li>Unlike PhotoTourism patch verification, image matching and patch retrieval tasks are not saturated.</li>
<li>HPatches contains illumination split, allowing the evaluation of descriptor robustness to illumination changes.</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>patches "misregistration" noise is of artificial nature, although paper claims that it has similar statistics</li>
<li>no non-planar structure</li>
<li>performance in HPaptches does not really correlate with the downstream performance [<a class="latex_cit" id="call-IMW2020" href="#cit-IMW2020">IMW2020</a>]</li>
</ul>
<p><img src="/wide-baseline-stereo-blog/images/copied_from_nb/2020-09-16-local-descriptors-validation_files/hpatches_vs_IMC.png" alt="image.png" title="Performance in HPaptches does not really correlate with the downstream performance on the Image Matching Benchmark" /></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="AMOSPatches">AMOSPatches<a class="anchor-link" href="#AMOSPatches"> </a></h3><p><a href="https://github.com/pultarmi/AMOS_patches">AMOS patches</a> is "HPatches illumination on steroids, without geometrical noise". It has the same advantanges and disadvantages, as HPatches and is mostly focused on illumination and weather changes.</p>
<p><img src="/wide-baseline-stereo-blog/images/copied_from_nb/2020-09-16-local-descriptors-validation_files/att_00002.png" alt="" title="Some images, contributing to AMOSPatches" />
<img src="/wide-baseline-stereo-blog/images/copied_from_nb/2020-09-16-local-descriptors-validation_files/amos_patches_small.png" alt="" title="Some patches from AMOS Patches dataset" /></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="PhotoSynth">PhotoSynth<a class="anchor-link" href="#PhotoSynth"> </a></h3><p><a href="https://github.com/rmitra/PS-Dataset">PhotoSynth</a> can be described and something in the middle between Phototour and HPatches. It contains patches, sampled from planar scenes, as well as from non-planar scenes.</p>
<p>At first glance, it should be great for the test and training purposes. However, there are several issues with it.
First, pre-trained HardNetPS descriptor, released together with the dataset, works well on HPatches, but poor in practice[<a class="latex_cit" id="call-pultar2020improving" href="#cit-pultar2020improving">pultar2020improving</a>].</p>
<p>Second, a couple of colleagues has tried to train the descriptor on top of it, as it was significantly worse than the authors reference model. Moreover, there is no testing/training code protocol available together with dataset.</p>
<p>So, while PhotoSynth might be a good dataset in principle, it definitely needs more love and work.</p>
<p><img src="/wide-baseline-stereo-blog/images/copied_from_nb/2020-09-16-local-descriptors-validation_files/att_00000.png" alt="image.png" title="Images, contributed to PS dataset" /></p>
<p><img src="/wide-baseline-stereo-blog/images/copied_from_nb/2020-09-16-local-descriptors-validation_files/att_00001.png" alt="image.png" title="Patches, samples from PS dataset" /></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Designing-the-evaluation-protocol">Designing the evaluation protocol<a class="anchor-link" href="#Designing-the-evaluation-protocol"> </a></h2><p>Classical local descriptor matching consists of two parts: finding nearest neighbors and filtering unreliable ones based on some criterion. 
I have wrote a <a href="https://medium.com/@ducha.aiki/how-to-match-to-learn-or-not-to-learn-part-2-1ab52ede2022">blogpost, describing the matching strategies in details</a>.</p>
<p>The most used in practice criterion is the first to second nearest neighbor distance (Lowe's) ratio threshold for filtering false positive matches. It is shown in the figure below.</p>
<p>The intuition is simple: if two candidates are too similar, then the match is unreliable and it is better to drop it.</p>
<p><img src="/wide-baseline-stereo-blog/images/copied_from_nb/2020-09-16-local-descriptors-validation_files/att_00004.png" alt="" title="Second nearest ratio strategy. Features from img1  -- blue circles -- are matched to features from img2  -- red squares. For each point in img1 we calculate two nearest neighbors and check their distance ratio . If both are too similar, i.e. &gt;0.8, bottom at Figure, then the match is discarded. Only confident matches are kept. Right graph is from SIFT paper, justification of such strategy." /></p>
<p>Somehow, none of the local patch evaluation protocols does not take such filtering criterion in mind, although it greatly influences the overall performance.</p>
<p>So, let's do the following:</p>
<ol>
<li>Take the patches, which are extracted from only two images. </li>
<li>For the each patch, calculate the descriptor distance to the correct match and to the hardnest (closest) non-match. Calculate the Lowe's ratio between this two.</li>
<li>Calculate accuracy for each of such triplets. If the correct match has smaller distance, score 1, if not - 0.</li>
<li>Sort the ratios from smallest to biggest and calculate <a href="https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval">mean average precision</a>#Mean_average_precision) (mAP).</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Brown-PhotoTour-Revisied:-implementation-details">Brown PhotoTour Revisied: implementation details<a class="anchor-link" href="#Brown-PhotoTour-Revisied:-implementation-details"> </a></h2><p>We have designed the protocol, now time for data. We could spend several month collecting and cleaning it...or we can just re-use great Brown PhotoTourism dataset. Re-visiting labeling and/or evaluation protocol of the time-tested dataset is a great idea.</p>
<p>Just couple of examples: <a href="https://github.com/fastai/imagenette">ImageNette</a> created by <a href="https://twitter.com/jeremyphoward">Jeremy Howard</a> from ImageNet, <a href="http://cmp.felk.cvut.cz/revisitop/">Revisited Oxford 5k</a> by <a href="https://filipradenovic.github.io/">Filip Radenovic</a> and so on.</p>
<p>For the protocol we designed above we need the information about the image id, where the patch was extracted from. Unfortunately, there is no such information in the Brown PhotoTourism, but there is suitable alternative -- the image id, where the reference patch was detected. What does it mean?</p>
<p>Suppose, we have 4 images and 5 keypoints. All the keypoints present in all images, which gives us 20 patches.
3 keypoints were first detected in Image 1 and 2 in image 2.<br />
That means that we will have 12 patches labeled image 1 and 8 patches labeled image 2.</p>
<p>So, we will have results for image 1 and image 2. Let's consider image 1. There are 12 patches, splitted in 3 "classes", 4 patches in each class.</p>
<p>Then, for the each of those 12 patches we:</p>
<ul>
<li>pick each of the corresponding patched as positives, so 3 positives. $P_1$, $P_2$, $P_3$</li>
<li>find the closest negative N. </li>
<li>add triplets (A, $P_1$, N), (A, $P_2$, N), (A, $P_3$, N) to the evaluation.</li>
</ul>
<p>Repeat the same for the image 2. 
That mimics the two-view matching process as close, as possible, given the data available to us.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Installation">Installation<a class="anchor-link" href="#Installation"> </a></h2><p><code>pip install brown_phototour_revisited</code></p>
<h2 id="How-to-use">How to use<a class="anchor-link" href="#How-to-use"> </a></h2><p>There is a single function, which does everything for you: <code>full_evaluation</code>. The original Brown benchmark consider evaluation, similar to cross-validation: train descriptor on one subset, evaluate on two others, repeat for all, so 6 evaluations are required. For the handcrafted descriptors, or those, that are trained on 3rd party datasets, only 3 evaluations are necessary.  We are following it here as well.</p>
<p>However, if you need to run some tests separately, or reuse some functions -- we will cover the usage below.
In the following example we will show how to use <code>full_evaluation</code> to evaluate SIFT descriptor as implemented in kornia.</p>

<pre><code># !pip install kornia</code></pre>

<pre><code>import torch
import kornia
from IPython.display import clear_output
from brown_phototour_revisited.benchmarking import *
patch_size = 65 

model = kornia.feature.SIFTDescriptor(patch_size, rootsift=True).eval()

descs_out_dir = 'data/descriptors'
download_dataset_to = 'data/dataset'
results_dir = 'data/mAP'

results_dict = {}
results_dict['Kornia RootSIFT'] = full_evaluation(model,
                                'Kornia RootSIFT',
                                path_to_save_dataset = download_dataset_to,
                                path_to_save_descriptors = descs_out_dir,
                                path_to_save_mAP = results_dir,
                                patch_size = patch_size, 
                                device = torch.device('cuda:0'), 
                           distance='euclidean',
                           backend='pytorch-cuda')
clear_output()
print_results_table(results_dict)</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">

<pre><code>------------------------------------------------------------------------------
Mean Average Precision wrt Lowe SNN ratio criterion on UBC Phototour Revisited
------------------------------------------------------------------------------
trained on       liberty notredame  liberty yosemite  notredame yosemite
tested  on           yosemite           notredame            liberty
------------------------------------------------------------------------------
Kornia RootSIFT        56.70              47.71               48.09 
------------------------------------------------------------------------------</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Results">Results<a class="anchor-link" href="#Results"> </a></h2><p>So, let's check how it goes. The latest results and implementation are in the following notebooks:</p>
<ul>
<li><a href="https://github.com/ducha-aiki/brown_phototour_revisited/blob/master/examples/evaluate_deep_descriptors.ipynb">Deep descriptors</a></li>
<li><a href="https://github.com/ducha-aiki/brown_phototour_revisited/blob/master/examples/evaluate_non_deep_descriptors.ipynb">Non-deep descriptors</a></li>
</ul>
<p>The results are the following:</p>

<pre><code>------------------------------------------------------------------------------
Mean Average Precision wrt Lowe SNN ratio criterion on UBC Phototour Revisited
------------------------------------------------------------------------------
trained on       liberty notredame  liberty yosemite  notredame yosemite
tested  on           yosemite           notredame            liberty
------------------------------------------------------------------------------
Kornia RootSIFT 32px   58.24              49.07               49.65 
HardNet 32px       70.64  70.31        61.93  59.56        63.06  61.64
SOSNet 32px        70.03  70.19        62.09  59.68        63.16  61.65
TFeat 32px         65.45  65.77        54.99  54.69        56.55  56.24
SoftMargin 32px    69.29  69.20        61.82  58.61        62.37  60.63
HardNetPS 32px         55.56              49.70               49.12 
R2D2_center_grayscal   61.47              53.18               54.98 
R2D2_MeanCenter_gray   62.73              54.10               56.17 
------------------------------------------------------------------------------

------------------------------------------------------------------------------
Mean Average Precision wrt Lowe SNN ratio criterion on UBC Phototour Revisited
------------------------------------------------------------------------------
trained on       liberty notredame  liberty yosemite  notredame yosemite
tested  on           yosemite           notredame            liberty
------------------------------------------------------------------------------
Kornia SIFT 32px       58.47              47.76               48.70 
OpenCV_SIFT 32px       53.16              45.93               46.00 
Kornia RootSIFT 32px   58.24              49.07               49.65 
OpenCV_RootSIFT 32px   53.50              47.16               47.37 
OpenCV_LATCH 65px  -----  -----        -----  37.26        -----  39.08
OpenCV_LUCID 32px      20.37              23.08               27.24 
skimage_BRIEF 65px     52.68              44.82               46.56 
Kornia RootSIFTPCA 3 60.73  60.64        50.80  50.24        52.46  52.02
MKD-concat-lw-32 32p 72.27  71.95        60.88  58.78        60.68  59.10
------------------------------------------------------------------------------

</code></pre>
<p>So far - in agreement with IMC benchmark: SIFT and RootSIFT are good, but not the best, SOSNet and HardNet are the leaders, but within tens of percents, not by orders of magnitude.</p>
<p><img src="/wide-baseline-stereo-blog/images/copied_from_nb/2020-09-16-local-descriptors-validation_files/att_00005.png" alt="" title="Image Matching Benchmark results, from https://arxiv.org/abs/2003.01587" /></p>
<h3 id="Disclaimer-1:-don't-trust-this-tables-fully">Disclaimer 1: don't trust this tables fully<a class="anchor-link" href="#Disclaimer-1:-don't-trust-this-tables-fully"> </a></h3><p>I haven't (yet!) checked if all the deep descriptors models, trained on Brown, were trained with flip-rotation 90 degrees augmentation. In the code below I assume that they were, however, it might not be true -- and the comparison might not be completely fair. I will do my best to check it, but if you know that I have used wrong weights - please <a href="https://github.com/ducha-aiki/brown_phototour_revisited/issues">open an issue</a>. Thank you.</p>
<h3 id="Disclaimer-2:-it-is-not-&quot;benchmark&quot;.">Disclaimer 2: it is not "benchmark".<a class="anchor-link" href="#Disclaimer-2:-it-is-not-&quot;benchmark&quot;."> </a></h3><p>The intended usage of the package is not to test and report the numbers in the paper. Instead think about is as cross-validation tool, helping the development. Thus, one CAN tune hyperparameters based on the benchmark results  instead of doing so on <a href="https://github.com/hpatches/hpatches-benchmark">HPatches</a>. After you have finished tuning, please, evaluate your local descriptors on some downstream task like <a href="https://github.com/vcg-uvic/image-matching-benchmark">IMC image matching benchmark</a> or <a href="https://www.visuallocalization.net/">visual localization</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Summary">Summary<a class="anchor-link" href="#Summary"> </a></h2><p>It really pays off, to spend time designing a proper evaluation pipeline and gathering the data for it. If you can re-use existing work - great. 
But don't blindly trust anything, even super-popular and widely adopted benchmarks. You need always check if the the protocol and data makes sense for your use-case personally.</p>
<p>Thanks for the reading, see you soon!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Citation">Citation<a class="anchor-link" href="#Citation"> </a></h2><p>If you use the benchmark/development set in an academic work, please cite it.</p>

<pre><code>@misc{BrownRevisited2020,
  title={UBC PhotoTour Revisied},
  author={Mishkin, Dmytro},
  year={2020},
  url = {https://github.com/ducha-aiki/brown_phototour_revisited}
}</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="References">References<a class="anchor-link" href="#References"> </a></h1><p>[<a id="cit-IMW2020" href="#call-IMW2020">IMW2020</a>] Jin Yuhe, Mishkin Dmytro, Mishchuk Anastasiia <em>et al.</em>, ``<em>Image Matching across Wide Baselines: From Paper to Practice</em>'', arXiv preprint arXiv:2003.01587, vol. , number , pp. ,  2020.</p>
<p>[<a id="cit-pultar2020improving" href="#call-pultar2020improving">pultar2020improving</a>] Pultar Milan, ``<em>Improving the HardNet Descriptor</em>'', arXiv ePrint:2007.09699, vol. , number , pp. ,  2020.</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="ducha-aiki/wide-baseline-stereo-blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/wide-baseline-stereo-blog/2020/09/23/local-descriptors-validation.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/wide-baseline-stereo-blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/wide-baseline-stereo-blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/wide-baseline-stereo-blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Everything you (didn&#39;t) want to know about image matching</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/ducha-aiki" target="_blank" title="ducha-aiki"><svg class="svg-icon grey"><use xlink:href="/wide-baseline-stereo-blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/ducha_aiki" target="_blank" title="ducha_aiki"><svg class="svg-icon grey"><use xlink:href="/wide-baseline-stereo-blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
