<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Image Matching Challenge 2023: SfM Unchained</h1><p class="page-description">3D reconstruction is harder than two view matching</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2023-07-05T00:00:00-05:00" itemprop="datePublished">
        Jul 5, 2023
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      8 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/ducha-aiki/wide-baseline-stereo-blog/tree/master/_notebooks/2023-07-05-IMC2023-Recap.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/wide-baseline-stereo-blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/ducha-aiki/wide-baseline-stereo-blog/master?filepath=_notebooks%2F2023-07-05-IMC2023-Recap.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/wide-baseline-stereo-blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/ducha-aiki/wide-baseline-stereo-blog/blob/master/_notebooks/2023-07-05-IMC2023-Recap.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/wide-baseline-stereo-blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
          <div class="px-2">
  <a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Fducha-aiki%2Fwide-baseline-stereo-blog%2Fblob%2Fmaster%2F_notebooks%2F2023-07-05-IMC2023-Recap.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/wide-baseline-stereo-blog/assets/badges/deepnote.svg" alt="Launch in Deepnote"/>
  </a>
</div>

        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2023-07-05-IMC2023-Recap.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-Unbearable-Weight-of-the-Bundle-Adjustment-and-$50K-money-prize">The Unbearable Weight of the Bundle Adjustment and $50K money prize<a class="anchor-link" href="#The-Unbearable-Weight-of-the-Bundle-Adjustment-and-$50K-money-prize"> </a></h2><p>This year Image Matching Challenge introduced two big changes. First, we went from two-view matching to full Structure-from-Motion as a task. Actually, the multiview track was present in pre-Kaggle era of IMC -- in <a href="https://image-matching-workshop.github.io/leaderboard/">2019</a>, <a href="https://www.cs.ubc.ca/research/image-matching-challenge/2020/leaderboard/">2020</a> and <a href="https://www.cs.ubc.ca/research/image-matching-challenge/2021/leaderboard/">2021</a>, so we kind of returned to the roots.</p>
<p>Participants were given with the sets of images and output should be the cameras poses for all of them. 
Second, thanks to our sponsors -- Google, <a href="https://www.haiper.ai">Haiper</a>, and Kaggle itself, we were able to propose $50k prize fund. With the strict open license condition (MIT/Apache 2/etc) for the "in-money solution".</p>
<h3 id="3D-reconstruction-is-not-cheap">3D reconstruction is not cheap<a class="anchor-link" href="#3D-reconstruction-is-not-cheap"> </a></h3><p>While going from image pairs to image sets might seem a small change, it has a significant impact on cumpute requirements. For the two-view case, one can run almost everything on GPU, e.g. SuperPoint for feature detection, SuperGlue for image matching, or the LoFTR for detector-less image matching. The only CPU part is the RANSAC, which can take as little as <a href="https://arxiv.org/abs/2106.10240">10ms per image pair for VSAC</a>, so not a big deal.</p>
<p>For the multiview case, on the other hand, one needs to performs the bundle-adjustment, which is a CPU-heavy task. Kaggle virtual machines, in addition to that, offer only 2-core CPU, so the 3D reconstruction itself becomes the main computational bottleneck of the whole process.</p>
<h3 id="Example-solution">Example solution<a class="anchor-link" href="#Example-solution"> </a></h3><p>To provide the participants a headstart, we have worked with Kaggle engineers to include the <a href="https://github.com/colmap/pycolmap">pycolmap</a> into the default Kaggle kernels. Based on it, we have provided an example submission, which uses local features included in <a href="link">kornia</a> library: <a href="https://zju3dv.github.io/loftr/">LoFTR</a>, <a href="https://github.com/cvlab-epfl/disk">DISK</a>, and <a href="https://kornia-tutorials.readthedocs.io/en/latest/_nbs/image_matching_adalam.html">KeyNet-AffNet-HardNet</a>.</p>
<p>All of them have Apache 2 license, and has shown a good performance in one of the previous IMCs: LoFTR was a part of <a href="https://www.kaggle.com/competitions/image-matching-challenge-2022/discussion/328854">top solutions in 2022</a>, <a href="https://ducha-aiki.github.io/wide-baseline-stereo-blog/2022/07/05/IMC2022-Recap.html">recap</a>, DISK - in <a href="https://www.cs.ubc.ca/research/image-matching-challenge/2020/leaderboard/">2020</a>, <a href="https://ducha-aiki.github.io/wide-baseline-stereo-blog/2021/05/14/IMC2020-competition-recap.html">recap</a> and <a href="https://www.cs.ubc.ca/research/image-matching-challenge/2021/leaderboard/">2021</a>, and KeyNet-AffNet-HardNet - one of the leaders in the <a href="https://arxiv.org/abs/2003.01587">original IJCV-2020 publication</a>.</p>
<p>The LoFTR example, however, was so heavy, that was causing time-out error without the modifications.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="New-datasets,-new-challenges:-UAV-to-ground,-day-night,-repeated-patterns,-wiry-objects,-scale-change">New datasets, new challenges: UAV-to-ground, day-night, repeated patterns, wiry objects, scale change<a class="anchor-link" href="#New-datasets,-new-challenges:-UAV-to-ground,-day-night,-repeated-patterns,-wiry-objects,-scale-change"> </a></h2><p>In 2023 we have prepared 3 new datasets - Heritage, Haiper and Urban.</p>
<p>Each dataset has been split into "training" -- public, and private part. The parts were geographically disjoint, but shared a similar nuisance factors. We haven't yet decided if we are going to release "hidden" part or not.</p>
<h3 id="Heritage">Heritage<a class="anchor-link" href="#Heritage"> </a></h3><p>This dataset features the high resolution photos of ancient buildings, taken with DLSR cameras from the ground, as well as UAV photos. Particular challenges are:</p>
<ul>
<li>large scale change (up to 20x), from that overview photo to the close-up of the small detail, together with in-plane rotation</li>
</ul>
<p><img src="/wide-baseline-stereo-blog/images/copied_from_nb/2023-07-05-IMC2023-Recap_files/Dioscuri.png" alt="" title="Images of the Dioscuri temple from Heritage dataset" /></p>
<ul>
<li><a href="https://dev.epicgames.com/community/learning/tutorials/1xM4/capturing-reality-banana-effect-what-to-do-if-my-model-is-bent">"banana effect"</a>, when the flat surface is often reconstructed as curved due to low overlap between consecutive frames and slight misalignments, which accululate altogether.</li>
</ul>
<p><img src="/wide-baseline-stereo-blog/images/copied_from_nb/2023-07-05-IMC2023-Recap_files/wall.png" alt="" title="3D model of the `wall` from Heritage dataset" /></p>
<ul>
<li>high-resolution image processing. See above about the computational challenges of bundle adjustment and then multiply that by high resolution AND large image number. Also, the most of deep learning features (DISK, LoFTR) can easily give you CUDA OOM error when run on 20 Mp image pair. </li>
</ul>
<h3 id="Haiper">Haiper<a class="anchor-link" href="#Haiper"> </a></h3><p>Haiper (training) dataset is similar to captures for NERFs - layered "dome" of cameras, going around some object. The object itself is often thin (bicycle) or textureless (statue). The test part of the dataset has very small number of images, making the viewpoint difference the biggest challenge.</p>
<p><img src="/wide-baseline-stereo-blog/images/copied_from_nb/2023-07-05-IMC2023-Recap_files/att_00000.png" alt="" title="3D model &#39;bike&#39; from the Haiper dataset" /></p>
<p>If IMC-2023 have discovered and used "<a href="https://research.nianticlabs.com/mapfree-reloc-benchmark">Map-free Visual Relocalization</a>" for training their pipelines, that would likely help them with Haiper dataset as well. However, everyone has missed this opportunity, including us - organizers.</p>
<h3 id="Urban">Urban<a class="anchor-link" href="#Urban"> </a></h3><p>The Urban dataset is, probably, the most similar one to IMC2020 PhotoTourism dataset. It covers photos of buldings in city, similar to PhotoTourism.</p>
<p>Here is the photos of the "Kyiv Puppet Theater" -- the easiest and public -- part of the dataset.</p>
<p><img src="/wide-baseline-stereo-blog/images/copied_from_nb/2023-07-05-IMC2023-Recap_files/att_00001.png" alt="image.png" /></p>
<p>In addition to the day-night photos, the hidden part of the dataset features highly symmetrical objects, such as the photo I took yesterday in Český Krumlov. The camera poses are actually look into each other, so visual overlap is zero. The only chance of non-wrongly-matching them, is either considering all the photos altogether, or utilizing the background. None of the existing feature matching solutions does this, as far as I know.</p>
<p><img src="/wide-baseline-stereo-blog/images/copied_from_nb/2023-07-05-IMC2023-Recap_files/IMG_4345_IMG_4352_matches.png" alt="" title="SuperGlue matches for symmetrical structures, photo from Český Krumlov to illustrate the challenges of the Urban dataset" /></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Findings-from-the-competition">Findings from the competition<a class="anchor-link" href="#Findings-from-the-competition"> </a></h2><h3 id="There-are-no-3d-reconstruction-libraries-besides-colmap">There are no 3d reconstruction libraries besides colmap<a class="anchor-link" href="#There-are-no-3d-reconstruction-libraries-besides-colmap"> </a></h3><p>At least, there are none, which you can easily compile on Kaggle kernel and then use from python notebook. I personally really hoped to see some global SfM solution like <a href="http://theia-sfm.org">Theia</a> or maybe <a href="https://github.com/mapillary/OpenSfM">OpenSfM</a> to appear among the top-solution,as it could provide a significant speed-up over the incremental SfM like Colmap. 
Python bindings and ease of compilation is crutial factor here.</p>
<h3 id="No-NERF-like-or-any-other-solution-than-classical-SfM">No NERF-like or any other solution than classical SfM<a class="anchor-link" href="#No-NERF-like-or-any-other-solution-than-classical-SfM"> </a></h3><p>Despite all the progress, it seems that if one doesn't have any additional information, such as RGBD, or initial ARKit cameras poses, the best thing one can do with a challenging image collection, is classical SfM. 
No <a href="https://arxiv.org/abs/2211.16991">SparsePose</a>, no <a href="https://arxiv.org/abs/2212.04492">FORGE</a></p>
<h3 id="Global-descriptor-based-co-visibility-is-hard">Global descriptor-based co-visibility is hard<a class="anchor-link" href="#Global-descriptor-based-co-visibility-is-hard"> </a></h3><p>Given that exhaistive image matching grows quadratically with number of images, it is very tempting to filter out some of those image pairs, based on some kind of covisibility criterion. And the simplest/fastest to compute is global descriptor one - you get a single vector per image, calculate global image similarity and remove those image pairs, which similarity is below threshold. Or take the top-K most similar images. In fact, I have used such approach in our paper <a href="https://arxiv.org/abs/2011.11986">"Efficient Initial Pose-graph Generation for Global SfM"</a>. However, what worked on large (thousands) image collections with dense viewpoint coverage, as 1DSfM, does not work that well on sparse image collections with many close-ups, rotations and illumination changes.</p>
<p>Some teams had moderate success with <a href="https://arxiv.org/abs/1511.07247">NetVLAD</a> and <a href="https://github.com/facebookresearch/dinov2">DINOv2</a> global descriptors, however, the best strategy, it seems, to use the two-view matching itself, but on a smaller resolution -- such as SuperGlue, or KeyNet-AffNet-HardNet-AdaLAM.</p>
<h3 id="PixelSfM-is-good-idea,-but-needs-improvements">PixelSfM is good idea, but needs improvements<a class="anchor-link" href="#PixelSfM-is-good-idea,-but-needs-improvements"> </a></h3><p>Many participants has tried to improve the initial SfM camera poses by utilizing <a href="https://github.com/cvg/pixel-perfect-sfm">PixelPerfectSfM</a> -- the feature-metric bundle adjustment. While it imrpoves results, it took a lot of time and especially memory, which rendered it unpractical for many teams. Another challenge is the package compilation itself, which is not easy either.</p>
<p><a href="https://www.kaggle.com/competitions/image-matching-challenge-2023/discussion/417407">1st-place team</a> proposed a novel version of the correspondence and poses refimenent instead - called <a href="https://zju3dv.github.io/DetectorFreeSfM/">Detector-Free Structure from Motion</a>.</p>
<p><img src="/wide-baseline-stereo-blog/images/copied_from_nb/2023-07-05-IMC2023-Recap_files/inbox-14597895-2b2b6c045d8a2dfa0a536090f025db02-main_fig.png" alt="" title="1st place solution, multiview refinement." /></p>
<h3 id="Rotation-invariance-is-important,-but-easy-to-achieve-for-SuperGlue-and-LoFTR">Rotation-invariance is important, but easy to achieve for SuperGlue and LoFTR<a class="anchor-link" href="#Rotation-invariance-is-important,-but-easy-to-achieve-for-SuperGlue-and-LoFTR"> </a></h3><p>Just rotate one of the images 4 times and select the best matches. 
Here is the image of the <a href="https://www.kaggle.com/competitions/image-matching-challenge-2023/discussion/416918">3rd place solution</a>, explaining it all.</p>
<p><img src="/wide-baseline-stereo-blog/images/copied_from_nb/2023-07-05-IMC2023-Recap_files/inbox-9249230-1036693590b6655d8d48d588bfd69e9c-IMC-solution.png" alt="" title="3rd place solution, brute-force rotation estimation, high-res image tiling" /></p>
<h3 id="Initial-image-pair-setup-in-Colmap-is-suboptimal">Initial image pair setup in Colmap is suboptimal<a class="anchor-link" href="#Initial-image-pair-setup-in-Colmap-is-suboptimal"> </a></h3><p>Several teams reported that manually setting the image pair to start incremental reconstruction in Colmap improved results. One even can improve results by running the incremental reconstruction several times.</p>
<h3 id="Many-things-do-not-work-until-they-do">Many things do not work until they do<a class="anchor-link" href="#Many-things-do-not-work-until-they-do"> </a></h3><p>Many teams have reported that LoFTR or DKM doesn't work for them - but both LoFTR and DKM are part of top-5 solutions. Recent "SiLK" keypoints are reported as not working, but maybe nobody just found a proper way to use them.</p>
<h3 id="&quot;Old&quot;-features-are-not-done-yet---KeyNetAffNet-HardNet-solution.">"Old" features are not done yet - KeyNetAffNet-HardNet solution.<a class="anchor-link" href="#&quot;Old&quot;-features-are-not-done-yet---KeyNetAffNet-HardNet-solution."> </a></h3><p>5th place actually end up in money by using an modified <a href="https://github.com/ducha-aiki/imc2023-kornia-starter-pack/blob/main/keynetaffnet-adalam-pycolmap-3dreconstruction.ipynb">example submission</a> with a classical pipeline using such ancient local features as DoG, Harris and GFTT, together with KeyNet (2019), coupled together with <a href="https://ducha-aiki.github.io/wide-baseline-stereo-blog/2020/07/17/affine-correspondences.html">local affine shape estimation</a> by AffNet, patch descriptor HardNet and hadncrafted AdaLAM matcher.</p>
<p>Basically, that is cleverly engineered submission of the off-the-shelf local features, available in <a href="https://kornia.readthedocs.io/en/latest/feature.html">kornia</a>.</p>
<p><img src="/wide-baseline-stereo-blog/images/copied_from_nb/2023-07-05-IMC2023-Recap_files/inbox-3964695-051e9413d0ce9cd8357723ba7efab1be-full_pipeline.png" alt="" title="5th place solution, classical affine features, off-the-shelf kornia. Candidate for 1st money prize" /></p>
<h3 id="Permissive-license-and-faster-SuperGlue-is-out---LightGlue">Permissive license and faster SuperGlue is out - LightGlue<a class="anchor-link" href="#Permissive-license-and-faster-SuperGlue-is-out---LightGlue"> </a></h3><p><a href="https://arxiv.org/pdf/2306.13643.pdf">LightGlue</a> - the solution by 7th place team and likely 2nd money prize, presents a SuperGlue-like architecture with early-stopping for easy image pairs and bunch of training recipes.
It also uses somewhat unpopular, but well-performing <a href="https://github.com/Shiaoming/ALIKED">ALIKED local feature</a></p>
<p><img src="/wide-baseline-stereo-blog/images/copied_from_nb/2023-07-05-IMC2023-Recap_files/att_00003.png" alt="" title="The LightGlue architecture from the paper" /></p>
<p>Here is the quite from the paper about tricks that matter:</p>
<blockquote><p>Since the depth maps of MegaDepth are often incomplete, we also label points with a large epipolar error as unmatch- able. Carefully tuning and annealing the learning rate boosts the accuracy. Training with more points also does:we use 2k per image instead of 1k. The batch size matters: we use gradient checkpointing [10] and mixed-precision to fit 32 image pairs on a single GPU with 24GB VRAM.</p>
</blockquote>
<p>In addition to that, homography-pretraining is crucial for the Light/Super-Glue performance.</p>
<h3 id="No-lines,-monodepth-or-semantic-segmentation-this-year.">No lines, monodepth or semantic segmentation this year.<a class="anchor-link" href="#No-lines,-monodepth-or-semantic-segmentation-this-year."> </a></h3><p>Actually I hoped to see <a href="https://github.com/cvg/limap/tree/main">Limap</a> line SfM or some kind of monocular depth models used. Bad luck.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Conclusion:-SfM-is-far-from-solved">Conclusion: SfM is far from solved<a class="anchor-link" href="#Conclusion:-SfM-is-far-from-solved"> </a></h2><p>As in many other computer vision tasks, if something seems to be "solved", that is just because the datasets are old and obsolete. You still need a good dense capture to do the 3D reconstruction. Many images, many compute, or the additional data like inertial module/GPS, or skilled person to do the capture, is required. See you hopefully next year.</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="ducha-aiki/wide-baseline-stereo-blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/wide-baseline-stereo-blog/2023/07/05/IMC2023-Recap.html" hidden></a>
</article>
