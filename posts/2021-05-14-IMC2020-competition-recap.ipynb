{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /2021/05/14/IMC2020-competition-recap\n",
    "badges: true\n",
    "branch: master\n",
    "date: '2021-05-14'\n",
    "description: SuperGlue, AdaLAM, DISK and others\n",
    "hide: false\n",
    "image: images/trevi_small.png\n",
    "output-file: 2021-05-14-imc2020-competition-recap.html\n",
    "search_exclude: false\n",
    "title: Image Matching Challenge 2020 Recap\n",
    "toc: false\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Image Matching Challenge?\n",
    "\n",
    "Image Matching Challenge is an on-going benchmark of the local features, matching methods and RANSACs, held since 2019. Its main idea is to measure a *downstream metric* of the image matching pipeline, such as *camera pose accuracy* after a *careful hyperparameter tuning*. \n",
    "\n",
    "![image.png](2021-05-14-IMC2020-competition-recap_files/att_00000.png)\n",
    "\n",
    "Before the challenge we have benchmarked existing popular and recent methods, such as (Root)SIFT, ORB, HardNet, SuperPoint, R2D2, D2Net, etc, and published a paper called \"[Image matching across wide baselines: From paper to practice](https://arxiv.org/pdf/2003.01587)\".\n",
    "\n",
    "The best results were obtained by a combinination of difference-of-Gaussians (DoG) local feature detector, commonly referred as [SIFT detector](https://docs.opencv.org/master/da/df5/tutorial_py_sift_intro.html), with deep learned patch descriptor such as [HardNet](https://kornia.readthedocs.io/en/latest/feature.html#kornia.feature.HardNet), [SOSNet](https://kornia.readthedocs.io/en/latest/feature.html#kornia.feature.SOSNet), or [AffNet](https://kornia.readthedocs.io/en/latest/feature.html#kornia.feature.LAFAffNetShapeEstimator)-[HardNet](https://kornia.readthedocs.io/en/latest/feature.html#kornia.feature.HardNet).\n",
    "\n",
    "Let's check what was proposed by the challenge participants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top solutions-2020  and follow-ups\n",
    "\n",
    "### SuperGlue\n",
    "\n",
    "\n",
    "![](2021-05-14-IMC2020-competition-recap_files/att_00001.png \"SuperGlue pipeline\")\n",
    "\n",
    "[SuperGlue](https://psarlin.com/superglue/) is an attention-based graph neural network for matching local features, taking into account both geometry (keypoint location) and appearance (descriptor). Unlike previous works, e.g. [CNe](https://arxiv.org/abs/1711.05971), or OANet below, it does not \"scoring-and-cleaning\" already established tentative correspondences. Instead, it establishes the correpondences given the local features from two images. \n",
    "SuperGlue won IMC-2020, as well as two other competitions at CVPR 2020. Its inference implementation is available [here](https://github.com/magicleap/SuperGluePretrainedNetwork).\n",
    "\n",
    "### OANet\n",
    "\n",
    "![](2021-05-14-IMC2020-competition-recap_files/att_00002.png \"OANet architecture\")\n",
    "[OANet](https://arxiv.org/pdf/1908.04964.pdf) is an specialized neural network architecture for \"scoring-and-cleaning\" already established tentative correspondences. OANet was run on top of DoG-HardNet local features.\n",
    "Pytorch version of OANet is available [here](https://github.com/zjhthu/OANet)\n",
    "\n",
    "### AdaLAM\n",
    "\n",
    "![](2021-05-14-IMC2020-competition-recap_files/att_00003.png \"AdaLAM stages, from original paper\")\n",
    "\n",
    "[AdaLAM](https://arxiv.org/abs/2006.04250) is a *handcrafted* algorithm for tentative correspondence cleaning, which work comparably or even better than a learning-based approaches. It is based on two core assumptions:\n",
    "\n",
    "- Keypoints, which are near from each other are probably corresponding to the neaighboring keypoints in the other images\n",
    "- If keypoints are in correspondence, it means that their orientation and scale are also in correspondence. Check my post [Local affine features: useful side product](https://ducha-aiki.github.io/wide-baseline-stereo-blog/2020/07/17/affine-correspondences.html) for an explanation\n",
    "\n",
    "The implementation is avilable [here](https://github.com/cavalli1234/AdaLAM). As DoG+HardNet were used as a local features.\n",
    "\n",
    "### DISK\n",
    "\n",
    "![](2021-05-14-IMC2020-competition-recap_files/att_00004.png \"DISK training objectives\")\n",
    "\n",
    "[DISK](https://arxiv.org/pdf/2006.13566.pdf) is local feature, which has two main differences from the rest of competitors (SuperPoint, R2D2):\n",
    "\n",
    "1. DISK is trained with a reinforcement-learning objective\n",
    "2. DISK has UNet-like architecture, unlike VGG-style for the rest of the features.\n",
    "\n",
    "Its implementation is available [here](https://github.com/cvlab-epfl/disk).\n",
    "\n",
    "### HyNet\n",
    "\n",
    "![](2021-05-14-IMC2020-competition-recap_files/att_00005.png \"HyNet architecture\")\n",
    "[HyNet](https://arxiv.org/pdf/2006.10202.pdf) is an next stage in the L2Net-HardNet-SOSNet series of local patch descriptors. It is different from the previous works in two ways:\n",
    "\n",
    "1. BatchNorm and ReLU in the HardNet architecture are replaced with FRN and TLU respecively. \n",
    "2. During training, distance to the negative (non-matching) $d(\\theta)$ and positive (matching) $s(\\theta)$ samples are calculated in a different way for better learning, see image below. *u* and *v* denote descriptors  is \n",
    "\n",
    "![](2021-05-14-IMC2020-competition-recap_files/att_00006.png \"Positive and negative distances used fro HyNet training\")\n",
    "\n",
    "HyNet submission also used semantic segmentation network to remove the keypoints from the non-matchable areas, such as sky and water. \n",
    "\n",
    "### HardNet8\n",
    "\n",
    "![](2021-05-14-IMC2020-competition-recap_files/att_00008.png \"HardNet8 architecture\")\n",
    "\n",
    "[HardNet8](https://arxiv.org/abs/2007.09699) is another improvement of the HardNet architecture:\n",
    "\n",
    "1. Deeper and wider network\n",
    "2. The output is compressed with a PCA.\n",
    "3. The training set and hyperparameters are carefully selected. \n",
    "\n",
    "It is available in [kornia](https://kornia.readthedocs.io/en/latest/feature.html#kornia.feature.HardNet8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2021 challenge\n",
    "\n",
    "This year challenge brings 2 new datasets: PragueParks and GoogleUrban.\n",
    "\n",
    "### The PragueParks dataset\n",
    "\n",
    "![](2021-05-14-IMC2020-competition-recap_files/att_00009.png \"PragueParks dataset samples\")\n",
    "\n",
    "The PragueParks dataset contains images from video sequences captured by the organizers with an iPhone 11, in 2021. The iPhone 11 has two cameras, with normal and wide lenses, both of which were used. Note that while the video is high quality, some of the frames suffer from motion blur. These videos were then processed by the commercial 3D reconstruction software [RealityCapture](https://www.capturingreality.com/), which is orders of magnitude faster than COLMAP, while delivering a comparable output in terms of accuracy. Similarly to we did for the \"PhotoTourism\" dataset, this data is then subsampled in order to generate the subsets used for evaluation.\n",
    "\n",
    "The dataset contains small-scale scenes like tree, pond, wooden and metal sculptures with different level of zoom, lots of vegetation, and no people. The distribution of its camera poses differs from Phototourism. \n",
    "\n",
    "\n",
    "### The GoogleUrban dataset\n",
    "\n",
    "![](2021-05-14-IMC2020-competition-recap_files/att_00010.png \"GoogleUrban dataset samples\")\n",
    "\n",
    "The GoogleUrban dataset contains images used by Google to evaluate localization algorithms, such as those in Google's Visual Positioning System, which powers Live View on millions on mobile devices. They are obtained from videos collected from different cell phones, on many countries all over the world, often years apart. They contain poses, but not depth maps. Please note that due to legal reasons, this data is released with a restricted license, and must be deleted by the end of the challenge.\n",
    "\n",
    "\n",
    "## Submit your solution!\n",
    "\n",
    "You can check out the tutorial on how to submit to IMC 2021 in the post [Submitting to Image Matching Challenge 2021\n",
    "](https://ducha-aiki.github.io/wide-baseline-stereo-blog/2021/05/12/submitting-to-IMC2021-step-by-step.html). Good luck!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "key",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": true,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
