{
  
    
        "post0": {
            "title": "Review of On the Limits of Pseudo Ground Truth in Visual Camera Re-localisation",
            "content": "I would like to share my thoughts on ICCV 2021 paper &quot;On the Limits of Pseudo Ground Truth in Visual Camera Re-localisation&quot; by Brachmann et.al. But first let&#39;s recap, what is Visual relocalization and how it is usually evaluated. . Visual (re-) localization: short recap . We have a robot with RGB or RGB-D camera. Our robot have been in some place once and have been explored it, taking the pictures. After some time, we bring it there again and turned on. Our robot (or, to be precise, the localization algorithm, which robot uses) task is to estimate its position and orientation from a single photo. How do we measure if it does it correctly? . We obtain two sequences of the images by going around our place. One is &quot;training&quot; or &quot;database&quot; sequence. The second is &quot;test&quot; sequence, which is emulates robot going there, wanting to know where it is. . | We run some known-to-be-reliable algorithm such as SfM or SLAM on those image sequences to jointly obtain the camera poses for all the images. Sometimes we can use additional sensors to get the pose directly and avoid such estimation completely. However, that is not the case we are discussing now. . | We provide the database images together with their poses to the algorithm we want to evaluate. It can do whatever it wants with them. . | Finally, we give the algorithm an image from the test sequence without a pose and algorithm estimate its camera pose. Then we compare this pose to the pose, which we receive with the pose we have and rank algorithms by the errors in the pose. . | There are different kinds of algorithms for visual localization. One possible way to first find the most similar iamges from our dataset to the query image, then match query to them and finally calculate the pose, given the poses of databased images. Another way is to train a model on a database images, such that for each pixel or region it tried to directly estimate its 3D location. Then based on the image content location, we deduce the camera pose. . What is the paper about? . The idea of the paper is very simple and yet nobody asked its main question before. When we benchmark our image matching or camera relocalization algorithms, we do not really have ground truth data for the camera pose, as we just recaped. Instead we rely on some sort of known-to-be-reliable algorithm to infer this information from (many) images. This process is called &quot;pseudo Ground Truth (pGT)&quot; in the paper. . Does it affect results to be in favor of the (family of) methods, which were used to generate the pseugo GT? . . Case-study: Image Matching Challenge . Let me give you an example. . Our Image Matching Challenge 2019, 2020 2021 does exactly this: the camera pose are reconstructed with COLMAP 3d reconstruction software given thousands of images. COLMAP is based on SIFT local features and LO-RANSAC for the pose stimation. . Can we use it to benchmark other local features and RANSACs in a fair way? That is the question, reviewers of our paper asked us. We answered this question by running the 3d reconstruction with very different local features: R2D2, SIFT, DoG-HardNet, SuperPoint. Then we compared the difference in the poses produced by the different methods is very small and much smaller than the accuracy threshold we used for evaluation. . . So we can safely use our benchmark to evaluate two view matching methods. Can we really? . SfM versus Dense SLAM . The &quot;On the Limits...&quot; asks the different question, than the one we have answered in IMC paper. While, the GT might be not biased towards specific local feature, it can be biased towards the higher-level algorithm -- Structure-from-Motion (SfM). But the (sparse) SfM is not the only way how we can obtain camera poses from the collection of images -- one could use (dense) Depth-based SLAM. Are the poses we got from them the same? . The answer is: no, they are not the same! . . The paper, however, does not stop here, it studies the following question: if the camera poses (think of them as &quot;labels&quot;) of the training set were obtained with a different family of algorithms, does it influence their performance? . The answer is yes. Look at the image below, which shows the trajectories, estimated by the relocalization methods, depending on the (pseudo) ground truth source. . . My take on the paper. . The paper asks a great question and provides good answers. It is also well-written and the experiments done with rigor. OK, time to stop &quot;CVPR-review&quot; mode and provide my thoughts. . I love the paper. However, (post-hoc) I believe that paper should stop earlier or go deeper. . First and main message of the paper (which paper does NOT formulate in that way): . The combination of the current methods (SfM or SLAM) with a current way of generating training sequence (single camera going around the place once) does not lead to the robust and precise enough camera poses. . I don&#39;t buy the paper&#39;s argument that &quot;depending on the metric you use, one or another is better&quot;. No. There are only 3 possible explanations: . a) SfM estimates wrong poses from the sequence . b) RGBD-SLAM estimates wrong poses from the sequence . c) both SfM and RGBD-SLAM estimate wrong poses. The variant: noone cannot estimate precise poses from the given images. . We are not in the quantum mechanics world, there is a single, well-defined (possibly unknown) correct camera pose. The correct pose exists and the question is whether SfM or SLAM (or neither) is closer to it. Yes, it would require some additional sensors, or running in simulated world, but anyway - the message should not be &quot;just provide 2 versions of pGT&quot;. The message is: if your 2 versions of pGT does not agree, GET MORE/BETTER DATA. . In addition to that I would like to see a paper, which answers the following question: given tha data we have (i.e. bad data) but with also available real GT camera poses, under which conditions, which algorithm (SfM or Depth SLAM) provides better results? . Which algorithm can adapt to (wrong) training data and be consistent with it? . One thing, which paper focuses on is how the &quot;test&quot; pGT is generated. However, even the bigger impact can be from the fact that training data may conflict with the poses, that algorithm estimates. . Imagine the augmented reality scenario, where one needs to draw the (virtual) teddy bear on the coach. In that case we actually don&#39;t care if the camera pose estimate is correct, or if our model of the room is correct. What we care about, is how naturally looking will be position of the Teddy bear (or Pikachu). It also important that bear not jumps around when we move our camera slightly. . Given the images and results in the paper, it seems that (learning-based) DSAC can suits this much better than (non-learning) AS. That is also stated in the paper conclusions. In the same time, it seems that the variance of the pose estimates is smaller for the SfM pGT than for the SLAM even for DSAC. . However, I would also like to see a paper, which studies how the error/bias/variance in the training data affects the different algorithms, so they could adapt to use bad data better, in more detail. . Conclusion . The paper is thought-provoking and I hope our community will read it and think about it. We also need to pay a way more attention to our data-gathering practice, because it really makes a difference. . P.S. Thanks to Eric for sending me additional visualizations, which were not in the arXiv paper. .",
            "url": "/wide-baseline-stereo-blog/2021/09/04/review-of-pseudoGT-limitations.html",
            "relUrl": "/2021/09/04/review-of-pseudoGT-limitations.html",
            "date": " • Sep 4, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "WxBS: Relaunching challenging benchmark for Image Matching",
            "content": "The hardest image matching benchmark . Previously we have discussed what is WxBS in general, how to match images from a really different viewpoints and also took a look into Image Matching Challenge 2020. Let&#39;s now speak about how to measure progress on images, which are really hard to match even to human? . For example, which have been taked in day-vs-night, years apart AND from different camera positions? One cannot register them to some 3d model, because they are too hard. And there is no 3D model either. . . Well, in that case one could go back to basics and handlabel the correspondences between the images. Such annotation is unlikely to be very precise, but it is going to be quite robust, if the labeler is careful. . That is what I have done 6 years ago and published such a dataset at BMVC 2015 together with a benchmark of popular image matchers of that time. None of them was able to successfully match even a fraction of the WxBS dataset. . . However, as I was quite inexpereinced in product packaging, as well as in software engineering, the benchmark existed as dataset + bunch of bash and Matlab scripts. It is needless to say, that nobody evaluated their algorithms on it. That made me sad, however, I never found time to make this benchmark easy to use. Well, now I did it and want to share the evaluation of the recent agorithms on such a hard data. . Not that fast! Problem with a fundamental matrix and how to solve them . OK, we have ground truth correspondences. Now what? Ideally one can estimate epipolar geometry from as little, as 7 correspondences, but in practice? No. The problem is that there could be many relative camera poses, which are consistent with the correspondences, especially when correspondences lie in plane and the camera calibration is unknown. . Let&#39;s consider a simple example: we have 13 correspondences and they seems to be correct. Are they enough for finding good epipolar geometry? This question can be answered via leave-one-out validation procedure, described in VSAC paper. We remove a single correspondence, estimate a fundamental matrix via DLT algorithm on the rest of the correspondences and see how the estimated epipolar geometry is changed (or not). . As you can see on the animation below, the epipolar geometry estimation from our 13 correspondences is not stable and removing any of them results in noticable change. . . So, if even manually labeled corresposndences are not good enough, what can be done? . First, simply add more correspondences, which are spread across image AND depth levels as even as possible. That is what I did recently, in order to make WxBS labeling better. However, it is not always possible -- sometimes you have lots of correspondence on &quot;faraway background plane&quot; and very little somewhere on foreground, see next picture. . . What else can be done? We can go other way round and not provide any ground truth fundamental matrix at all. Instead, we ask methods, which we evaluate, to give us such matrix F_est, then check, how many ground truth correspondences are consident with it. . The bad thing about it, is that unless all 100% GT correspondences are consistent with F_est, F_est is completely wrong in terms of camera pose accuracy. So, such a binary signal on a very challenging pairs is not that useful. The good thing about it, is that if, say, 50% GT correspondences are consistent with F_est, it means that detected (not GT) correspondences on the part of the image are correct (e.g. on some dominant plane) and at least there we are OK. . If you are interested in more mathematical definition, here it is: . The recall on ground truth correspondences $C_i$ of image pair $i$ and for geometry model $ mathbf{M}_i$ is computed as a function of a threshold $ theta$ . begin{equation} mathrm{r}_{i, mathbf{M}_i}( theta) = frac{| {( mathbf{u}, mathbf{v}) : ( mathbf{u}, mathbf{v}) in C_i, e( mathbf{M}_i, mathbf{u}, mathbf{v}) &lt; theta }|}{| C_i |} % mathrm{r}_{i, mathbf{M}_i}( theta) = frac{| {( mathbf{u}_i, mathbf{v}_i) : ( mathbf{u}_i, mathbf{v}_i) in C_i, e( mathbf{M}_i, mathbf{u}, mathbf{v}) &lt; theta }|}{| C_i |} end{equation}using symmetric epipolar distance. . Let&#39;s evaluate! . First I will show you how to run the WxBS benchmark yourself and then present the results I got. Benchmark is available from pip: . !pip install wxbs_benchmark !pip install kornia_moons . Now we will write a simple function using OpenCV RootSIFT and MAGSAC++. RootSIFT is a gold standard handcrafted local feature and MAGSAC++ is a state-of-the-art RANSAC, which is not sensitive to an inlier threshold, as my recent benchmark shows. We will match then using mutual second nearest ratio, as implemented in kornia . import matplotlib.pyplot as plt import numpy as np import cv2 import torch import kornia.feature as KF from kornia_moons.feature import * from wxbs_benchmark.dataset import * from wxbs_benchmark.evaluation import * from tqdm import tqdm def sift2rootsift(desc): desc /= desc.sum(axis=1, keepdims=True) + 1e-8 desc = np.sqrt(desc) return desc def estimate_F_WithRootSIFT(img1, img2): det = cv2.SIFT_create(8000, contrastThreshold=-10000, edgeThreshold=10000) kps1, descs1 = det.detectAndCompute(img1, None) kps2, descs2 = det.detectAndCompute(img2, None) descs1 = sift2rootsift(descs1) descs2 = sift2rootsift(descs2) snn_ratio, idxs = KF.match_smnn(torch.from_numpy(descs1), torch.from_numpy(descs2), 0.95) tentatives = cv2_matches_from_kornia(snn_ratio, idxs) src_pts = np.float32([ kps1[m.queryIdx].pt for m in tentatives ]).reshape(-1,2) dst_pts = np.float32([ kps2[m.trainIdx].pt for m in tentatives ]).reshape(-1,2) F, inlier_mask = cv2.findFundamentalMat(src_pts, dst_pts, cv2.USAC_MAGSAC, 0.25, 0.999, 100000) return F subset = &#39;test&#39; dset = WxBSDataset(&#39;.WxBS&#39;, subset=subset, download=True) F_results_RootSIFT = [] for pair_dict in tqdm(dset): current_F = estimate_F_WithRootSIFT(pair_dict[&#39;img1&#39;], pair_dict[&#39;img2&#39;]) F_results_RootSIFT.append(current_F) result_dict_rootsift, thresholds = evaluate_Fs(F_results_RootSIFT, subset) . We can check results for individual image pairs, or just take an average . print(result_dict_rootsift.keys()) plt.figure() plt.plot(thresholds, result_dict_rootsift[&#39;average&#39;], &#39;-x&#39;) plt.ylim([0,1.05]) plt.xlabel(&#39;Thresholds&#39;) plt.ylabel(&#39;Recall on GT corrs&#39;) plt.grid(True) plt.legend([&#39;RootSIFT + MAGSAC++&#39;]) . dict_keys([&#39;WGABS/kremlin&#39;, &#39;WGABS/kyiv&#39;, &#39;WGABS/strahov&#39;, &#39;WGABS/vatutin&#39;, &#39;WGALBS/bridge&#39;, &#39;WGALBS/flood&#39;, &#39;WGALBS/kyiv_dolltheater&#39;, &#39;WGALBS/rovenki&#39;, &#39;WGALBS/stadium&#39;, &#39;WGALBS/submarine&#39;, &#39;WGALBS/submarine2&#39;, &#39;WGALBS/tyn&#39;, &#39;WGALBS/zanky&#39;, &#39;WGBS/kn-church&#39;, &#39;WGLBS/alupka&#39;, &#39;WGLBS/berlin&#39;, &#39;WGLBS/charlottenburg&#39;, &#39;WGLBS/church&#39;, &#39;WGLBS/him&#39;, &#39;WGLBS/maidan&#39;, &#39;WGLBS/ministry&#39;, &#39;WGLBS/silasveta2&#39;, &#39;WGLBS/warsaw&#39;, &#39;WGSBS/kettle&#39;, &#39;WGSBS/kettle2&#39;, &#39;WGSBS/lab&#39;, &#39;WGSBS/lab2&#39;, &#39;WGSBS/window&#39;, &#39;WLABS/dh&#39;, &#39;WLABS/kpi&#39;, &#39;WLABS/kyiv&#39;, &#39;WLABS/ministry&#39;, &#39;average&#39;]) . /opt/homebrew/Caskroom/miniforge/base/envs/python39/lib/python3.9/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above. and should_run_async(code) . &lt;matplotlib.legend.Legend at 0x12dedf2e0&gt; . I have run several popular recent methods in this Colab. The code is very dirty, that is why I don&#39;t put it here, and just present results. . Methods tested . Local features without learned matching. . RootSIFT (ICCV1999 - CVPR-2012), DoG+HardNet (NeurIPS 2017), R2D2 (NeurIPS 2019), DISK (NeurIPS 2020). . Local features with learned matching. . SuperGlue (CVPR2020) uses attention-based graph neural network to match SuperPoint local features. . Dense matching . PDCNet (CVPR 2021). On top of pretrained ImageNet coarse-to-fine feature correlation, PDCNet predicts a dense flow and a confidence for each pixel. Trained on huge dataset with synthetic warps. . Patch2pix (CVPR 2021). Similarly to PDCNet, patch2pix is uses pretrained ImageNet features to get the initial corresponcences. Then the matches are progressively refined or rejected with a regressor network. . DFM (CVPRW 2021). Like PDCNet and Patch2pix, DFM uses ImageNet-pretrained and frozen VGGNet backbone as dense feature extractor. Unlike them, DFM does not requite any training and relies on simple handcrafted coarse-to-fine matching from conv5 to conv1. . Dense matching with transformers . COTR, (ICCV2021). Given the pixel location in a query image, COTR finds a corresponing location in the reference image. It operated on the 16x16 feature map produced by pretrained ImageNet model, which then are fed together with a positional embeddings to a transformer-based regressor. It is also the slowest method among evaluated - because you have to run it per each pixel separately, if want dense correspondences. . LoFTR (CVPR2021). LoFTR is very similar to Patch2pix, with the difference of using transformer architecture to for both coarse initial matching and consequent refinement. The second difference, is that, unlike all other methods, which are using ImageNet-pretrained models as a feature backbone, LoFTR trains ResNet18 from scratch . . Results &amp; Discussion . The clear leader is LoFTR, then we have (patch2pix, COTR and SuperGlue) group, followed by (PDCNet, DISK, R2D2). Then DoG-HardNet and RootSIFT, where latter has barely matched couple of image pairs correctly. . I have no idea, why LoFTR is so good, my guesses would be richer training data and training the backbone from scratch, instead of relying on ImageNet feaures. I am also quite surprised by the great performance of the patch2pix, which probably can be improved even more, if the backbone would be at least fine-tuned. . Another surprise is a great performance of the DFM matcher, which is slightly better than PDCNet and worse (but comparable) than COTR. My hypothesis is that all this methods rely on the same ImageNet backbone: if they fail, all the methods fail. . Regarding SuperGlue - one of my guesses is that original SuperPoint features were not trained for different illuminations. Moreover, in some images, there just not enough matching corners - when I was annotating the correspondences, I have to use also &quot;center of the blobs&quot;. . Finally, I would like to that than even excellent LofTR result is far from the ideal -- there are stil tens of pairs, where it fails. . P.S. I would also like to thank Prune Truong and Ufuk Efe who helped me to debug my originally wrong run of the PDCNet and DFM respectively. . Qualiative examples . Here are some examples of how different methods work on the same image pairs. . Season + viewpoint . LoFTR . . SuperGlue . . patch2pix . . Thermal vs visible + viewpoint . LoFTR . . SuperGlue . . patch2pix . . Season + illumination + viewpoint . LoFTR . . SuperGlue . . patch2pix . . Illumination + viewpoint change (zoom) . LoFTR . . SuperGlue . patch2pix . . As you can, see, there are many incorrect correspondences even in those images, where methods are performing well. . Here I will conclude and go to vacation. . In the 2nd part of the post, I would like to evaluate COTR, PDCNet and different dense descriptors directly on the GT correspondences, by giving a point in image A and asking for point in image B. .",
            "url": "/wide-baseline-stereo-blog/2021/07/30/Reviving-WxBS-benchmark.html",
            "relUrl": "/2021/07/30/Reviving-WxBS-benchmark.html",
            "date": " • Jul 30, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Submitting to IMC 2021 with custom matcher",
            "content": "Intro . In the previous tutorial we have created a pipeline for submitting to the IMC2021. However, we have not covered all the possibilities. First, we haven&#39;t submissted custom matches, instead we just run a standard Lowe SNN test with cross-check. Second, we haven&#39;t evaluate multiview (COLMAP) part. In this tutorial we will do both. I am assuming that you have completed the previous part. If not, please do, because we will be relying on the already extracted features. Let&#39;s check if the feature are there . cd imc2021-sample-kornia-submission ls extracted/cv2-dog-affnet-hardnet8/googleurban/edinburgh/ . angles.h5 descriptors.h5 keypoints.h5 scales.h5 scores.h5 . . Warning: There is a subtle problem with previous tutorial, so we cannot use pre-extracted features. Specifically, because OpenCV SIFT does not exactly respect max_features parameter and can sometimes output 8002 features, instead of 8000. When we were importing the features alone, benchmark import_features.py script automatically re-sorted features based on the score and clip the extra 1-3 features. However, this functionality is not available for the importing custom matches. I have already corrected previous post, so you can use it for the re-extration. Or, if you are reading this whole tutorial after May 24, just ignore this. . Now we will install AdaLAM - one of the winners of IMC2020 Challenge. It uses keypoint geometry to filter out unreliable matches. . pip install git+https://github.com/cavalli1234/AdaLAM.git . Let&#39;s check if it works on the sample image pair. We will read the pre-extracted features for it. . import matplotlib.pyplot as plt import numpy as np import cv2 import os import torch import kornia as K import kornia.feature as KF import h5py import json from PIL import Image from adalam import AdalamFilter from kornia_moons.feature import * def load_h5(filename): &#39;&#39;&#39;Loads dictionary from hdf5 file&#39;&#39;&#39; dict_to_load = {} try: with h5py.File(filename, &#39;r&#39;) as f: keys = [key for key in f.keys()] for key in keys: dict_to_load[key] = f[key][()] except: print(&#39;Cannot find file {}&#39;.format(filename)) return dict_to_load PATH_TO_FEATS = &#39;extracted/cv2-dog-affnet-hardnet8/googleurban/edinburgh/&#39; kps = load_h5(os.path.join(PATH_TO_FEATS, &#39;keypoints.h5&#39;)) angles = load_h5(os.path.join(PATH_TO_FEATS, &#39;angles.h5&#39;)) scales = load_h5(os.path.join(PATH_TO_FEATS, &#39;scales.h5&#39;)) descs = load_h5(os.path.join(PATH_TO_FEATS, &#39;descriptors.h5&#39;)) . I have selected two images, which are matching: . IMG_DIR = &#39;../imc-2021-data/googleurban/edinburgh/set_100/images/&#39; img1_key = &#39;2b5315968bc5468c995b978620879439&#39; img2_key = &#39;6264aee21d1b48b7985901c4bedfdbd4&#39; img1 = cv2.cvtColor(cv2.imread(os.path.join(IMG_DIR, f&#39;{img1_key}.png&#39;)), cv2.COLOR_BGR2RGB) img2 = cv2.cvtColor(cv2.imread(os.path.join(IMG_DIR, f&#39;{img2_key}.png&#39;)), cv2.COLOR_BGR2RGB) plt.imshow(np.concatenate([img1, img2], axis=1)) . &lt;matplotlib.image.AxesImage at 0x7fa29003f550&gt; . We will start with matching and drawing the matches with OpenCV for sanity check. . def opencv_from_imc(kps, sizes, angles): return [cv2.KeyPoint(kp[0], kp[1], float(s), float(a)) for kp, s, a in zip(kps, sizes, angles)] def get_data(kps, angles, scales, descs, img_key): kp1 = kps[img_key] s1 = scales[img_key] a1 = angles[img_key] descs1 = descs[img_key] return kp1, s1, a1, descs1 def match(img1_key, img2_key, kps, angles, scales, descs): kp1, s1, a1, descs1 = get_data(kps, angles, scales, descs, img1_key) kp2, s2, a2, descs2 = get_data(kps, angles, scales, descs, img2_key) dists, idxs = KF.match_smnn(torch.from_numpy(descs1), torch.from_numpy(descs2), 0.9) return dists, idxs def draw_matches(img1_key, img2_key, dists, idxs, kps, angles, scales, descs): tentatives = cv2_matches_from_kornia(dists, idxs) draw_params = dict(matchColor = (255,255,0), # draw matches in yellow color singlePointColor = None, matchesMask = [True for x in idxs], # draw only inliers flags = 2) img1 = cv2.cvtColor(cv2.imread(os.path.join(IMG_DIR, f&#39;{img1_key}.png&#39;)), cv2.COLOR_BGR2RGB) img2 = cv2.cvtColor(cv2.imread(os.path.join(IMG_DIR, f&#39;{img2_key}.png&#39;)), cv2.COLOR_BGR2RGB) kp1, s1, a1, _ = get_data(kps, angles, scales, descs, img1_key) kp2, s2, a2, descs2 = get_data(kps, angles, scales, descs, img2_key) img_out = cv2.drawMatches(img1,opencv_from_imc(kp1, s1, a1), img2,opencv_from_imc(kp2, s2, a2), tentatives,None,**draw_params) plt.figure() fig, ax = plt.subplots(figsize=(15, 15)) ax.imshow(img_out, interpolation=&#39;nearest&#39;) return dists, idxs = match(img1_key, img2_key, kps, angles, scales, descs) draw_matches(img1_key, img2_key, dists, idxs, kps, angles, scales, descs) . &lt;Figure size 432x288 with 0 Axes&gt; . Everything seems to be working quite well. But, as you can see, we have some wrong matches among tentatives. Let&#39;s try AdaLAM . def match_adalam(img1_key, img2_key, kps, angles, scales, descs): kp1, s1, a1, descs1 = get_data(kps, angles, scales, descs, img1_key) kp2, s2, a2, descs2 = get_data(kps, angles, scales, descs, img2_key) matcher = AdalamFilter() # AdaLAM wants image sizes, so we have to read them. img1 = cv2.cvtColor(cv2.imread(os.path.join(IMG_DIR, f&#39;{img1_key}.png&#39;)), cv2.COLOR_BGR2RGB) img2 = cv2.cvtColor(cv2.imread(os.path.join(IMG_DIR, f&#39;{img2_key}.png&#39;)), cv2.COLOR_BGR2RGB) idxs = matcher.match_and_filter(kp1, kp2, descs1, descs2, im1shape=img1.shape[:2], im2shape=img2.shape[:2], o1=a1.reshape(-1), o2=a2.reshape(-1), s1=s1.reshape(-1), s2=s2.reshape(-1)) # AdaLAM does not provide confidence score, so we will create dummy one dists = torch.ones_like(idxs)[:,0] return dists, idxs dists, idxs = match_adalam(img1_key, img2_key, kps, angles, scales, descs) draw_matches(img1_key, img2_key, dists, idxs, kps, angles, scales, descs) . &lt;Figure size 432x288 with 0 Axes&gt; . We have significantly more matches, which are also cleaner, although not ideal. There is one more thing: if we are going to submit custom matches, we have to run RANSAC ourself (if needed), here is a quote from the competition rules: . The &quot;geom&quot;/&quot;method&quot; field must be set to &quot;cv2-8pt&quot; if custom matches are enabled, as we assume you tune and run your favourite RANSAC algorithm, if applicable. . Thus, we need to merge RANSAC (we pick inlier threshold from the previous tutorial) and matching: . import pydegensac def match_adalam_with_degensac(img1_key, img2_key, kps, angles, scales, descs, ds_name=&#39;phototourism&#39;): kp1, s1, a1, descs1 = get_data(kps, angles, scales, descs, img1_key) kp2, s2, a2, descs2 = get_data(kps, angles, scales, descs, img2_key) matcher = AdalamFilter() # AdaLAM wants image sizes, so we have to read them. img1 = cv2.cvtColor(cv2.imread(os.path.join(IMG_DIR, f&#39;{img1_key}.png&#39;)), cv2.COLOR_BGR2RGB) img2 = cv2.cvtColor(cv2.imread(os.path.join(IMG_DIR, f&#39;{img2_key}.png&#39;)), cv2.COLOR_BGR2RGB) idxs = matcher.match_and_filter(kp1, kp2, descs1, descs2, im1shape=img1.shape[:2], im2shape=img2.shape[:2], o1=a1.reshape(-1), o2=a2.reshape(-1), s1=s1.reshape(-1), s2=s2.reshape(-1)).detach().cpu().numpy() src_pts = kp1[idxs[:,0]] dst_pts = kp2[idxs[:,1]] max_iters = 100000 if ds_name.lower() == &#39;phototourism&#39;: inl_th = 0.5 elif ds_name.lower() == &#39;pragueparks&#39;: inl_th = 1.5 elif ds_name.lower() == &#39;googleurban&#39;: inl_th = 0.75 else: raise ValueError(&#39;Unknown dataset&#39;) F, inliers_mask = pydegensac.findFundamentalMatrix(src_pts, dst_pts, inl_th, 0.999999, max_iters) out_idxs = idxs[inliers_mask] # AdaLAM does not provide confidence score, so we will create dummy one dists = np.ones_like(out_idxs)[:,0] return dists, out_idxs dists, idxs = match_adalam_with_degensac(img1_key, img2_key, kps, angles, scales, descs) draw_matches(img1_key, img2_key, dists, idxs, kps, angles, scales, descs) . &lt;Figure size 432x288 with 0 Axes&gt; . The last, but not least: we have to transpose output indexes, as benchmark expects them in the shape [2 x B]. Do not ask why :) . So, we are ready to process and save our matches. I have a bit optimized the loading process, but not much. Do not expect blazing speed :) The full script is accesible here. . import os import h5py from tqdm import tqdm from PIL import Image def match_adalam_with_degensac(kp1, kp2, s1, s2, a1, a2, descs1, descs2, h1, w1, h2, w2, ds_name=&#39;phototourism&#39;): matcher = AdalamFilter() idxs = matcher.match_and_filter(kp1, kp2, descs1, descs2, im1shape=(h1,w1), im2shape=(h2,w2), o1=a1.reshape(-1), o2=a2.reshape(-1), s1=s1.reshape(-1), s2=s2.reshape(-1)).detach().cpu().numpy() if len(idxs) &lt; 7: return np.empty((0,1), dtype=np.float32), np.empty((0,2), dtype=np.int32) src_pts = kp1[idxs[:,0]] dst_pts = kp2[idxs[:,1]] max_iters = 100000 if ds_name.lower() == &#39;phototourism&#39;: inl_th = 0.5 elif ds_name.lower() == &#39;pragueparks&#39;: inl_th = 1.5 elif ds_name.lower() == &#39;googleurban&#39;: inl_th = 0.75 else: raise ValueError(&#39;Unknown dataset&#39;) F, inliers_mask = pydegensac.findFundamentalMatrix(src_pts, dst_pts, inl_th, 0.999999, max_iters) out_idxs = idxs[inliers_mask] # AdaLAM does not provide confidence score, so we will create dummy one dists = np.ones_like(out_idxs)[:,0] return dists, out_idxs INPUT_DIR = &#39;../imc-2021-data&#39; OUT_DIR = &#39;extracted/cv2-dog-affnet-hardnet8&#39; os.makedirs(OUT_DIR, exist_ok=True) datasets = os.listdir(INPUT_DIR) datasets = [&#39;googleurban&#39;] for ds in datasets: ds_in_path = os.path.join(INPUT_DIR, ds) ds_out_path = os.path.join(OUT_DIR, ds) os.makedirs(ds_out_path, exist_ok=True) seqs = os.listdir(ds_in_path) for seq in seqs: if os.path.isdir(os.path.join(ds_in_path, seq, &#39;set_100&#39;)): seq_in_path = os.path.join(ds_in_path, seq, &#39;set_100&#39;, &#39;images&#39;) else: seq_in_path = os.path.join(ds_in_path, seq) seq_out_path = os.path.join(ds_out_path, seq) kps = load_h5(os.path.join(seq_out_path, &#39;keypoints.h5&#39;)) angles = load_h5(os.path.join(seq_out_path, &#39;angles.h5&#39;)) scales = load_h5(os.path.join(seq_out_path, &#39;scales.h5&#39;)) descs = load_h5(os.path.join(seq_out_path, &#39;descriptors.h5&#39;)) img_fnames = sorted(os.listdir(seq_in_path))[::-1] num_matches = [] with h5py.File(f&#39;{seq_out_path}/matches_stereo_0.h5&#39;, &#39;w&#39;) as f_m: for i1, img1_fname in tqdm(enumerate(img_fnames)): img1_key = os.path.splitext(os.path.basename(img1_fname))[0] img1_fname_full = os.path.join(seq_in_path, img1_fname) img1 = Image.open(img1_fname_full) w1, h1 = img1.size kp1, s1, a1, descs1 = get_data(kps, angles, scales, descs, img1_key) for img2_fname in img_fnames[i1+1:]: img2_key = os.path.splitext(os.path.basename(img2_fname))[0] img2_fname_full = os.path.join(seq_in_path, img2_fname) img2 = Image.open(img2_fname_full) w2, h2 = img2.size match_key = f&#39;{img1_key}-{img2_key}&#39; kp2, s2, a2, descs2 = get_data(kps, angles, scales, descs, img2_key) _, idxs = match_adalam_with_degensac(kp1, kp2, s1, s2, a1, a2, descs1, descs2, h1, w1, h2, w2, ds_name=ds) num_matches.append(len(idxs)) if len(idxs) == 0: idxs = np.empty([0, 2], dtype=np.int32) idxs = idxs.T assert idxs.shape[0] == 2 f_m[match_key] = idxs print(f&#39;Finished processing &quot;{ds}/{seq}&quot; -&gt; {np.array(num_matches).mean()} matches/image&#39;) #We can use a single match file for multiview and stereo, but let&#39;s pretend that we have different ones copyfile(f&#39;{seq_out_path}/matches_stereo_0.h5&#39;, f&#39;{seq_out_path}/matches_multiview.h5&#39;) . Matching will take us couple of hours on GPU. . Creating config json file . In addition to features and matches, we should submit a config file, which tells the benchmark, how the features should be matched and which RANSAC (no options, only cv2-8pt for using custom matches) we prefer. So, we will create two config files - one, which standard matcher and second one, which uses AdaLAM. . First part of the config is metadata -- information about the method and authors. If your method is under review, you may want to set flag publish_anonymously to True. . metadata_dict = { &quot;publish_anonymously&quot;: False, &quot;authors&quot;: &quot;Dmytro Mishkin, Milan Pultar and kornia team&quot;, &quot;contact_email&quot;: &quot;ducha.aiki@gmail.com&quot;, &quot;method_name&quot;: &quot;CV-DoG-AffNet-HardNet8-AdaLAM-DEGENSAC&quot;, &quot;method_description&quot;: r&quot;&quot;&quot;OpeCV SIFT keypoints 8000 features, followed by the AffNet normalization and HardNet8 descriptor as implemented in kornia. Matched using AdaLAM with DEGENSAC&quot;&quot;&quot;, &quot;link_to_website&quot;: &quot;https://github.com/kornia/kornia&quot;, &quot;link_to_pdf&quot;: &quot;https://arxiv.org/abs/2007.09699&quot; } . Second part is config_common: it tells the benchmark, which keypoints and descriptors you use. We will also need this names when importing our features during tuning on the validation set. . config_common_dict = {&quot;json_label&quot;: &quot;dog-affnet-hardnet8-degensac-adalam&quot;, &quot;keypoint&quot;: &quot;cv2dog&quot;, &quot;descriptor&quot;: &quot;affnethardnet8&quot;, &quot;num_keypoints&quot;: 8000} . Finally, we have to specify robust geometry estimation method. We have no other choice than cv2-8pt, as pre-filtering with DEGENSAC is already performed. . from copy import deepcopy geom_template_dict = {&quot;method&quot;: &quot;cv2-8pt&quot; } . Let&#39;s assemble and save our base config. . import json base_config = { &quot;metadata&quot;: metadata_dict, &quot;config_common&quot;: config_common_dict, &quot;config_phototourism_stereo&quot;: { &quot;use_custom_matches&quot;: True, &quot;custom_matches_name&quot;: &quot;adalam&quot;, &quot;geom&quot;: deepcopy(geom_template_dict) }, &quot;config_phototourism_multiview&quot;: { &quot;use_custom_matches&quot;: True, &quot;custom_matches_name&quot;: &quot;adalam&quot;, &quot;colmap&quot;: {}}, &quot;config_pragueparks_stereo&quot;: { &quot;use_custom_matches&quot;: True, &quot;custom_matches_name&quot;: &quot;adalam&quot;, &quot;geom&quot;: deepcopy(geom_template_dict) }, &quot;config_pragueparks_multiview&quot;: { &quot;use_custom_matches&quot;: True, &quot;custom_matches_name&quot;: &quot;adalam&quot;, &quot;colmap&quot;: {}}, &quot;config_googleurban_stereo&quot;: { &quot;use_custom_matches&quot;: True, &quot;custom_matches_name&quot;: &quot;adalam&quot;, &quot;geom&quot;: deepcopy(geom_template_dict) }, &quot;config_googleurban_multiview&quot;: { &quot;use_custom_matches&quot;: True, &quot;custom_matches_name&quot;: &quot;adalam&quot;, &quot;colmap&quot;: {}} } . Finally, benchmark expects multiple configs, so we have to create a list, and then we can save our config . import json with open(&#39;base_config_adalam.json&#39;, &#39;w&#39;) as f: json.dump([base_config], f, indent=2) . Preliminary evaluation . Now let&#39;s check how our features perform on validation set. We have to import our feature to the benchmark and run the benchmark. . I will cheat a little bit here and skip the multiview evaluation. The reason is that it requires colmap, which might be not easy to install. . Importing features . Here we have to provide the same keypoint and descriptor names, as we wrote in json config. The rest of arguments are straightforward: path to features, json, etc. . cd ../image-matching-benchmark/ python -utt import_features.py --kp_name cv2dog --desc_name affnethardnet8 --num_keypoints 8000 --path_features ../imc2021-sample-kornia-submission/extracted/cv2-dog-affnet-hardnet8 --path_results ../benchmark-results --subset both --is_challenge false --path_json ../imc2021-sample-kornia-submission/base_config_adalam.json --datasets phototourism googleurban --match_name adalam . Running the evaluation . Now we are ready to run the evaluation. Note, that now we are running multiview evaluation as well. So, let us check if colmap is available. . colmap --help . The output should be something like that: . COLMAP 3.6 -- Structure-from-Motion and Multi-View Stereo (Commit Unknown on Unknown with CUDA) . OK, now time to run the evaluation . python -utt run.py --run_mode=interactive --json_method=../imc2021-sample-kornia-submission/base_config_adalam.json --subset=val --path_data ../imc-2021-data/ --path_results ../benchmark-results --is_challenge false . After a while (an 6 hours for 32 cores machine), the process will finish and you will see the following log message: . -- Saving to: &quot;packed-val/dog-affnet-hardnet8-degensac-adalam.json&quot; . To compare with results without AdaLAM, let&#39;s also run multiview evaluation for the previous setup. I did not do that, because of the time it takes. . bash python -utt run.py --run_mode=interactive --json_method=../imc2021-sample-kornia-submission/base_config.json --subset=val --path_data ../imc-2021-data/ --path_results ../benchmark-results --is_challenge false . Reading results . Json file with evaluation results are saved to image-matching-benchmark/packed-val/dog-affnet-hardnet8-degensac.json and image-matching-benchmark/packed-val/dog-affnet-hardnet8-degensac-adalam.json , and some visualizations -- to ../benchmark-visualization/png. . First, we come back to our imc2021-sample-kornia-submission directory: . cd ../imc2021-sample-kornia-submission . Metric, which are used for the competition is mean average accuracy (mAA) at visibility threshold 0.1 . import os hashname=&#39;dog-affnet-hardnet8-degensac&#39; res_fname = os.path.join(&#39;../image-matching-benchmark/packed-val&#39;, f&#39;{hashname}.json&#39;) with open(res_fname, &#39;r&#39;) as f: results = json.load(f) submission_name = results[&#39;config&#39;][&#39;metadata&#39;][&#39;method_name&#39;] datasets = [&#39;phototourism&#39;, &#39;pragueparks&#39;, &#39;googleurban&#39;] tasks = [&#39;stereo&#39;, &#39;multiview&#39;] metric = &#39;qt_auc_10_th_0.1&#39; for dset in datasets: mAA_stereo = results[dset][&#39;results&#39;][&#39;allseq&#39;][&#39;stereo&#39;][&#39;run_avg&#39;][metric][&#39;mean&#39;] mAA_colmap = results[dset][&#39;results&#39;][&#39;allseq&#39;][&#39;multiview&#39;][&#39;run_avg&#39;][&#39;bag_avg&#39;][&#39;qt_auc_colmap_10&#39;][&#39;mean&#39;] print (f&#39;{submission_name} stereo mAA for {dset} is {mAA_stereo:.4f}&#39;) print (f&#39;{submission_name} multiview mAA for {dset} is {mAA_colmap:.4f}&#39;) # Remember, I did not run multiview evaluation for the original submission. . CV-DoG-AffNet-HardNet8 (kornia) stereo mAA for phototourism is 0.7108 CV-DoG-AffNet-HardNet8 (kornia) multiview mAA for phototourism is 0.0000 CV-DoG-AffNet-HardNet8 (kornia) stereo mAA for pragueparks is 0.5850 CV-DoG-AffNet-HardNet8 (kornia) multiview mAA for pragueparks is 0.0000 CV-DoG-AffNet-HardNet8 (kornia) stereo mAA for googleurban is 0.3099 CV-DoG-AffNet-HardNet8 (kornia) multiview mAA for googleurban is 0.0000 . import os hashname=&#39;dog-affnet-hardnet8-degensac-adalam&#39; res_fname = os.path.join(&#39;../image-matching-benchmark/packed-val&#39;, f&#39;{hashname}.json&#39;) with open(res_fname, &#39;r&#39;) as f: results_adalam = json.load(f) submission_name_adalam = results_adalam[&#39;config&#39;][&#39;metadata&#39;][&#39;method_name&#39;] datasets = [&#39;phototourism&#39;, &#39;pragueparks&#39;, &#39;googleurban&#39;] tasks = [&#39;stereo&#39;] # [&#39;stereo&#39;, &#39;multiview&#39;] #Remember, that we skip colmap evaluations metric = &#39;qt_auc_10_th_0.1&#39; for dset in datasets: mAA_stereo = results_adalam[dset][&#39;results&#39;][&#39;allseq&#39;][&#39;stereo&#39;][&#39;run_avg&#39;][metric][&#39;mean&#39;] mAA_colmap = results_adalam[dset][&#39;results&#39;][&#39;allseq&#39;][&#39;multiview&#39;][&#39;run_avg&#39;][&#39;bag_avg&#39;][&#39;qt_auc_colmap_10&#39;][&#39;mean&#39;] print (f&#39;{submission_name_adalam} stereo mAA for {dset} is {mAA_stereo:.4f}&#39;) print (f&#39;{submission_name_adalam} multiview mAA for {dset} is {mAA_colmap:.4f}&#39;) . CV-DoG-AffNet-HardNet8-AdaLAM-DEGENSAC stereo mAA for phototourism is 0.7326 CV-DoG-AffNet-HardNet8-AdaLAM-DEGENSAC multiview mAA for phototourism is 0.8566 CV-DoG-AffNet-HardNet8-AdaLAM-DEGENSAC stereo mAA for pragueparks is 0.6624 CV-DoG-AffNet-HardNet8-AdaLAM-DEGENSAC multiview mAA for pragueparks is 0.5584 CV-DoG-AffNet-HardNet8-AdaLAM-DEGENSAC stereo mAA for googleurban is 0.3310 CV-DoG-AffNet-HardNet8-AdaLAM-DEGENSAC multiview mAA for googleurban is 0.1597 . We can also see results sequence-by-sequence . import seaborn as sns import matplotlib.pyplot as plt import numpy as np sns.set_context(&#39;paper&#39;, font_scale=1.7) seqs = [] mAAs_adalam = [] mAAs = [] for dset in datasets: for task in tasks: for seq in results[dset][&#39;results&#39;].keys(): if seq == &#39;allseq&#39;: continue mAA = results[dset][&#39;results&#39;][seq][task][&#39;run_avg&#39;][metric][&#39;mean&#39;] mAA_adalam = results_adalam[dset][&#39;results&#39;][seq][task][&#39;run_avg&#39;][metric][&#39;mean&#39;] mAAs.append(mAA) mAAs_adalam.append(mAA_adalam) seqs.append(seq) fig, ax = plt.subplots(figsize=(15,5)) xticks = 2*np.arange(len(seqs)) ax.set_xticks(xticks) ax.bar(xticks, mAAs) ax.bar(xticks+0.5, mAAs_adalam) ax.set_xticklabels(seqs) ax.legend([&#39;SMNN&#39;, &#39;AdaLAM&#39;]) ax.set_ylabel(&#39;mAA&#39;) ax.set_xlabel(&#39;Sequence&#39;) . Text(0.5, 0, &#39;Sequence&#39;) . How do our feature correspondences look like? . Creating final submission . Its time to create our final submission! . configs = [] current_config = deepcopy(base_config) current_config[&#39;metadata&#39;][&#39;method_name&#39;] = &#39;KORNIA TUTORIAL AdaLAM CV-DoG-AffNet-HardNet8&#39; label = current_config[&#39;config_common&#39;][&#39;json_label&#39;] current_config[&#39;config_common&#39;][&#39;json_label&#39;] = f&#39;{label}&#39; configs.append(current_config) print (current_config) with open(&#39;final_submission_adalam.json&#39;, &#39;w&#39;) as f: json.dump(configs, f, indent=2) . {&#39;metadata&#39;: {&#39;publish_anonymously&#39;: False, &#39;authors&#39;: &#39;Dmytro Mishkin, Milan Pultar and kornia team&#39;, &#39;contact_email&#39;: &#39;ducha.aiki@gmail.com&#39;, &#39;method_name&#39;: &#39;KORNIA TUTORIAL AdaLAM CV-DoG-AffNet-HardNet8&#39;, &#39;method_description&#39;: &#39;OpeCV SIFT keypoints 8000 features, followed by the AffNet normalization n and HardNet8 descriptor as implemented in kornia. n Matched using AdaLAM with DEGENSAC&#39;, &#39;link_to_website&#39;: &#39;https://github.com/kornia/kornia&#39;, &#39;link_to_pdf&#39;: &#39;https://arxiv.org/abs/2007.09699&#39;}, &#39;config_common&#39;: {&#39;json_label&#39;: &#39;dog-affnet-hardnet8-degensac-adalam&#39;, &#39;keypoint&#39;: &#39;cv2dog&#39;, &#39;descriptor&#39;: &#39;affnethardnet8&#39;, &#39;num_keypoints&#39;: 8000}, &#39;config_phototourism_stereo&#39;: {&#39;use_custom_matches&#39;: True, &#39;custom_matches_name&#39;: &#39;adalam&#39;, &#39;geom&#39;: {&#39;method&#39;: &#39;cv2-8pt&#39;}}, &#39;config_phototourism_multiview&#39;: {&#39;use_custom_matches&#39;: True, &#39;custom_matches_name&#39;: &#39;adalam&#39;, &#39;colmap&#39;: {}}, &#39;config_pragueparks_stereo&#39;: {&#39;use_custom_matches&#39;: True, &#39;custom_matches_name&#39;: &#39;adalam&#39;, &#39;geom&#39;: {&#39;method&#39;: &#39;cv2-8pt&#39;}}, &#39;config_pragueparks_multiview&#39;: {&#39;use_custom_matches&#39;: True, &#39;custom_matches_name&#39;: &#39;adalam&#39;, &#39;colmap&#39;: {}}, &#39;config_googleurban_stereo&#39;: {&#39;use_custom_matches&#39;: True, &#39;custom_matches_name&#39;: &#39;adalam&#39;, &#39;geom&#39;: {&#39;method&#39;: &#39;cv2-8pt&#39;}}, &#39;config_googleurban_multiview&#39;: {&#39;use_custom_matches&#39;: True, &#39;custom_matches_name&#39;: &#39;adalam&#39;, &#39;colmap&#39;: {}}} . Submission Zip file should have folder structure as follow: ├── config.json ├── [Dataset 1] │ ├── [Sequence 1] │ │ ├── keypoints.h5 │ │ ├── descriptors.h5 │ │ ├── matches.h5 │ ├── [Sequence 2] │ │ ├── ... ├── [Dataset 2] │ ├── ... . So we have to just copy our features, add config and zip them. . cp final_submission_adalam.json extracted/cv2-dog-affnet-hardnet8/config.json cd extracted/cv2-dog-affnet-hardnet8 zip -r submission.zip * . Last step before the submission - check the submission for correctness with provided script . cd ../../../image-matching-benchmark python -utt submission_validator.py --submit_file_path ../imc2021-sample-kornia-submission/extracted/cv2-dog-affnet-hardnet8/submission.zip --benchmark_repo_path . --raw_data_path ../imc-2021-data/ --datasets googleurban phototourism pragueparks . If everything is correct, you will see: . Validating method 1/1: &quot;dog-affnet-hardnet8-degensac&quot; [&#39;googleurban&#39;, &#39;phototourism&#39;, &#39;pragueparks&#39;] Running: googleurban, stereo track Running: googleurban, multiview track Running: phototourism, stereo track Running: phototourism, multiview track Running: pragueparks, stereo track Running: pragueparks, multiview track Validating key &quot;config_googleurban_stereo&quot; Validating key &quot;config_googleurban_multiview&quot; Validating key &quot;config_phototourism_stereo&quot; Validating key &quot;config_phototourism_multiview&quot; Validating key &quot;config_pragueparks_stereo&quot; Validating key &quot;config_pragueparks_multiview&quot; . And file submission_log.txt will appear near our .zip file. . cat ../imc2021-sample-kornia-submission/extracted/cv2-dog-affnet-hardnet8/submission_log.txt . Submission is in proper format, please submit to IMW 2021 website. . That&#39;s all, folks! We can submit! But, please, do not just submit this sample submission - make your own :) .",
            "url": "/wide-baseline-stereo-blog/2021/05/27/submitting-to-IMC2021-with-custom-matcher.html",
            "relUrl": "/2021/05/27/submitting-to-IMC2021-with-custom-matcher.html",
            "date": " • May 27, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Evaluating OpenCV new RANSACs",
            "content": "OpenCV RANSAC is dead. Long live the OpenCV USAC! . Year ago we published a paper &quot;Image Matching across Wide Baselines: From Paper to Practice&quot;, which, among other messages, has shown that OpenCV RANSAC for fundamental matrix estimation is terrible: it was super inaccurate and slow. Since then my colleague Maksym Ivashechkin has spent a summer 2020 improving OpenCV RANSACs. His work was released as a part of OpenCV 4.5.0 release. . Now it is time to benchmark them. Let&#39;s go! . Evaluation methodology . The benchmark is done on the validation subset of the Image Matching Challenge 2021 datasets. We have detected RootSIFT features, matched them with optimal mutual SNN ratio test and feed into the tested RANSACs. The resulting fundamental matrixes were transformed into relative poses and compared to the ground truth poses. You can check details in the paper &quot;Image Matching across Wide Baselines: From Paper to Practice&quot;. . For all RANSACs we first determine the optimal inlier threshold by the grid search, whereas number of iterations (max_iter) was set to a reasonable 100k. Then, after fixing this optimal threshold, we vary number of iterations from 10 to 10M. This gives us an accuracy-time curve. . Methods evaluated . Non-OpenCV methods: . DEGENSAC - from pydegensac package, based on the original implementation of the method, proposed in CVPR 2005 paper &quot;Two-View Geometry Estimation Unaffected by a Dominant Plane&quot;. It is the default choise for the Image Matching Challenge 2020 and 2021. | PyRANSAC - also from pydegensac package, with flag enable_degeneracy_check=False, which is equivalent to a vanilla LO-RANSAC implementation. | . OpenCV methods, named after the flag, one needs to pass into cv2.findFundamentalMatrix function: . USAC_DEFAULT – LO-RANSAC + degeneracy tests | USAC_FAST – LO-RANSAC + degeneracy tests. Fewer iterations in local optimization step than USAC_DEFAULT. Uses RANSAC score to maximize number of inliers and terminate earlier. | USAC_ACCURATE. Implements Graph-Cut RANSAC + degeneracy tests. | USAC_MAGSAC – MAGSAC++ implementation + degeneracy tests. | RANSAC -- OpenCV RANSAC implementation from the previous versions of the library, no degeneracy tests | . All OpenCV USAC methods also use SPRT-test for speeding-up the evaluation. . Results . Here are results for all 3 datasets. The lefter and upper is curve, the better. Dashed vertical line marks 1/25 sec (&quot;realtime&quot;) and 0.5 sec (challenge limit) time budget. Legend shows the method name and the optimal inlier threshold for the datasets: Phototourism, GoogleUrban and PragueParks respectively. . . The first and main message -- all new flags are much better than the old OpenCV implementation (green curve, worst results), which still a default option. . | 10k iterations and USAC_ACCURATE (red curve) gives you great results within 0.01 sec . | All OpenCV advanced USACs are better than for the small/medium time budget (&lt; 0.1 sec per image) than pydegensac (blue curve). . | The best methods for the higher budget are OpenCV USAC_MAGSAC and DEGENSAC from the pydegensac package. . | There is no point is using flag &quot;USAC_FAST&quot; it is always better to use USAC_DEFAULT, USAC_ACCURATE or USAC_MAGSAC. . | USAC_MAGSAC is the only method, which optimal threshold is the same across all datasets. This is a valuable property for practice, as it requires the least tuning. . | If you are interesting in results for an individual datasets, here they are. . Phototourism . GoogleUrban . PragueParks . Why do I tune and evaluate on the same set? . It is true, that tuning and evaluation of the method on the same dataset does not make any sense. However, let me defend my choice. Here are the arguments: . I do not want to compromise an integrity of the test set, which is the basis of the on-going competition Image Matching Challenge 2021 with prize money. That is why I do not want to leak information from the abovementioned test set and this is my primarly optimization objective. I also cannot tune the threshold on the &quot;training subset&quot;, as both GoogleUrban and PragueParks do not have such. . | I am interested more in the rough speed-accuracy trade-off than the precise rankings of the methods. It is quite likely, that those methods, which have an small acuracy gap on the validation set, would switch on the test set -- as it happened with DEGENSAC and MAGSAC in our original paper. However, it is very unlikely, that method, which performs poorly on the validation set would magically outperform everyone on the test set. Again, see PyRANSAC vs DEGENSAC in the original paper. . | I clearly state this fact as a limitation and do not publish a paper ;) . | Conclusion . New OpenCV RANSACs are fast and have comparable accuracy, you can safely pick one of them. However, if you are using pydegensac and have &gt; 0.1 sec time budget, there is no need to switch. . Use proper RANSACs and be happy :) .",
            "url": "/wide-baseline-stereo-blog/2021/05/17/OpenCV-New-RANSACs.html",
            "relUrl": "/2021/05/17/OpenCV-New-RANSACs.html",
            "date": " • May 17, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Image Matching Challenge 2020 Recap",
            "content": "What is Image Matching Challenge? . Image Matching Challenge is an on-going benchmark of the local features, matching methods and RANSACs, held since 2019. Its main idea is to measure a downstream metric of the image matching pipeline, such as camera pose accuracy after a careful hyperparameter tuning. . . Before the challenge we have benchmarked existing popular and recent methods, such as (Root)SIFT, ORB, HardNet, SuperPoint, R2D2, D2Net, etc, and published a paper called &quot;Image matching across wide baselines: From paper to practice&quot;. . The best results were obtained by a combinination of difference-of-Gaussians (DoG) local feature detector, commonly referred as SIFT detector, with deep learned patch descriptor such as HardNet, SOSNet, or AffNet-HardNet. . Let&#39;s check what was proposed by the challenge participants. . Top solutions-2020 and follow-ups . SuperGlue . SuperGlue is an attention-based graph neural network for matching local features, taking into account both geometry (keypoint location) and appearance (descriptor). Unlike previous works, e.g. CNe, or OANet below, it does not &quot;scoring-and-cleaning&quot; already established tentative correspondences. Instead, it establishes the correpondences given the local features from two images. SuperGlue won IMC-2020, as well as two other competitions at CVPR 2020. Its inference implementation is available here. . OANet . OANet is an specialized neural network architecture for &quot;scoring-and-cleaning&quot; already established tentative correspondences. OANet was run on top of DoG-HardNet local features. Pytorch version of OANet is available here . AdaLAM . AdaLAM is a handcrafted algorithm for tentative correspondence cleaning, which work comparably or even better than a learning-based approaches. It is based on two core assumptions: . Keypoints, which are near from each other are probably corresponding to the neaighboring keypoints in the other images | If keypoints are in correspondence, it means that their orientation and scale are also in correspondence. Check my post Local affine features: useful side product for an explanation | . The implementation is avilable here. As DoG+HardNet were used as a local features. . DISK . DISK is local feature, which has two main differences from the rest of competitors (SuperPoint, R2D2): . DISK is trained with a reinforcement-learning objective | DISK has UNet-like architecture, unlike VGG-style for the rest of the features. | Its implementation is available here. . HyNet . HyNet is an next stage in the L2Net-HardNet-SOSNet series of local patch descriptors. It is different from the previous works in two ways: . BatchNorm and ReLU in the HardNet architecture are replaced with FRN and TLU respecively. | During training, distance to the negative (non-matching) $d( theta)$ and positive (matching) $s( theta)$ samples are calculated in a different way for better learning, see image below. u and v denote descriptors is | . HyNet submission also used semantic segmentation network to remove the keypoints from the non-matchable areas, such as sky and water. . HardNet8 . HardNet8 is another improvement of the HardNet architecture: . Deeper and wider network | The output is compressed with a PCA. | The training set and hyperparameters are carefully selected. | It is available in kornia . 2021 challenge . This year challenge brings 2 new datasets: PragueParks and GoogleUrban. . The PragueParks dataset . The PragueParks dataset contains images from video sequences captured by the organizers with an iPhone 11, in 2021. The iPhone 11 has two cameras, with normal and wide lenses, both of which were used. Note that while the video is high quality, some of the frames suffer from motion blur. These videos were then processed by the commercial 3D reconstruction software RealityCapture, which is orders of magnitude faster than COLMAP, while delivering a comparable output in terms of accuracy. Similarly to we did for the &quot;PhotoTourism&quot; dataset, this data is then subsampled in order to generate the subsets used for evaluation. . The dataset contains small-scale scenes like tree, pond, wooden and metal sculptures with different level of zoom, lots of vegetation, and no people. The distribution of its camera poses differs from Phototourism. . The GoogleUrban dataset . The GoogleUrban dataset contains images used by Google to evaluate localization algorithms, such as those in Google&#39;s Visual Positioning System, which powers Live View on millions on mobile devices. They are obtained from videos collected from different cell phones, on many countries all over the world, often years apart. They contain poses, but not depth maps. Please note that due to legal reasons, this data is released with a restricted license, and must be deleted by the end of the challenge. . Submit your solution! . You can check out the tutorial on how to submit to IMC 2021 in the post Submitting to Image Matching Challenge 2021. Good luck! .",
            "url": "/wide-baseline-stereo-blog/2021/05/14/IMC2020-competition-recap.html",
            "relUrl": "/2021/05/14/IMC2020-competition-recap.html",
            "date": " • May 14, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Submitting to Image Matching Challenge 2021",
            "content": "What is Image Matching Challenge? . IMC is a benchmark and challenge for the local features (such as SIFT, SuperPoint, etc), matching methods (CNe, SuperGlue, etc.) and robust geometry estimators such as RANSAC, at CVPR 2021 Workshop on Image Matching. . I will walk you through the submission process, including writing setting-up an environment, writing a processing script and tuning matching and RANSAC for the best performance. As this is a tutorial, not a research paper, we use local feature descriptor available in kornia for this sample submission. . All the codes and scripts in this tutorial are also avilable at https://github.com/ducha-aiki/imc2021-sample-kornia-submission. Let&#39;s go! . Setting up the environment . First, let&#39;s clone the benchmark repository. . git clone https://github.com/ubc-vision/image-matching-benchmark cd image-matching-benchmark git submodule update --init --recursive . Now we need to create conda virtual environment. I assume that you have conda installed, and if not - please, follow instructions here . conda env create -f system/conda_env_dm.yml . After successfull environment creation, let&#39;s activate it . conda activate sfm . Downloading the data . IMC-2021 data consists of 3 datasets: Phototourism, PragueParks and GoogleUrban. You can download first two freely, but need to request a credentials and accept license agreement for the GoogleUrban dataset. . Latter is done by writing an email to image-matching@googlegroups.com and asking for the password. All datasets consist of two parts each: test part, which contains only images and validation part, which comes together with ground truth. Validation ground truth can (and should!) be used for hyperparameter tuning for your submission. . cd .. mkdir imc-2021-data cd imc-2021-data wget https://www.cs.ubc.ca/research/kmyi_data/imc2021-public/imc-2021-test-public-pragueparks.tar.gz wget https://www.cs.ubc.ca/research/kmyi_data/imc2021-public/imc-2021-validation-pragueparks.tar.gz wget https://www.cs.ubc.ca/research/kmyi_data/imc2021-public/imc-2021-test-public-phototourism.tar.gz wget https://www.cs.ubc.ca/research/kmyi_data/imc2021-public/imc-2021-validation-phototourism.tar.gz . I am assuming that you have requsted an access to the GoogleUrban dataset, downloaded it and put in the same directory, as the rest of the data: . (sfm) mishkdmy@n33:~/dev/imc-2021-data$ ls imc-2021-test-public-googleurban.tar.gz imc-2021-validation-googleurban.tar.gz imc-2021-test-public-phototourism.tar.gz imc-2021-validation-phototourism.tar.gz imc-2021-test-public-pragueparks.tar.gz imc-2021-validation-pragueparks.tar.gz . Now let&#39;s unpack it. . for f in *.tar.gz ; do tar -xzf $f; done . Now the directory should look like this: . (sfm) mishkdmy@n33:~/dev/imc-2021-data$ ls googleurban imc-2021-validation-phototourism.tar.gz imc-2021-test-public-googleurban.tar.gz imc-2021-validation-pragueparks.tar.gz imc-2021-test-public-phototourism.tar.gz phototourism imc-2021-test-public-pragueparks.tar.gz pragueparks imc-2021-validation-googleurban.tar.gz . Extracting the features . Let&#39;s start with creating the directory for our scripts (or you can clone it from here ) . cd .. mkdir imc2021-sample-kornia-submission cd imc2021-sample-kornia-submission . Now we will create a script, which extracts AffNet-HardNet8 descriptors on top of OpenCV SIFT keypoints. . We need to install pytorch and kornia for this: . pip install torch torchvision kornia pip install kornia_moons --no-deps . Great! Now we are ready to extract the features from the images. Required imports and initializations: . import matplotlib.pyplot as plt import numpy as np import cv2 import torch import kornia as K import kornia.feature as KF from kornia_moons.feature import * device = torch.device(&#39;cpu&#39;) try: if torch.cuda.is_available(): device = torch.device(&#39;cuda&#39;) print (&quot;GPU mode&quot;) except: print (&#39;CPU mode&#39;) #device = torch.device(&#39;cpu&#39;) # SIFT (DoG) Detector sift_det = cv2.SIFT_create(8000, contrastThreshold=-10000, edgeThreshold=-10000) # HardNet8 descriptor hardnet8 = KF.HardNet8(True).eval().to(device) # Affine shape estimator affnet = KF.LAFAffNetShapeEstimator(True).eval().to(device) . Now we can define extract_features function. Feel free to modify it for your own features. . def extract_features(img_fname, detector, affine, descriptor, device, visualize=False): img = cv2.cvtColor(cv2.imread(img_fname), cv2.COLOR_BGR2RGB) if visualize: plt.imshow(img) kpts = detector.detect(img, None)[:8000] # We will not train anything, so let&#39;s save time and memory by no_grad() with torch.no_grad(): timg = K.image_to_tensor(img, False).float()/255. timg = timg.to(device) timg_gray = K.rgb_to_grayscale(timg) # kornia expects keypoints in the local affine frame format. # Luckily, kornia_moons has a conversion function lafs = laf_from_opencv_SIFT_kpts(kpts, device=device) lafs_new = affine(lafs, timg_gray) if visualize: visualize_LAF(timg, lafs_new, 0) patches = KF.extract_patches_from_pyramid(timg_gray, lafs_new, 32) B, N, CH, H, W = patches.size() # Descriptor accepts standard tensor [B, CH, H, W], while patches are [B, N, CH, H, W] shape # So we need to reshape a bit :) descs = descriptor(patches.view(B * N, CH, H, W)).view(B * N, -1).detach().cpu().numpy() return kpts, descs . Let&#39;s check how it works on a single image. . img_fname = &#39;../imc-2021-data/pragueparks/wooden_lady/set_100/images/IMG_9603.MOV_frame000001.jpg&#39; kpts, descs = extract_features(img_fname, sift_det, affnet, hardnet8, device, True) . So far, so good. Now we need to convert our keypoints from OpenCV format to the benchmark format, which is numpy.array [N x dim] . def convert_kpts_to_imc(cv2_kpts): keypoints = np.array([(x.pt[0], x.pt[1]) for x in cv2_kpts ]).reshape(-1, 2) scales = np.array([12.0* x.size for x in cv2_kpts ]).reshape(-1, 1) angles = np.array([x.angle for x in cv2_kpts ]).reshape(-1, 1) responses = np.array([x.response for x in cv2_kpts]).reshape(-1, 1) return keypoints, scales, angles, responses . Now we are ready to write a script, which extracts local features for all images in the IMC-2021. The full script is accesible here . import os import h5py from tqdm import tqdm INPUT_DIR = &#39;../imc-2021-data&#39; OUT_DIR = &#39;extracted/cv2-dog-affnet-hardnet8&#39; os.makedirs(OUT_DIR, exist_ok=True) datasets = os.listdir(INPUT_DIR) for ds in datasets: ds_in_path = os.path.join(INPUT_DIR, ds) ds_out_path = os.path.join(OUT_DIR, ds) os.makedirs(ds_out_path, exist_ok=True) seqs = os.listdir(ds_in_path) for seq in seqs: if os.path.isdir(os.path.join(ds_in_path, seq, &#39;set_100&#39;)): seq_in_path = os.path.join(ds_in_path, seq, &#39;set_100&#39;, &#39;images&#39;) else: seq_in_path = os.path.join(ds_in_path, seq) seq_out_path = os.path.join(ds_out_path, seq) os.makedirs(seq_out_path, exist_ok=True) img_fnames = os.listdir(seq_in_path) num_kp = [] with h5py.File(f&#39;{seq_out_path}/keypoints.h5&#39;, &#39;w&#39;) as f_kp, h5py.File(f&#39;{seq_out_path}/descriptors.h5&#39;, &#39;w&#39;) as f_desc, h5py.File(f&#39;{seq_out_path}/scores.h5&#39;, &#39;w&#39;) as f_score, h5py.File(f&#39;{seq_out_path}/angles.h5&#39; &#39;w&#39;) as f_ang, h5py.File(f&#39;{seq_out_path}/scales.h5&#39;, &#39;w&#39;) as f_scale: for img_fname in tqdm(img_fnames): img_fname_full = os.path.join(seq_in_path, img_fname) key = os.path.splitext(os.path.basename(img_fname))[0] kpts, descs = extract_features(img_fname_full, sift_det, affnet, hardnet8, device, False) keypoints, scales, angles, responses = convert_kpts_to_imc(kpts) f_kp[key] = keypoints f_desc[key] = descs.reshape(-1, 128) f_score[key] = responses f_ang[key] = angles f_scale[key] = scales num_kp.append(len(keypoints)) print(f&#39;Finished processing &quot;{ds}/{seq}&quot; -&gt; {np.array(num_kp).mean()} features/image&#39;) . Creating config json file . In addition to features, we should submit a config file, which tells the benchmark, how the features should be matched and which RANSAC we prefer. In priciple, we can just write an arbitrary config file and submit already, but this may lead to the bad results. Let&#39;s instead generate a config file from python, so we can easily re-generate it. Why would we need this? Quite simple - to try different parameters on the validation set and only then create a final config. . First part of the config is metadata -- information about the method and authors. If your method is under review, you may want to set flag publish_anonymously to True. . metadata_dict = { &quot;publish_anonymously&quot;: False, &quot;authors&quot;: &quot;Dmytro Mishkin, Milan Pultar and kornia team&quot;, &quot;contact_email&quot;: &quot;ducha.aiki@gmail.com&quot;, &quot;method_name&quot;: &quot;CV-DoG-AffNet-HardNet8 (kornia)&quot;, &quot;method_description&quot;: r&quot;&quot;&quot;OpeCV SIFT keypoints 8000 features, followed by the AffNet normalization and HardNet8 descriptor as implemented in kornia. Matched using the built-in matcher (bidirectional filter with the &#39;both&#39; strategy, hopefully optimal inlier and ratio test thresholds) with DEGENSAC&quot;&quot;&quot;, &quot;link_to_website&quot;: &quot;https://github.com/kornia/kornia&quot;, &quot;link_to_pdf&quot;: &quot;https://arxiv.org/abs/2007.09699&quot; } . Second part is config_common: it tells the benchmark, which keypoints and descriptors you use. We will also need this names when importing our features during tuning on the validation set. . config_common_dict = {&quot;json_label&quot;: &quot;dog-affnet-hardnet8-degensac&quot;, &quot;keypoint&quot;: &quot;cv2dog&quot;, &quot;descriptor&quot;: &quot;affnethardnet8&quot;, &quot;num_keypoints&quot;: 8000} . Now comes the information how to match your local features. It may vary from dataset to dataset and also be different for the multiview and stereo mode. That is why we will create a template dictionary and change some parameters later. . Specifically, we have to specify, which distance our descriptor prefers: L2, L1 and Hamming are supported. . Then comes the tentative matches filtering. One can pass none for no filtering, snn_ratio_pairwise for Lowe&#39;s SNN ratio and fginn_ratio_pairwise for FGINN. If you are not familiar with filtering strategies, checkout this blogpost: &quot;How to match: to learn or not to learn?&quot;. The threshold is what have to be tunes. . We will use SNN, because of simplicity. Finally, we would like to make sure that tentative matches are cross-consistent, that is why we will enable symmetric matching. . Warning! We will use FLANN approximate nearest neighbor matching for speed-up tuning procedure, but it is better to turn it off for the final submission. . from copy import deepcopy matcher_template_dict = { &quot;method&quot;: &quot;nn&quot;, &quot;distance&quot;: &quot;L2&quot;, &quot;flann&quot;: True, &quot;num_nn&quot;: 1, &quot;filtering&quot;: { &quot;type&quot;: &quot;snn_ratio_pairwise&quot;, &quot;threshold&quot;: 0.90 }, &quot;symmetric&quot;: { &quot;enabled&quot;: True, &quot;reduce&quot;: &quot;both&quot;, } } . Finally, we have to specify robust geometry estimation method. We will pick the default choise from the previous challenge - DEGENSAC. threshold is what have to be tuned, the rest of parameters are already optimal, or fixed by the competition rules -- max_iter. . geom_template_dict = {&quot;method&quot;: &quot;cmp-degensac-f&quot;, &quot;threshold&quot;: 0.5, &quot;confidence&quot;: 0.999999, &quot;max_iter&quot;: 100000, &quot;error_type&quot;: &quot;sampson&quot;, &quot;degeneracy_check&quot;: True, } . Let&#39;s assemble and save our base config. . import json base_config = { &quot;metadata&quot;: metadata_dict, &quot;config_common&quot;: config_common_dict, &quot;config_phototourism_stereo&quot;: { &quot;use_custom_matches&quot;: False, &quot;matcher&quot;: deepcopy(matcher_template_dict), &quot;outlier_filter&quot;: { &quot;method&quot;: &quot;none&quot; }, &quot;geom&quot;: deepcopy(geom_template_dict) }, &quot;config_phototourism_multiview&quot;: { &quot;use_custom_matches&quot;: False, &quot;matcher&quot;: deepcopy(matcher_template_dict), &quot;outlier_filter&quot;: { &quot;method&quot;: &quot;none&quot; }, &quot;colmap&quot;: {}}, &quot;config_pragueparks_stereo&quot;: { &quot;use_custom_matches&quot;: False, &quot;matcher&quot;: deepcopy(matcher_template_dict), &quot;outlier_filter&quot;: { &quot;method&quot;: &quot;none&quot; }, &quot;geom&quot;: deepcopy(geom_template_dict) }, &quot;config_pragueparks_multiview&quot;: { &quot;use_custom_matches&quot;: False, &quot;matcher&quot;: deepcopy(matcher_template_dict), &quot;outlier_filter&quot;: { &quot;method&quot;: &quot;none&quot; }, &quot;colmap&quot;: {}}, &quot;config_googleurban_stereo&quot;: { &quot;use_custom_matches&quot;: False, &quot;matcher&quot;: deepcopy(matcher_template_dict), &quot;outlier_filter&quot;: { &quot;method&quot;: &quot;none&quot; }, &quot;geom&quot;: deepcopy(geom_template_dict) }, &quot;config_googleurban_multiview&quot;: { &quot;use_custom_matches&quot;: False, &quot;matcher&quot;: deepcopy(matcher_template_dict), &quot;outlier_filter&quot;: { &quot;method&quot;: &quot;none&quot; }, &quot;colmap&quot;: {}} } . Finally, benchmark expects multiple configs, so we have to create a list, and then we can save our config . import json with open(&#39;base_config.json&#39;, &#39;w&#39;) as f: json.dump([base_config], f, indent=2) . Preliminary evaluation . Now let&#39;s check how our features perform on validation set. We have to import our feature to the benchmark and run the benchmark. . I will cheat a little bit here and skip the multiview evaluation. The reason is that it requires colmap, which might be not easy to install. . Importing features . Here we have to provide the same keypoint and descriptor names, as we wrote in json config. The rest of arguments are straightforward: path to features, json, etc. . cd ../image-matching-benchmark/ python -utt import_features.py --kp_name cv2dog --desc_name affnethardnet8 --num_keypoints 8000 --path_features ../imc2021-sample-kornia-submission/extracted/cv2-dog-affnet-hardnet8 --path_results ../benchmark-results --subset both --is_challenge false --path_json ../imc2021-sample-kornia-submission/base_config.json --datasets phototourism googleurban pragueparks . Running the evaluation . Now we are ready to run the evaluation . python -utt run.py --run_mode=interactive --json_method=../imc2021-sample-kornia-submission/base_config.json --subset=val --eval_multiview=False --path_data ../imc-2021-data/ --path_results ../benchmark-results --is_challenge false . After a while (an 30 min for 32 cores machine), the process will finish and you will see the following log message: . -- Saving to: &quot;packed-val/dog-affnet-hardnet8-degensac.json&quot; . Reading results . Json file with evaluation results is saved to image-matching-benchmark/packed-val/dog-affnet-hardnet8-degensac.json, and some visualizations -- to ../benchmark-visualization/png. . First, we come back to our imc2021-sample-kornia-submission directory: . cd ../imc2021-sample-kornia-submission . Metric, which are used for the competition is mean average accuracy (mAA) at visibility threshold 0.1 . import os hashname=&#39;dog-affnet-hardnet8-degensac&#39; res_fname = os.path.join(&#39;../image-matching-benchmark/packed-val&#39;, f&#39;{hashname}.json&#39;) with open(res_fname, &#39;r&#39;) as f: results = json.load(f) submission_name = results[&#39;config&#39;][&#39;metadata&#39;][&#39;method_name&#39;] datasets = [&#39;phototourism&#39;, &#39;pragueparks&#39;, &#39;googleurban&#39;] tasks = [&#39;stereo&#39;] # [&#39;stereo&#39;, &#39;multiview&#39;] #Remember, that we skip colmap evaluations metric = &#39;qt_auc_10_th_0.1&#39; for dset in datasets: for task in tasks: mAA = results[dset][&#39;results&#39;][&#39;allseq&#39;][task][&#39;run_avg&#39;][metric][&#39;mean&#39;] print (f&#39;{submission_name} {task} mAA for {dset} is {mAA:.4f}&#39;) . CV-DoG-AffNet-HardNet8 (kornia) stereo mAA for phototourism is 0.7108 CV-DoG-AffNet-HardNet8 (kornia) stereo mAA for pragueparks is 0.5850 CV-DoG-AffNet-HardNet8 (kornia) stereo mAA for googleurban is 0.3099 . We can also see results sequence-by-sequence . import seaborn as sns sns.set_context(&#39;paper&#39;, font_scale=1.7) seqs = [] mAAs = [] for dset in datasets: for task in tasks: for seq in results[dset][&#39;results&#39;].keys(): if seq == &#39;allseq&#39;: continue mAA = results[dset][&#39;results&#39;][seq][task][&#39;run_avg&#39;][metric][&#39;mean&#39;] mAAs.append(mAA) seqs.append(seq) fig, ax = plt.subplots(figsize=(15,5)) xticks = 2*np.arange(len(seqs)) ax.set_xticks(xticks) ax.bar(xticks, mAAs) ax.set_xticklabels(seqs) ax.set_ylabel(&#39;mAA&#39;) ax.set_xlabel(&#39;Sequence&#39;) . Text(0.5, 0, &#39;Sequence&#39;) . How do our feature correspondences look like? . import cv2 def plot_image_grid(images, ncols=None, cmap=&#39;gray&#39;): #Taken from https://stackoverflow.com/a/66961099/1983544 &#39;&#39;&#39;Plot a grid of images&#39;&#39;&#39; if not ncols: factors = [i for i in range(1, len(images)+1) if len(images) % i == 0] ncols = factors[len(factors) // 2] if len(factors) else len(images) // 4 + 1 nrows = int(len(images) / ncols) + int(len(images) % ncols) imgs = [images[i] if len(images) &gt; i else None for i in range(nrows * ncols)] f, axes = plt.subplots(nrows, ncols, figsize=(3*ncols, 2*nrows)) axes = axes.flatten()[:len(imgs)] for img, ax in zip(imgs, axes.flatten()): if np.any(img): if len(img.shape) &gt; 2 and img.shape[2] == 1: img = img.squeeze() ax.imshow(img, cmap=cmap) imgs = [] VIS_DIR = f&#39;../benchmark-visualization/png/{hashname}&#39; for dset in os.listdir(VIS_DIR): dset_dir = os.path.join(VIS_DIR, dset) for seq in os.listdir(dset_dir): seq_dir = os.path.join(dset_dir, seq, &#39;stereo&#39;) for img_fname in os.listdir(seq_dir): full_fname = os.path.join(seq_dir, img_fname) img = cv2.resize(cv2.cvtColor(cv2.imread(full_fname), cv2.COLOR_BGR2RGB), (200,100)) imgs.append(img) plot_image_grid(imgs) . Hyperparameters tuning . Let&#39;s now tune our hyperparameters, specifically, matching threshold and RANSAC inlier ratio. First we tune RANSAC and then matching ratio. We need to generate jsons for each configuration. . inl_ths = [0.25, 0.5, 0.75, 1.0, 1.5, 2.0, 2.5, 3.0] configs = [] for inl_th in inl_ths: current_config = deepcopy(base_config) for dset in [&#39;phototourism&#39;, &#39;pragueparks&#39;, &#39;googleurban&#39;]: current_config[f&#39;config_{dset}_stereo&#39;][&#39;geom&#39;][&#39;threshold&#39;] = inl_th label = current_config[&#39;config_common&#39;][&#39;json_label&#39;] current_config[&#39;config_common&#39;][&#39;json_label&#39;] = f&#39;{label}-inlth-{inl_th}&#39; configs.append(current_config) with open(&#39;ransac_tuning.json&#39;, &#39;w&#39;) as f: json.dump(configs, f, indent=2) . cd ../image-matching-benchmark/ python -utt run.py --run_mode=interactive --json_method=../imc2021-sample-kornia-submission/base_config.json --subset=val --eval_multiview=False --path_data ../imc-2021-data/ --path_results ../benchmark-results --is_challenge false cd ../imc2021-sample-kornia-submission . Now we will write a function, which reads results and picks the best threshold per dataset . hashname=&#39;dog-affnet-hardnet8-degensac&#39; res_dict = {} datasets = [&#39;phototourism&#39;, &#39;pragueparks&#39;, &#39;googleurban&#39;] for dset in datasets: res_dict[dset] = {} task = &#39;stereo&#39; metric = &#39;qt_auc_10_th_0.1&#39; for inl_th in inl_ths: res_fname = os.path.join(&#39;../image-matching-benchmark/packed-val&#39;, f&#39;{hashname}-inlth-{inl_th}.json&#39;) try: with open(res_fname, &#39;r&#39;) as f: results = json.load(f) except: continue submission_name = results[&#39;config&#39;][&#39;metadata&#39;][&#39;method_name&#39;] res_dict[inl_th] = {} for dset in datasets: mAA = results[dset][&#39;results&#39;][&#39;allseq&#39;][task][&#39;run_avg&#39;][metric][&#39;mean&#39;] res_dict[dset][inl_th] = mAA fig, ax = plt.subplots(figsize=(5,5)) colors = [&#39;r&#39;,&#39;b&#39;,&#39;k&#39;] final_ths = {} for i, dset in enumerate(datasets): inl_ths = [] mAAs = [] for inl_th, mAA in res_dict[dset].items(): inl_ths.append(inl_th) mAAs.append(mAA) best_th_idx = np.argmax(np.array(mAAs)) best_th = inl_ths[best_th_idx] best_mAA = mAAs[best_th_idx] print (f&#39;Best {dset} mAA = {best_mAA:.4f} with inl_th = {best_th}&#39;) ax.plot(inl_ths, mAAs, label=dset, color=colors[i]) ax.plot(best_th, best_mAA, label = f&#39;{dset}-best&#39;, marker=&#39;x&#39;, linestyle=&#39;&#39;, color=colors[i]) final_ths[dset] = best_th ax.legend() ax.set_ylabel(&#39;mAA&#39;) ax.set_xlabel(&#39;DEGENSAC inlier threshold&#39;) . Best phototourism mAA = 0.7108 with inl_th = 0.5 Best pragueparks mAA = 0.6700 with inl_th = 1.5 Best googleurban mAA = 0.3116 with inl_th = 0.75 . Text(0.5, 0, &#39;DEGENSAC inlier threshold&#39;) . Creating final submission . Its time to create our final submission! . configs = [] current_config = deepcopy(base_config) for dset in [&#39;phototourism&#39;, &#39;pragueparks&#39;, &#39;googleurban&#39;]: current_config[f&#39;config_{dset}_stereo&#39;][&#39;geom&#39;][&#39;threshold&#39;] = final_ths[dset] # I did a little bit of tuning offline for multiview, so we will put it here current_config[f&#39;config_{dset}_multiview&#39;][&#39;matcher&#39;][&#39;filtering&#39;][&#39;threshold&#39;] = 0.95 #Remember, that we should not forget to turn FLANN ofd current_config[f&#39;config_{dset}_multiview&#39;][&#39;matcher&#39;][&#39;flann&#39;] = False current_config[f&#39;config_{dset}_stereo&#39;][&#39;matcher&#39;][&#39;flann&#39;] = False current_config[&#39;metadata&#39;][&#39;method_name&#39;] = &#39;KORNIA TUTORIAL CV-DoG-AffNet-HardNet8&#39; label = current_config[&#39;config_common&#39;][&#39;json_label&#39;] current_config[&#39;config_common&#39;][&#39;json_label&#39;] = f&#39;{label}&#39; configs.append(current_config) print (current_config) with open(&#39;final_submission.json&#39;, &#39;w&#39;) as f: json.dump(configs, f, indent=2) . {&#39;metadata&#39;: {&#39;publish_anonymously&#39;: False, &#39;authors&#39;: &#39;Dmytro Mishkin, Milan Pultar and kornia team&#39;, &#39;contact_email&#39;: &#39;ducha.aiki@gmail.com&#39;, &#39;method_name&#39;: &#39;KORNIA TUTORIAL CV-DoG-AffNet-HardNet8&#39;, &#39;method_description&#39;: &#34;OpeCV SIFT keypoints 8000 features, followed by the AffNet normalization n and HardNet8 descriptor as implemented in kornia. n Matched using the built-in matcher (bidirectional filter with the &#39;both&#39; strategy, n hopefully optimal inlier and ratio test thresholds) with DEGENSAC&#34;, &#39;link_to_website&#39;: &#39;https://github.com/kornia/kornia&#39;, &#39;link_to_pdf&#39;: &#39;https://arxiv.org/abs/2007.09699&#39;}, &#39;config_common&#39;: {&#39;json_label&#39;: &#39;dog-affnet-hardnet8-degensac&#39;, &#39;keypoint&#39;: &#39;cv2dog&#39;, &#39;descriptor&#39;: &#39;affnethardnet8&#39;, &#39;num_keypoints&#39;: 8000}, &#39;config_phototourism_stereo&#39;: {&#39;use_custom_matches&#39;: False, &#39;matcher&#39;: {&#39;method&#39;: &#39;nn&#39;, &#39;distance&#39;: &#39;L2&#39;, &#39;flann&#39;: False, &#39;num_nn&#39;: 1, &#39;filtering&#39;: {&#39;type&#39;: &#39;snn_ratio_pairwise&#39;, &#39;threshold&#39;: 0.9}, &#39;symmetric&#39;: {&#39;enabled&#39;: True, &#39;reduce&#39;: &#39;both&#39;}}, &#39;outlier_filter&#39;: {&#39;method&#39;: &#39;none&#39;}, &#39;geom&#39;: {&#39;method&#39;: &#39;cmp-degensac-f&#39;, &#39;threshold&#39;: 0.5, &#39;confidence&#39;: 0.999999, &#39;max_iter&#39;: 100000, &#39;error_type&#39;: &#39;sampson&#39;, &#39;degeneracy_check&#39;: True}}, &#39;config_phototourism_multiview&#39;: {&#39;use_custom_matches&#39;: False, &#39;matcher&#39;: {&#39;method&#39;: &#39;nn&#39;, &#39;distance&#39;: &#39;L2&#39;, &#39;flann&#39;: False, &#39;num_nn&#39;: 1, &#39;filtering&#39;: {&#39;type&#39;: &#39;snn_ratio_pairwise&#39;, &#39;threshold&#39;: 0.95}, &#39;symmetric&#39;: {&#39;enabled&#39;: True, &#39;reduce&#39;: &#39;both&#39;}}, &#39;outlier_filter&#39;: {&#39;method&#39;: &#39;none&#39;}, &#39;colmap&#39;: {}}, &#39;config_pragueparks_stereo&#39;: {&#39;use_custom_matches&#39;: False, &#39;matcher&#39;: {&#39;method&#39;: &#39;nn&#39;, &#39;distance&#39;: &#39;L2&#39;, &#39;flann&#39;: False, &#39;num_nn&#39;: 1, &#39;filtering&#39;: {&#39;type&#39;: &#39;snn_ratio_pairwise&#39;, &#39;threshold&#39;: 0.9}, &#39;symmetric&#39;: {&#39;enabled&#39;: True, &#39;reduce&#39;: &#39;both&#39;}}, &#39;outlier_filter&#39;: {&#39;method&#39;: &#39;none&#39;}, &#39;geom&#39;: {&#39;method&#39;: &#39;cmp-degensac-f&#39;, &#39;threshold&#39;: 1.5, &#39;confidence&#39;: 0.999999, &#39;max_iter&#39;: 100000, &#39;error_type&#39;: &#39;sampson&#39;, &#39;degeneracy_check&#39;: True}}, &#39;config_pragueparks_multiview&#39;: {&#39;use_custom_matches&#39;: False, &#39;matcher&#39;: {&#39;method&#39;: &#39;nn&#39;, &#39;distance&#39;: &#39;L2&#39;, &#39;flann&#39;: False, &#39;num_nn&#39;: 1, &#39;filtering&#39;: {&#39;type&#39;: &#39;snn_ratio_pairwise&#39;, &#39;threshold&#39;: 0.95}, &#39;symmetric&#39;: {&#39;enabled&#39;: True, &#39;reduce&#39;: &#39;both&#39;}}, &#39;outlier_filter&#39;: {&#39;method&#39;: &#39;none&#39;}, &#39;colmap&#39;: {}}, &#39;config_googleurban_stereo&#39;: {&#39;use_custom_matches&#39;: False, &#39;matcher&#39;: {&#39;method&#39;: &#39;nn&#39;, &#39;distance&#39;: &#39;L2&#39;, &#39;flann&#39;: False, &#39;num_nn&#39;: 1, &#39;filtering&#39;: {&#39;type&#39;: &#39;snn_ratio_pairwise&#39;, &#39;threshold&#39;: 0.9}, &#39;symmetric&#39;: {&#39;enabled&#39;: True, &#39;reduce&#39;: &#39;both&#39;}}, &#39;outlier_filter&#39;: {&#39;method&#39;: &#39;none&#39;}, &#39;geom&#39;: {&#39;method&#39;: &#39;cmp-degensac-f&#39;, &#39;threshold&#39;: 0.75, &#39;confidence&#39;: 0.999999, &#39;max_iter&#39;: 100000, &#39;error_type&#39;: &#39;sampson&#39;, &#39;degeneracy_check&#39;: True}}, &#39;config_googleurban_multiview&#39;: {&#39;use_custom_matches&#39;: False, &#39;matcher&#39;: {&#39;method&#39;: &#39;nn&#39;, &#39;distance&#39;: &#39;L2&#39;, &#39;flann&#39;: False, &#39;num_nn&#39;: 1, &#39;filtering&#39;: {&#39;type&#39;: &#39;snn_ratio_pairwise&#39;, &#39;threshold&#39;: 0.95}, &#39;symmetric&#39;: {&#39;enabled&#39;: True, &#39;reduce&#39;: &#39;both&#39;}}, &#39;outlier_filter&#39;: {&#39;method&#39;: &#39;none&#39;}, &#39;colmap&#39;: {}}} . Submission Zip file should have folder structure as follow: ├── config.json ├── [Dataset 1] │ ├── [Sequence 1] │ │ ├── keypoints.h5 │ │ ├── descriptors.h5 │ │ ├── matches.h5 │ ├── [Sequence 2] │ │ ├── ... ├── [Dataset 2] │ ├── ... . So we have to just copy our features, add config and zip them. . cp final_submission.json extracted/cv2-dog-affnet-hardnet8/config.json cd extracted/cv2-dog-affnet-hardnet8 zip -r submission.zip * . Last step before the submission - check the submission for correctness with provided script . cd ../../../image-matching-benchmark python -utt submission_validator.py --submit_file_path ../imc2021-sample-kornia-submission/extracted/cv2-dog-affnet-hardnet8/submission.zip --benchmark_repo_path . --raw_data_path ../imc-2021-data/ --datasets googleurban phototourism pragueparks . If everything is correct, you will see: . Validating method 1/1: &quot;dog-affnet-hardnet8-degensac&quot; [&#39;googleurban&#39;, &#39;phototourism&#39;, &#39;pragueparks&#39;] Running: googleurban, stereo track Running: googleurban, multiview track Running: phototourism, stereo track Running: phototourism, multiview track Running: pragueparks, stereo track Running: pragueparks, multiview track Validating key &quot;config_googleurban_stereo&quot; Validating key &quot;config_googleurban_multiview&quot; Validating key &quot;config_phototourism_stereo&quot; Validating key &quot;config_phototourism_multiview&quot; Validating key &quot;config_pragueparks_stereo&quot; Validating key &quot;config_pragueparks_multiview&quot; . And file submission_log.txt will appear near our .zip file. . cat ../imc2021-sample-kornia-submission/extracted/cv2-dog-affnet-hardnet8/submission_log.txt . Submission is in proper format, please submit to IMW 2021 website. . That&#39;s all, folks! We can submit! But, please, do not just submit this sample submission - make your own :) .",
            "url": "/wide-baseline-stereo-blog/2021/05/12/submitting-to-IMC2021-step-by-step.html",
            "relUrl": "/2021/05/12/submitting-to-IMC2021-step-by-step.html",
            "date": " • May 12, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "WxBS step by step",
            "content": "Delving deeper into WxBS algorithm steps . The wide baseline stereo problem is commonly addressed by a family of algorithms, the general structure of which is shown in Figure above. We will be referring to it as the WxBS pipeline or the WxBS algorithm. Let us describe it in more detail and discuss the reasoning behind each block. . A set of the local features (Also known as keypoints, local regions, distinguished regions, salient regions, salient points, etc.) is detected in each image independently. In automated systems the local features are usually low level structures like corners, blobs and so on. However, they can also be more high level semantic structures, as we used in the example in intro: &quot;a long staircase on the left side&quot;, &quot;the top of the lampost&quot; and so on. An important detail is that detection is typically done in each image separately. Why is it the case? If the task is to match only a single image pair, that would be an unnecessary restriction. It is even benefitial to process the images jointly, as a human would do, by placing both images side-by-side and looking at them back and forth. However, the wide baseline stereo task rarely arises by itself, more often it is only a part of a bigger system, e.g. visual localization or 3D recontruction from the collection of images. Therefore, one needs to match an image to not the one, but multiple other images. That is why it is benefitial to perform feature extraction only once per image and then load the stored results. Moreover, independent feature extraction is a task which is easy to parallelize and that is typically done in most of libraries and frameworks for the WxBS. One could be wondering if the local feature detection process is necessary at all? Indeed, it is possible to avoid feature detection and consider all the pixels as &quot;detections&quot;. The problem with such approach is the high computational and memory complexity -- even a small 800x600 image contains half a million pixels, which need to be matched to half a million pixels in another image. | A region around the local feature to be described is selected. If one considers a keypoint to be literally a point, it is impractical to distinguish between them based only on coordinates and, maybe, the single RGB value of the pixel. On the other extreme, part of the image, which really far from the current keypoint helps little to nothing in terms of finding a correspondence. Thus, a reasonable trade-off needs to be made. Keypoint therefore can be think of as the &quot;reference point of the distinguished region&quot;, e.g. a center of the blob. It worth mention that some detectors return a region by default, so this step is omitted, or, to be precise, included into step 1 &quot;local features detection&quot;. However, it is useful to have it discussed separately . | A patch around each local feature is described with a local feature descriptor, i.e. converted to a compact format. Such procedure also should be robust to changes in acquisition conditions so that descriptors related to the same 3D points are similar and dissimilar otherwise. The local feature descriptors are then used for the efficient generation of tentative correspondences. Could one skip this stage? Yes, but as with the local feature detection, the skipping is not desirable from a computational point of view -- the benefits are discussed in the next stage -- matching. Local feature detection, measurement region selection and description together convert an image into a sparse representation, which is suitable for the correspondence search. Such representation is more robust to the acquisition conditions and can be further indexed if used for image retrieval. | Tentative correspondences between detected features are established and then filtered. The simplest and common way to generate tentative correspondences is to perform a nearest neighbor search in the descriptor space. The commonly used descriptors are the binary or float point vectors, which allows to employ various algorithms for approximate nearest neighbor search and trade a small amount of accuracy for orders of magnitude speed-up. Such correspondences need to be filtered, that is why the are called &quot;tentative&quot; or &quot;putative&quot; -- a significant percantage of them is incorrect. There are many reasons for that -- imprerfection of the local feature detector, descriptor, matching process and simply the fact that some parts of the scene are visible only on one image, but not another. | The geometric relationship between the two images is recovered, which is the final goal of the whole process. In addition, the tentative correspondences, which are not consistent with the found geometry, are called outliers and are discarded. The most common way of robust model estimation in the presense of outliers is called RANSAC -- random sample consensus. There are other methods as well, e.g. re-weighted least squares, but RANSAC predominantely used in practice. |",
            "url": "/wide-baseline-stereo-blog/2021/02/11/WxBS-step-by-step.html",
            "relUrl": "/2021/02/11/WxBS-step-by-step.html",
            "date": " • Feb 11, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Wide multiple baseline stereo in simple terms",
            "content": "What is wide multiple baseline stereo? . Imagine you have a nice photo you took in autumn and would like to take one in summer, from the same spot. How would you achieve that? You go to the place and you start to compare what you see on the camera screen and on the printed photo. Specifically, you would probably try to locate the same objects, e.g., &quot;that high lamppost&quot; or &quot;this wall clock&quot;. Then one would estimate how differently they are arranged on the old photo and camera screen. For example, by checking whether the lamppost is occluding the clock on the tower or not. That would give an idea of how you should move your camera. . Now, what if you are not allowed to take that photo with you, because it is a museum photo and taking pictures is prohibited there. Instead you can create a description of it. In that case, it is likely that you would try to make a list of features and objects in the photo together with the descriptions, which are sufficient to distinguish the objects. For example, &quot;a long staircase on the left side&quot;, &quot;The nice building with a dark roof and two towers&quot; or &quot;the top of the lampost&quot;. It would be useful to also describe where these objects and features are pictured in the photo, &quot;The lamp posts are on the left, the closest to the viewer is in front of the left tower with a clock. The clock tower is not the part of the building and stands on its own&quot;. Then when arriving, you would try to find those objects, match them to the description you have, and try to estimate where you should go. You repeat the procedure until the camera screen shows a picture which is fitting the description you have and the image you have in your memory. . Congratulations! You just have successfully registered the two images, which have a significant difference in viewpoint, appearance, and illumination. In the process of doing so, you were solving multiple times the wide multiple-baseline stereo problem (WxBS) -- estimating the relative camera pose from a pair of images, different in many aspects, yet depicting the same scene. . Let us write down the steps, which we took. . Identify salient objects and features in the images -- &quot;trees&quot;, &quot;statues&quot;, “tip of the tower”, etc in images. . | Describe the objects and features, taking into account their neighborhood: “statue with a blue left ear&quot;. . | Establish potential correspondences between features in the different images, based on their descriptors. . | Estimate, which direction one should move the camera to align the objects and features. . | That is it! detailed explanation of the each of the steps is in this post. If you are interested in the formal definition, check here, and the history of the WxBS is here. .",
            "url": "/wide-baseline-stereo-blog/2021/01/09/wxbs-in-simple-terms.html",
            "relUrl": "/2021/01/09/wxbs-in-simple-terms.html",
            "date": " • Jan 9, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Lessons learned and future directions",
            "content": "I would like to share some lessons I have learned about wide baseline stereo and propose some research directions, which are worth exploring in the short and longer term. . Lessons learned . 1. Benchmark = metrics + implementation + dataset . In our paper &quot;Image Matching across Wide Baselines: From Paper to Practice&quot; we focused on the first two parts. Specifically, metrics -- if they are not &quot;downstream&quot; metrics, the improvements in the single component might not translate to the overall system improvements. And implementation -- implementing the simplest possible setup is, of course, a valuable tool, but one have to also incorporate the best known practices, e.g. matching, RANSAC tuning as so on. . I hope, that we have delivered that message to the community. But the last component -- the dataset -- we have, perhaps, overlooked a bit ourself. The problem with dataset limitations, e.g. lack of illumination or seasonal changes is not that one does not properly address. Usually benchmark papers are adressing their limitations quite clearly. The problem is that researchers (including myself) have tendency to work on improving results, which are easily measurable, therefore the implicitly designing the methods, which solve only some specific problem, encoded in the form of the dataset. . 2. Trying to work for the &quot;most general&quot; case might be detrimental for the practical applications. . This is kind of opposite side of the lesson 1. For example, the classical SIFT matching is rotation-invariant, because that was assumed to be the requirement to work in the &quot;real world&quot;. However, the practice of image retrieval and then most of learned local features like R2D2, SuperPoint, DELF and so on, showed that &quot;up-is-up&quot; is a reasonable assumption to built on. The rotational invariance in lots of scenarios hurt more than helps. . 3. When borrow idea from classical paper, adapt it . HardNet borrows the idea of the using second nearest neighbor(SNN) descriptor from the SIFT. However, using SNN ratio for descriptor learning leads to inferior results. We had to modify it to the triplet margin loss. . 4. Classic handcrafted algorithms are not dead and can be improved a lot . Modern versions of RANSAC are still necessary for two view matching even when so complex methods as SuperGlue as used. Moreover, they still have quite a lot things for improvement, as proved by my colleague, Daniel Barath . They also have a benefit, that once you have an idea, you can make a paper out it faster, as you don&#39;t need gathering data and training the model. . Future research directions . 1. Application-specific local features . One thing about the current local features is that they are kind of universal. SIFT works reasonably well for lots of applications: 3d reconstruction, SLAM, image retrieval, etc. The learned features, like SuperPoint or R2D2, although are biased towards the data they are trained on, still don&#39;t have anything domain specific. . Let me explain. There are different qualities about the local feature (detectors). It can be more or less robust to nuisance factors like the illumination and camera position. It can be more or less precisely localized. It can be more or less dense and/or evenly distributed over the image. . For example, in image retrieval, one does not really care about precise localization, the robustness is much more important. For the 3d reconstruction one would like to have a lot of 3d points to get the reasonable reconstruction. On the other hand, for the SLAM/relocalization application, sparse features would be more advantageous because of smaller memory footprint and computational cost. . There are, actually, some steps in that direction. Let me name a few. . HOW local features designed for the image retrieval . | SIPs: Succinct Interest Points from Unsupervised Inlierness Probability Learning sparse local features for the SLAM. . | Reinforced Feature Points: Optimizing Feature Detection and Description for a High-Level Task . | I believe, that it is only beginning and we are yet to experience AlphaZero moment for the local features. . 2. Rethinking overall wide baseline stereo pipeline, optimized for the specific application . It is often perceived, that image matching across the unordered collection of the images is a task with quadratic complexity w.r.t. number of images. Some operations can be done separately, e.g. feature detection, but others, like feature matching and RANSAC cannot. Right? . Not necessarily. It turns out, that one can avoid running feature matching and RANSAC for more than 90% of image pairs with clever preprocessing, ordering and re-using results from the previous matching. Moreover, in order to do the whole task faster (matching image collections), one may need to introduce additional steps, which are not necessary, or slowing things down for the two images case. . That&#39;s what we done for the intial pose estimation for the global SfM in our paper, which reduced the matching runtime from 200 hours to 29. . Another example would be &quot;SuperGlue: Learning Feature Matching with Graph Neural Networks&quot;, where authors abandoned traditional descriptor matching and instead leveraged all the information for keypoint and descriptors from both images altogether. . 3. Rethinking and improving the process of training data generation for the WBS . So far, all the local feature papers I have seen rely on one of the ground truth source. . SfM data, obtained with COLMAP with, possibly, a cleaned depth information. | Affine and color augmentation. | Synthetic images (e.g. corners for SuperPoint). | There are several problems with them. . SfM data assumes that the data is matchable by the existing methods, at least for a some extent. That might not always be true for cross-seasonal, medical and other kind of data. It is also not applicable for historical photographies and other types of data. Moreover, SfM data takes quite a time for compute and space to store. I believe that we may do better. . Affine and color augmentation can take us only this far -- we actually want our detectors and descriptors to be robust to the changes, which we don&#39;t know how to simulate/augment. . Synthetic images as they are in, say, CARLA simulator lack fine details and photorealism. However, I am optimistic about using neural renderers and learned wide baseline stereo is a GAN-like self-improving loop. . 4. Matching with On-Demand View Synthesis revisited . I like the &quot;on-demand&quot; principle a lot and I think we can explore it much more that we are now. So far we have either affine view synthesis (ASIFT, MODS), or GAN-based stylizations for the day-night matching. . That is why I am glad to see papers like Single-Image Depth Prediction Makes Feature Matching Easier, which generate normalized views based on depth in order to help the matching. . Why not go further? Combine viewpoint, illumination, season, sensor synthesis? . 5. Moar inputs! . I have mentioned above that monocular depth may help the feature matching or camera pose estimation. However, why stop here? . Let&#39;s use other networks as well, especially given that we will need them on robot or vehicle anyway. Semantic segmentation? Yes, please. Surface normals? Why not? Intrinsic images? Йой, най буде! 1 . 1. Ukrainian, means let is be↩ . . What do you think would be good idea for the WBS research? . Let me know in comments/twitter. I am also going to update this page from time to time .",
            "url": "/wide-baseline-stereo-blog/2020/11/26/lessons-and-future-directions.html",
            "relUrl": "/2020/11/26/lessons-and-future-directions.html",
            "date": " • Nov 26, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Benchmarking Image Retrieval for Visual Localization",
            "content": "I would like to share my thoughts on 3DV 2020 paper &quot;Benchmarking Image Retrieval for Visual Localization&quot; by Pion et.al. . . What is the paper about? . How one would approach visual localization? The most viable way to do it is hierarchical approach, similar to image retrieval with spatial verification. . You get the query image, retrieve the most similar images to it from some database by some efficient method, e.g. global descriptor search. Then given the top-k images you estimate the pose of the query image by doing two-view matching, or some other method. . The question is -- how much influence the quality of the image retrieval has? Should you spend more or less time improving it? That is the questions, paper trying to answer. . Authors design 3 re-localization systems. . &quot;Task 1&quot; system estimates the query image pose as average of the short-list images pose, weighted by the similarity to the query image. . | &quot;Task 2a&quot; system performs pairwise two-view matching between short-list images and query triangulates the query image pose using 3d map built from the successully matches images. . | &quot;Task 2b&quot; pre-builds the 3D map from the database images offline. At the inference time, local feature 2d-3d matching is done on the shortlist images. . | What is benchmarked? . The paper compares . SIFT-based DenseVLAD | . and CNN-based . NetVLAD | APGeM | DELG | . What is important (and adequately mentioned in the paper, although I would prefer the disclamer in the each figure) is that all CNN-based methods have different architectures AND training data. Basically, the paper uses author-released models. Thus one cannot say if APGeM is better or worse than NetVLAD as method, because they were trained on the very different data. However, I also understand that one cannot easily afford to re-implement and re-train everything. . As the sanity check paper provides the results on the Revisited Oxford and Paris image retrieval benchmark. . Summary of results . Paper contains a lot of information and I definitely recommend you to read it. Nevertheless, let me try to summarize paper messages and then my take on it. . For the task1 (similarity-weighted pose) there is no clear winner. (SIFT)-DenseVLAD works the best for the daytime datasets. Probably DenseVLAD is good because it is not invariant and if it can match images, they are really close -&gt; high pose accuracy. For the night both DeLG and AP-GeM are good. As paper guesses, that it because they are only ones, which were trained on night images as well. | There is almost no difference between CNN-based methods for the task2a and task2b (retrieval -&gt; local features matching). This indicates that the limit is the mostly in the number of images and local features. | . . My take-away messages . Image Relocalization seems to be is more real-world and engineering task, than image retrieval. . And that it why it actually ALREADY WORKS, because if there some weak spot, it is compensated by the system design. Thhe same conclusion from our IMC paper, experiment with ground truth -- if you have 1k images for the 3d model, you can use as bad features, as you want. The COLMAP will recover anyway . . The retrieval, on the other hand is more interesting to work on, because it is kind of deliberately hard and you can do some fancy stuff, which do not matter in the real world. . Task1 (global descriptor-only) system are quite useless now . Unless we are speaking about the quite dense image representation. I mean, top-accuracy is 35% vs almost 100% for those, which include local features. . Good news: it has a LOT of space for the improvement to work on. . For the task 2a and 2b, robust global descriptors are a way to do the retrieval, sorry VLAD. . The precision will come from the local features. Which I like a lot, because VLAD is more complex to train and initalize, I never liked it (nothing personal). . For the task2a and 2b we need new metrics, e.g. precisition @ X Mb memory footprint . Because otherwise, the task is easily solved by the brute force -- either by photo taking, or, at least with image syntesis, see 24/7 place recognition by view synthesis. . Such steps are already taken in the paper Learning and aggregating deep local descriptors for instance-level recognition -- see the table with memory footprint. . . That is how one could have an interesting research challenge, also having some grounds in the real-world -- to work in mobile phones. Otherwise, any method would work, if the database is dense enough. . Robust local features matter for illumination changes . It is a bit hidden in the Appendix, so go directly to the Figure 9. It clearly shows that localization performance is bounded by SIFT, if it is used for two view matching, making retrieval improvements irrelevant. When R2D2 or D2Net are used for matching instead, the overall results for night-time are much better. . . That is in line with my small visual benchmark I did recently. . https://twitter.com/ducha_aiki/status/1330495426865344515 . . That&#39;s all, folks! Now please, check the paper and the code they provided. .",
            "url": "/wide-baseline-stereo-blog/2020/11/25/review-of-retrieval-for-localization.html",
            "relUrl": "/2020/11/25/review-of-retrieval-for-localization.html",
            "date": " • Nov 25, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Revisiting Brown patch dataset and benchmark",
            "content": "In this post . Why one needs good development set? What is wrong with existing sets for local patch descriptor learning? | One should validate in the same way, as it is used in production. | Brown patch revisited -- implementation details | Local patch descriptors evaluation results. | Really quick intro into local patch descriptors . Local patch descriptor is the thing, which helps you to automatically decide, if two patches in the pair of images correspond to the same point in a real world, or not. It should be robust to illumination, viewpoint and other changes. . . There are lots of ways how to implement a local patch descriptor: engineered and learned. Local patch descriptor is the crucial component of the wide baseline stereo pipeline and a popular computer vision research topic. . Why do you need development set? . Good data is crucial for any machine learning problem -- everyone now knows that. One needs high quality training set for training a good model. One also needs good test set, to know, what is real performance. However, there is one more, often forgotten, crucial component -- validation or development set. We use it to decide hyperparameters and validate design choices we make. It should be different from both training and test sets, yet, be good predictor of test set performance. Moreover, it should allow fast iterations, so be not too small. . While such set is commonly called validation set, I do like Andrew Ng&#39;s term &quot;development&quot; set more - because it helps to develop your model. . Existing datasets for local patch descriptors . So, what are the development set options for local patch descriptors? . Brown PhotoTourism. . The most commonly and successfully used dataset for local descriptor learning is PhotoTourism, created in 2008. Here is its description by authors: . The dataset consists of corresponding patches sampled from 3D reconstructions of the Statue of Liberty (New York), Notre Dame (Paris) and Half Dome (Yosemite). . It also comes with evaluation protocol:patch pairs are labeled as &quot;same&quot; or &quot;different&quot; and the false positive rate at recall of 95% (FPR95) is reported. The variable, used to build ROC curve is descriptor distance between two patches. Advantages: . It contains local patches, extracted for two types of local feature detector -- DoG (SIFT) and Harris corners. | It is extracted from images, contraining non-planar structures and the geometrical noise present is caused by the local feature detector, not added artificially. | Descriptors, trained on the dataset, show very good performance [IMW2020], therefore the data itself is good. | . Disadvantages: . when used as a benchmark, it shows unrealistic results: SIFT is 40x worse than deep learned descriptor. In practice, the difference is much smaller. | . HPatches . HPatches, where H stands for the &quot;homography&quot; was proposed to overcome the problem of unrealisting metric and, seemingly, too easy data, used in Phototourism dataset. . It was constructed in a different way than a Phototourism. First, local features were detected in the &quot;reference&quot; image and then reprojected to other images in sequences. Reprojection is prossible, because all the images are photographies of the planes -- graffity, drawing, print, etc, or are all taken from the same position. After the reprojection, some amount of geometrical noise -- rotation, translation, scaling, was added to the local features and the patches were extracted. . This process is illustration on the picture below (both taken from the HPatches website). . . HPAtches also provide 3 testing protocol, evaluating mean average precision (mAP) for 3 different tasks: patch verification -- similar to Brown Phototourism, image matching and patch retrieval. The variable, used to build mAP is descriptor distance between two patches. . Advantages: . Unlike PhotoTourism patch verification, image matching and patch retrieval tasks are not saturated. | HPatches contains illumination split, allowing the evaluation of descriptor robustness to illumination changes. | . Disadvantages: . patches &quot;misregistration&quot; noise is of artificial nature, although paper claims that it has similar statistics | no non-planar structure | performance in HPaptches does not really correlate with the downstream performance [IMW2020] | . . AMOSPatches . AMOS patches is &quot;HPatches illumination on steroids, without geometrical noise&quot;. It has the same advantanges and disadvantages, as HPatches and is mostly focused on illumination and weather changes. . . PhotoSynth . PhotoSynth can be described and something in the middle between Phototour and HPatches. It contains patches, sampled from planar scenes, as well as from non-planar scenes. . At first glance, it should be great for the test and training purposes. However, there are several issues with it. First, pre-trained HardNetPS descriptor, released together with the dataset, works well on HPatches, but poor in practice[pultar2020improving]. . Second, a couple of colleagues has tried to train the descriptor on top of it, as it was significantly worse than the authors reference model. Moreover, there is no testing/training code protocol available together with dataset. . So, while PhotoSynth might be a good dataset in principle, it definitely needs more love and work. . . . Designing the evaluation protocol . Classical local descriptor matching consists of two parts: finding nearest neighbors and filtering unreliable ones based on some criterion. I have wrote a blogpost, describing the matching strategies in details. . The most used in practice criterion is the first to second nearest neighbor distance (Lowe&#39;s) ratio threshold for filtering false positive matches. It is shown in the figure below. . The intuition is simple: if two candidates are too similar, then the match is unreliable and it is better to drop it. . . Somehow, none of the local patch evaluation protocols does not take such filtering criterion in mind, although it greatly influences the overall performance. . So, let&#39;s do the following: . Take the patches, which are extracted from only two images. | For the each patch, calculate the descriptor distance to the correct match and to the hardnest (closest) non-match. Calculate the Lowe&#39;s ratio between this two. | Calculate accuracy for each of such triplets. If the correct match has smaller distance, score 1, if not - 0. | Sort the ratios from smallest to biggest and calculate mean average precision#Mean_average_precision) (mAP). | Brown PhotoTour Revisied: implementation details . We have designed the protocol, now time for data. We could spend several month collecting and cleaning it...or we can just re-use great Brown PhotoTourism dataset. Re-visiting labeling and/or evaluation protocol of the time-tested dataset is a great idea. . Just couple of examples: ImageNette created by Jeremy Howard from ImageNet, Revisited Oxford 5k by Filip Radenovic and so on. . For the protocol we designed above we need the information about the image id, where the patch was extracted from. Unfortunately, there is no such information in the Brown PhotoTourism, but there is suitable alternative -- the image id, where the reference patch was detected. What does it mean? . Suppose, we have 4 images and 5 keypoints. All the keypoints present in all images, which gives us 20 patches. 3 keypoints were first detected in Image 1 and 2 in image 2. That means that we will have 12 patches labeled image 1 and 8 patches labeled image 2. . So, we will have results for image 1 and image 2. Let&#39;s consider image 1. There are 12 patches, splitted in 3 &quot;classes&quot;, 4 patches in each class. . Then, for the each of those 12 patches we: . pick each of the corresponding patched as positives, so 3 positives. $P_1$, $P_2$, $P_3$ | find the closest negative N. | add triplets (A, $P_1$, N), (A, $P_2$, N), (A, $P_3$, N) to the evaluation. | . Repeat the same for the image 2. That mimics the two-view matching process as close, as possible, given the data available to us. . Installation . pip install brown_phototour_revisited . How to use . There is a single function, which does everything for you: full_evaluation. The original Brown benchmark consider evaluation, similar to cross-validation: train descriptor on one subset, evaluate on two others, repeat for all, so 6 evaluations are required. For the handcrafted descriptors, or those, that are trained on 3rd party datasets, only 3 evaluations are necessary. We are following it here as well. . However, if you need to run some tests separately, or reuse some functions -- we will cover the usage below. In the following example we will show how to use full_evaluation to evaluate SIFT descriptor as implemented in kornia. . # !pip install kornia . import torch import kornia from IPython.display import clear_output from brown_phototour_revisited.benchmarking import * patch_size = 65 model = kornia.feature.SIFTDescriptor(patch_size, rootsift=True).eval() descs_out_dir = &#39;data/descriptors&#39; download_dataset_to = &#39;data/dataset&#39; results_dir = &#39;data/mAP&#39; results_dict = {} results_dict[&#39;Kornia RootSIFT&#39;] = full_evaluation(model, &#39;Kornia RootSIFT&#39;, path_to_save_dataset = download_dataset_to, path_to_save_descriptors = descs_out_dir, path_to_save_mAP = results_dir, patch_size = patch_size, device = torch.device(&#39;cuda:0&#39;), distance=&#39;euclidean&#39;, backend=&#39;pytorch-cuda&#39;) clear_output() print_results_table(results_dict) . Mean Average Precision wrt Lowe SNN ratio criterion on UBC Phototour Revisited trained on liberty notredame liberty yosemite notredame yosemite tested on yosemite notredame liberty Kornia RootSIFT 56.70 47.71 48.09 . Results . So, let&#39;s check how it goes. The latest results and implementation are in the following notebooks: . Deep descriptors | Non-deep descriptors | . The results are the following: . Mean Average Precision wrt Lowe SNN ratio criterion on UBC Phototour Revisited trained on liberty notredame liberty yosemite notredame yosemite tested on yosemite notredame liberty Kornia RootSIFT 32px 58.24 49.07 49.65 HardNet 32px 70.64 70.31 61.93 59.56 63.06 61.64 SOSNet 32px 70.03 70.19 62.09 59.68 63.16 61.65 TFeat 32px 65.45 65.77 54.99 54.69 56.55 56.24 SoftMargin 32px 69.29 69.20 61.82 58.61 62.37 60.63 HardNetPS 32px 55.56 49.70 49.12 R2D2_center_grayscal 61.47 53.18 54.98 R2D2_MeanCenter_gray 62.73 54.10 56.17 Mean Average Precision wrt Lowe SNN ratio criterion on UBC Phototour Revisited trained on liberty notredame liberty yosemite notredame yosemite tested on yosemite notredame liberty Kornia SIFT 32px 58.47 47.76 48.70 OpenCV_SIFT 32px 53.16 45.93 46.00 Kornia RootSIFT 32px 58.24 49.07 49.65 OpenCV_RootSIFT 32px 53.50 47.16 47.37 OpenCV_LATCH 65px -- -- -- 37.26 -- 39.08 OpenCV_LUCID 32px 20.37 23.08 27.24 skimage_BRIEF 65px 52.68 44.82 46.56 Kornia RootSIFTPCA 3 60.73 60.64 50.80 50.24 52.46 52.02 MKD-concat-lw-32 32p 72.27 71.95 60.88 58.78 60.68 59.10 . So far - in agreement with IMC benchmark: SIFT and RootSIFT are good, but not the best, SOSNet and HardNet are the leaders, but within tens of percents, not by orders of magnitude. . . Disclaimer 1: don&#39;t trust this tables fully . I haven&#39;t (yet!) checked if all the deep descriptors models, trained on Brown, were trained with flip-rotation 90 degrees augmentation. In the code below I assume that they were, however, it might not be true -- and the comparison might not be completely fair. I will do my best to check it, but if you know that I have used wrong weights - please open an issue. Thank you. . Disclaimer 2: it is not &quot;benchmark&quot;. . The intended usage of the package is not to test and report the numbers in the paper. Instead think about is as cross-validation tool, helping the development. Thus, one CAN tune hyperparameters based on the benchmark results instead of doing so on HPatches. After you have finished tuning, please, evaluate your local descriptors on some downstream task like IMC image matching benchmark or visual localization. . Summary . It really pays off, to spend time designing a proper evaluation pipeline and gathering the data for it. If you can re-use existing work - great. But don&#39;t blindly trust anything, even super-popular and widely adopted benchmarks. You need always check if the the protocol and data makes sense for your use-case personally. . Thanks for the reading, see you soon! . Citation . If you use the benchmark/development set in an academic work, please cite it. . @misc{BrownRevisited2020, title={UBC PhotoTour Revisied}, author={Mishkin, Dmytro}, year={2020}, url = {https://github.com/ducha-aiki/brown_phototour_revisited} } . References . [IMW2020] Jin Yuhe, Mishkin Dmytro, Mishchuk Anastasiia et al., ``Image Matching across Wide Baselines: From Paper to Practice&#39;&#39;, arXiv preprint arXiv:2003.01587, vol. , number , pp. , 2020. . [pultar2020improving] Pultar Milan, ``Improving the HardNet Descriptor&#39;&#39;, arXiv ePrint:2007.09699, vol. , number , pp. , 2020. .",
            "url": "/wide-baseline-stereo-blog/2020/09/23/local-descriptors-validation.html",
            "relUrl": "/2020/09/23/local-descriptors-validation.html",
            "date": " • Sep 23, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "How to match images taken from really extreme viewpoints?",
            "content": "In this post . What to do, if you are in a desperate need of matching this particular image pair? | What are the limitations of the affine-covariant detectors like Hessian-Affine or HesAffNet? | ASIFT: brute-force affine view synthesis | Do as little as possible: MODS | What is the key factor of affine view synthesis? Ablation study | How to match images taken from really extreme viewpoints? . Standard wide-baseline stereo or 3d reconstruction pipelines work well in the many situations. Even if some image pair is not matched, it is usually not a problem. For example, one could match images from very different viewpoints, if there is a sequence of images in between, as shown in Figure below, from &quot;From Single Image Query to Detailed 3D Reconstruction&quot; paper[SingleImage3dRec2015]. . . However, that might not always be possible. For example, the number of pictures is limited because they historical and there is no way how one could go and take more without inventing a time machine. . What to do? One way would be to use affine features like Hessian-AffNet[AffNet2018] or MSER[MSER2002]. However, they help only up to some extent and what if the view, we need to match are more extreme? . . The image pair above is from &quot;Location recognition over large time lags dataset&quot; paper [LostInPast2015]. . The solution is to simulate real viewpoint change by affine or perspective warps of the current image. This idea was first proposed by Lepetit and Fua in 2006[AffineTree2006]. You can think about it as a special version of test-time augmentation, popular nowadays in deep learning. Later affine view synthesis for wide baseline stereo was extended and mathematically justified by Morel &amp; Yu in ASIFT paper[ASIFT2009]. They proved that perspective image warps are can be approximated by synthetic affine views. . What is wrong with affine-covariant local detectors? . One could say that the goal of affine-covariant detectors like MSER, Hessian-Affine or Hessian-AffNet is to detect the same region on a planar surface, regardless the camera angle change. It is true to some extent, as we demostrate on toy example below with Hessian-Affine feature. . . The problem arises, when the image content, e.g. 3 blobs on the figure below are situated close to each other, so under the tilt transform the merge into a single blob. So it is not the shape of region, which is detected incorrectly, but the center of the features themselves. For clarity, we omited affine shape estimation on the image below. . . ASIFT: brute-force affine view synthesis . So, to solve the problem explained above, Morel &amp; Yu [ASIFT2009] proposed to do a lot affine warps of each image, as shown on the Figure below, as match each view against all others, which is $O(n^2)$ complexity, where $n$ is number of views generated. . . The motivation do doing so it that assuming, original image to be a fronto-parallel one, to cover viewsphere really dense, as shown in the Figure below. . . This leads to impressive performance on a very challenging image pairs, see an example below . . In this section I have used great illustrations done by Mariano Rodríguez for his paper &quot;Fast Affine Invariant Image Matching&quot; [FastASIFT2018]. Please, checkout his blog. . MODS: do as little as possible . The main drawback of ASIFT algorithm is a huge computational cost: 82 views are generated regardless of the image pair difficulty. To overcome this, we proposed MODS[MODS2015] algorithm: Matching with On-Demand Synthesis. . . One starts with the fastest detector-descriptor without view synthesys and then uses more and more computationally expensive methods if needed. Moreover, by using affine-covariant detectors like MSER or Hessian-Affine, one could synthetise significantly less views, saving computations spent on local descriptor and matching. . This, together with FGINN matching strategy, specifically designed for the handling re-detections, MODS is able to match more challenging image pairs in less time than ASIFT. . . Why does affine synthesis help? . Despite that ASIFT and other view-synthesis based approaches are know more than decade, we are not aware of a study, why does affine synthesis helps in practice. Could one get a similar performance without view synthesis? Specificallly: . May it be that the most of improvements come from the fact that we have much more features? That is why we fix the number of features for all approaches. | Some regions from ASIFT, when reprojected to the original image, are quite narrow. Could we get them just by removing edge-like feature filtering, which is done in SIFT, Hessian and other detectors. Denoted +edge | Instead of doing affine view synthesis, one could directly use the same affine parameters to get the affine regions to describe, so the each keypoint would have several associated regions+descriptors. Denoted +MD | Using AffNet to directly estimated local affine shape without multiple descriptors. Denoted +AffNet | Combine (1), (2) and (3). | So, we did the study on HPatches Sequences dataset, the hardest image pairs (1-6) of viewpoint subset. The metric is similar to one used in the &quot;Image Matching across Wide Baselines: From Paper to Practice&quot; and CVPR 2020 RANSAC in 2020 - mean average accuracy of the estimated homography. . . We run Hessian detector with RootSIFT descriptor, FLANN matching and LO-RANSAC, as implemented in MODS. Features are sorted according the the detector response and their total number is clipped to 2048 or 8000 to ensure that the improvements do not come from just having more features. . Note, that we do not study, if view synthesis helps for the regular image pairs - it might actually hurt performance, similarly to affine features. Instead we are focusing on the case, when view synthesis definitely helps: matching obscure views of the mostly planar scenes. . 8000 feature budget . Results are in Figure below. Indeed, all of the factors: detecting more edge-like features, having multiple descriptors or better affine shape improve results over the plain Hessian detector, but even all of the combined are not good enough to match performance of the affine view synthesis + plain Hessian detector. . But the best setup is to use both Hessian-AffNet and view synthesis. . . 2048 feature budget . The picture is a bit different in a small feature budget: neither multiple-(affine)-descriptors per keypoint, nor allowing edge-like feature help. From other hand, affine view synthesis still improves results of the Hessian. And, again, the best performance is achieved with combination of view synthesis and AffNet shape estimation. . . Summary . Affine view synthesis helps for matching challenging image pairs and its improvement are not just because of more local features used. It can be done effective and efficient -- in the iterative MODS framework. . References . [SingleImage3dRec2015] J.L. Schonberger, F. Radenovic, O. Chum et al., ``From Single Image Query to Detailed 3D Reconstruction&#39;&#39;, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015. . [AffNet2018] D. Mishkin, F. Radenovic and J. Matas, ``Repeatability is Not Enough: Learning Affine Regions via Discriminability&#39;&#39;, ECCV, 2018. . [MSER2002] J. Matas, O. Chum, M. Urban et al., ``Robust Wide Baseline Stereo from Maximally Stable Extrema Regions&#39;&#39;, BMVC, 2002. . [LostInPast2015] Fernando Basura, Tommasi Tatiana and Tuytelaars Tinne, ``Location recognition over large time lags&#39;&#39;, Computer Vision and Image Understanding, vol. 139, number , pp. , 2015. . [AffineTree2006] Lepetit Vincent and Fua Pascal, ``Keypoint Recognition Using Randomized Trees&#39;&#39;, IEEE Trans. Pattern Anal. Mach. Intell., vol. 28, number 9, pp. , sep 2006. . [ASIFT2009] Morel Jean-Michel and Yu Guoshen, ``ASIFT: A New Framework for Fully Affine Invariant Image Comparison&#39;&#39;, SIAM J. Img. Sci., vol. 2, number 2, pp. , apr 2009. . [FastASIFT2018] Rodríguez Mariano, Delon Julie and Morel Jean-Michel, ``Fast Affine Invariant Image Matching&#39;&#39;, Image Processing On Line, vol. 8, number , pp. , 2018. . [MODS2015] Mishkin Dmytro, Matas Jiri and Perdoch Michal, ``MODS: Fast and robust method for two-view matching &#39;&#39;, Computer Vision and Image Understanding , vol. , number , pp. , 2015. .",
            "url": "/wide-baseline-stereo-blog/2020/08/06/affine-view-synthesis.html",
            "relUrl": "/2020/08/06/affine-view-synthesis.html",
            "date": " • Aug 6, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Patch extraction: devil in details",
            "content": "When working with local features one needs to pay attention to even a smallest details, or the whole process can be ruined. One of such details is how to extract the patch, which will be described by local descriptor such as SIFT or HardNet. . . Unfortunately, we cannot just extract patch from the image by cropping the patch and then resizing it. Or can we? Let&#39;s check. We will use two versions of image: original and 4x smaller one and would like to extract same-looking fixed size patch from both of them. The patch we want to crop is showed by oriented red circle. . . Aliasing . And here what we get by doing a simple crop and resize to 32x32 pixels. . . Doesn&#39;t look good. It is called &quot;aliasing&quot; - a problem, which arise when we are trying to downscale big images into small resolution. Specifically: the original image contains finer details, than we could represent in thumbnail, which leads to artifacts. . The solution: anti-aliasing . The solution, which follows out of sampling theorem is known: remove the details, which cannot be seens in small image first, then resample image to small size. . The simplest way to remove the fine details is to blur image with the Gaussian kernel. . Lets do it and compare the results. . By the way, you can try for yourself, all the required code is here, in kornia-examples . Performance . The problem is solved. Or is it? . The problem with properly antialiased patch extraction is that it is quite slow for two reasons. First, blurring a whole image is a costly operation. But, the worst part is that the required amount of blur depends on the patch size in the original image, or, in other words, keypoint scale. So for extracting, say 8000 patches, one needs to perform blurring 8000 times. Moreover, if one wants to extract elongated region and warp it to the square patch, the amount of blur in vertical and horizontal directions should be different! . What can be done? Well, instead of doing blurring 8000 times, one could create so called scale pyramid and then pick the level, which is the closest to optimal one, predicted by theorem. . . This is exactly, what kornia function extract_patches_from_pyramid does. . Also - I have a bit cheated with you above: the &quot;anti-aliased&quot; patches were actually extracted using the function above. . How it impacts local descriptor matching? . Let&#39;s do the toy example first - describe four patches we have in the example above with HardNet descriptor and calculate the distance between them. . . So the descriptor difference between antialiased patches is 0.09 and between naively extracted -- 0.44. 0.09 is not a big deal, but 0.44 is a lot, actually. . Let&#39;s move to the non-toy example from the paper devoted to this topic: &quot;A Few Things One Should Know About Feature Extraction, Description and Matching&quot;[PatchExtraction2014]. . The original data is lost I am too lazy to redo the experiments for the post, so I will just copy-past images with results. Here are abbrevations used in the paper: . OPE -- Optimal Patch Extraction. The most correct and slow way of extracting, including different amount of bluring in different directions. . | NBPE -- No-Blur Patch Extraction. The most naive way we started with . | PNBPE -- Pyramid, No-Blur Patch Extraction. The one, we described above - sampling patches from scale pyramid. . | PSPE Pyramid-Smoothing Patch Extraction. Pick the matching pyramid level and then add anisotropic blur missing. . | . As you can see, doing things optimally is quite slow. . Now let&#39;s see how it influences performance. . . It looks like that influence is smaller than we thought. But recall that the experiment above is for SIFT descriptor only. Doing pyramid helps for the small viewpoint change almost as good, as going fully optimal, but with increasing the viewpoint difference, such approximation degrades. Moreover, it influnces MSER detector much more that than Hessian-Affine. . How does it work with deep descriptors like HardNet or SoSNet? That is the question which not answered yet. Drop me a message if you want to do it yourself and we can do the follow-up post together. . References . [PatchExtraction2014] K. Lenc, J. Matas and D. Mishkin, ``A few things one should know about feature extraction, description and matching&#39;&#39;, Proceedings of the Computer Vision Winter Workshop, 2014. .",
            "url": "/wide-baseline-stereo-blog/2020/07/22/patch-extraction.html",
            "relUrl": "/2020/07/22/patch-extraction.html",
            "date": " • Jul 22, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Local affine features: useful side product",
            "content": "Keypoints are not just points . Wide baseline stereo matching often as perceived as establishing (key-)point correspondences between images. While this might be true for the some local features like SuperPoint [SuperPoint2017], typically it is more than that. . Specifically, detectors like DoG[Lowe99], Harris[Harris88], Hessian[Hessian78], KeyNet[KeyNet2019], ORB[ORB2011], and many others rate on scale-space provide at least 3 parameters: x, y, and scale. . Most of the local descriptors -- SIFT[Lowe99], HardNet[HardNet2017] and so on -- are not rotation invariant and those which are - mostly require complex matching function[RIFT2005], [sGLOH2], so the patch orientation has to be estimated anyway, in order to match reliably. This can be done by various methods: corners center of mass (ORB[ORB2011], dominant gradient orientation (SIFT)[Lowe99] or by some learned estimator (OriNets[OriNet2016],[AffNet2018]). Sometimes it is possible to rely on smartphone camera IMU or photographer and assume that images are upright[PerdochRetrieval2009]. . Thus, we can assume that if local descriptors match, this means the local feature scale and orientation also match, at least approximately -- see Figure below. Possible exceptions are cases, when the patch is symmetrical and orientation is ambiguous up to some symmetry group. . . In addition, one could assume that we observe the patch not from the fronto-parallel position and try to estimate local normal, or, more precisely, affine shape of the feature point, modeling it as an ellipse instead of circle. One could also think of affine shape estimation as finding the camera position, from where the patch is seen in some &quot;canonical&quot; view. . . This gives us 3 points correspondences from a single local feature match, see an example in Figure below. . . Why is it important and how to use it -- see in current post. How to esimate local affine features robustly -- in the next post. . Benefits of local affine features . Making descriptor job easier . The most straightforward benefit of using local affine features is that they increase the repeatability of the detector and potentially reduce appearance changes of a local patch caused by viewpoint difference. This makes possible matching more challenging image pairs. . . . The practice is a little bit more complicated. Our recent benchmark[IMW2020], which measure the accuracy of the output fundamental matrix, shows that the difference in using affine and similarity-covariant features is quite minor. Specifically, the relative difference between SIFT vs SIFT-Affine features is 0.5% and between Hessian-SIFT and Hessian-AffNet SIFT is 5.1%, see Table below. . . Therefore, if the benefit of local features would be to only improve descriptor extraction stage, it would be arguably not worth it. Luckily, there are more benefits, which are more pronounced. . Making RANSAC job easier . Let&#39;s recall how RANSAC works. . Randomly sample a minimally required number of tentative correspondences to fit the geometrical model of the scene: 4 for homography, 7 for epipolar geometry and estimate the model. | Calculate &quot;support&quot;: other correspondeces, which are consistent with the model. | Repeat steps (1), (2) and output the model which is supported with the most of correspondences. If you were lucky and have sampled all-inlier sample, meaning that all correspondences used to estimate the model were correct, you would have a correct model. | Reality is more complicated than I have just described, but the principle is the same. The most important part is the sampling and it is sensitive to inlier ratio $ nu$ - the percentage of the correct correspondences in the set. Lets denote the minimal number of correspondences required to estimate the model as m. To recover the correct model with the confidence p one needs to sample the number of correspondences, which is described by formula: . begin{equation} N = frac{ log{(1 - p)}}{ log{(1 - nu^{m})}} end{equation}Lets plot the how the number of required samples changes with inlier ratio for confidence equal 99%. Note the log scale on Y axis. Different lines are for different minimal sample size m. . . As you can see from the plot above, reducing the minimal sample size required for the model estimation even by 1 saves and order of magnitude of computation. In reality the benefit is a smaller, as modern RANSACs like GC-RANSAC[gcransac2018] and MAGSAC[magsac2019] could estimate the correct model from the sample containing outliers, but it is still huge, especially for low inlier rate cases. . Image retrieval . The ideal case would be to estimate a model from just a single sample and that is exactly what is done in spatial reranking paper &quot;Object retrieval with large vocabularies and fast spatial matching&quot; by Philbin et.al [Philbin07]. . Specifically, they are solving a particular object retrieval problem: given an image containing some object, return all the images from the database, which also containg the same object. . The inital list of images is formed by the descriptor distance and then is reranked. The authors propose to approximate a perspective change between two images as an affine image transformation, and count number of feature points, which are reprojected inside the second image. This number produces better ranking that the original short-list. . . Back to wide baseline stereo . While working for spatial re-ranking, 3-degrees of freedom camera model is too rough for the wide baseline stereo. Yet, going from 4 point correspondences (PC) to 2 affine correspondences (AC) for homogaphy and from 7 PC to 3 AC for the fundamental matrix would be huge benefit anyway for the robust model estimation. . Various variant of RANSAC working for local features were proposed in the last 15 years: Perdoch et.al[perd2006epipolar], Pritts et.al.[PrittsRANSAC2013], Barath and Kukelova [Barath2019ICCV], Rodríguez et.al[RANSACAffine2020]. . Finally, the systematic study of using is presented by Barath et.al[barath2020making] in &quot;Making Affine Correspondences Work in Camera Geometry Computation&quot; paper. Authors show that if used naively, affine correspondence lead to worse results, because they are more noisy than point correspondences. However, there is a bag of tricks presented in the paper, which allow to solve the noise issue and make the affine RANSAC working in practice, resulting in orders of magnitude faster computation. . . Moreover, for a special cases like autonomous driving, where the motion is mostly horizonal, one could even use 2 affine correspondes for both motion estimation and consistency check, significantly improving the efficiency of the outliers removal compared to the standard RANSAC loop[guan2020relative]. . . Besides the special case considerations, additional contraints can also come from running other algorithms, like monocular depth estimation. Such a constraint could reduce the required number of matches from two affine correspondences to a single one for calibrated camera case[OneACMonoDepth2020]. . . . Application-specific benefits . Besides the wide baseline stereo, local affine features and correspondences have other applications. I will briefly describe some of them here (to be updated). . Image rectification . Instead of matching local features between two images one might match them within a single image. Why would someone do it? This allows finding repeated pattern: think about windows, doors and so on. Typically they have the same physical size, therefore the diffrence in local features around them could tell us about the geometry of the scene and lens distortion. . . This is the idea of the series of works by Pritts and Chum. . . Surface normals estimation . Ivan Eichhardt and Levente Hajder have a series of works, exploiting the local affine correspondences for surface normals estimation[SurfaceNormals2019] . . Summary . Despite not being popular right now, treating keypoints as local affine features has a lot of advantages over the traditional treatment the local correspondence as the point correspondences. In the next post I will describe a way of estimating the local feature affine shape and orientation. . References . [SuperPoint2017] Detone D., Malisiewicz T. and Rabinovich A., ``Superpoint: Self-Supervised Interest Point Detection and Description&#39;&#39;, CVPRW Deep Learning for Visual SLAM, vol. , number , pp. , 2018. . [Lowe99] D. Lowe, ``Object Recognition from Local Scale-Invariant Features&#39;&#39;, ICCV, 1999. . [Harris88] C. Harris and M. Stephens, ``A Combined Corner and Edge Detector&#39;&#39;, Fourth Alvey Vision Conference, 1988. . [Hessian78] P.R. Beaudet, ``Rotationally invariant image operators&#39;&#39;, Proceedings of the 4th International Joint Conference on Pattern Recognition, 1978. . [KeyNet2019] A. Barroso-Laguna, E. Riba, D. Ponsa et al., ``Key.Net: Keypoint Detection by Handcrafted and Learned CNN Filters&#39;&#39;, ICCV, 2019. . [ORB2011] E. Rublee, V. Rabaud, K. Konolidge et al., ``ORB: An Efficient Alternative to SIFT or SURF&#39;&#39;, ICCV, 2011. . [HardNet2017] A. Mishchuk, D. Mishkin, F. Radenovic et al., ``Working Hard to Know Your Neighbor&#39;s Margins: Local Descriptor Learning Loss&#39;&#39;, NeurIPS, 2017. . [RIFT2005] {Lazebnik} S., {Schmid} C. and {Ponce} J., ``A sparse texture representation using local affine regions&#39;&#39;, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 27, number 8, pp. 1265-1278, 2005. . [sGLOH2] {Bellavia} F. and {Colombo} C., ``Rethinking the sGLOH Descriptor&#39;&#39;, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 40, number 4, pp. 931-944, 2018. . [OriNet2016] K. M., Y. Verdie, P. Fua et al., ``Learning to Assign Orientations to Feature Points&#39;&#39;, CVPR, 2016. . [AffNet2018] D. Mishkin, F. Radenovic and J. Matas, ``Repeatability is Not Enough: Learning Affine Regions via Discriminability&#39;&#39;, ECCV, 2018. . [PerdochRetrieval2009] M. {Perd&#39;och}, O. {Chum} and J. {Matas}, ``Efficient representation of local geometry for large scale object retrieval&#39;&#39;, CVPR, 2009. . [IMW2020] Jin Yuhe, Mishkin Dmytro, Mishchuk Anastasiia et al., ``Image Matching across Wide Baselines: From Paper to Practice&#39;&#39;, arXiv preprint arXiv:2003.01587, vol. , number , pp. , 2020. . [gcransac2018] D. Barath and J. Matas, ``Graph-Cut RANSAC&#39;&#39;, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. . [magsac2019] J.N. Daniel Barath, ``MAGSAC: marginalizing sample consensus&#39;&#39;, CVPR, 2019. . [Philbin07] J. Philbin, O. Chum, M. Isard et al., ``Object Retrieval with Large Vocabularies and Fast Spatial Matching&#39;&#39;, CVPR, 2007. . [perd2006epipolar] M. Perd&#39;och, J. Matas and O. Chum, ``Epipolar geometry from two correspondences&#39;&#39;, ICPR, 2006. . [PrittsRANSAC2013] J. {Pritts}, O. {Chum} and J. {Matas}, ``Approximate models for fast and accurate epipolar geometry estimation&#39;&#39;, 2013 28th International Conference on Image and Vision Computing New Zealand (IVCNZ 2013), 2013. . [Barath2019ICCV] D. Barath and Z. Kukelova, ``Homography From Two Orientation- and Scale-Covariant Features&#39;&#39;, ICCV, 2019. . [RANSACAffine2020] M. {Rodríguez}, G. {Facciolo}, R. G. et al., ``Robust estimation of local affine maps and its applications to image matching&#39;&#39;, 2020 IEEE Winter Conference on Applications of Computer Vision (WACV), 2020. . [barath2020making] Barath Daniel, Polic Michal, Förstner Wolfgang et al., ``Making Affine Correspondences Work in Camera Geometry Computation&#39;&#39;, arXiv preprint arXiv:2007.10032, vol. , number , pp. , 2020. . [guan2020relative] Guan Banglei, Zhao Ji, Barath Daniel et al., ``Relative Pose Estimation for Multi-Camera Systems from Affine Correspondences&#39;&#39;, arXiv preprint arXiv:2007.10700, vol. , number , pp. , 2020. . [OneACMonoDepth2020] D.B. Ivan Eichhardt, ``Relative Pose from Deep Learned Depth and a Single Affine Correspondence&#39;&#39;, ECCV, 2020. . [SurfaceNormals2019] {Baráth} D., {Eichhardt} I. and {Hajder} L., ``Optimal Multi-View Surface Normal Estimation Using Affine Correspondences&#39;&#39;, IEEE Transactions on Image Processing, vol. 28, number 7, pp. 3301-3311, 2019. .",
            "url": "/wide-baseline-stereo-blog/2020/07/17/affine-correspondences.html",
            "relUrl": "/2020/07/17/affine-correspondences.html",
            "date": " • Jul 17, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "WxBS: Wide Multiple Baseline Stereo as a task",
            "content": "Definition of WxBS . Let us denote observations $O_{i}, i=1..n$, each of which belongs to one of the views $V_{j}, j=1..m$, $m leq n$. . Observation consist of spatial information and the &quot;descriptor&quot;. View contains the information, which is shared and the same for a group of observations. . For example, a single observation can be an RGB pixel. Its spatial information is the pixel coordinates and the &quot;descriptor&quot; is RGB value. The view then is the image, with information about the camera pose, camera intrinsics, sensor and the time of the photo. Some of this information can be unknown to the user, i.e. hidden variable. . Another example could an event camera[EventCameraSurvey2020]. In that case the observation contains the pixel coordinates and the descriptor is the sign of the intensity change. The view will contain the information about the sensor, camera pose and the single observation inside it, because every event has an unique timestamp. . Observations and views can be of different nature and dimentionality. E.g. $V_1$, $V_2$ -- RGB images, $V_3$ -- point cloud from a laser scaner, $V_4$ -- image from a thermal camera, and so on. . An unordered pair of observations $(O_{i},O_{k})$ forms a correspondence $c_{ik}$ if they are belong to different views $V_{j}$. The group of observations is called multivew correspondence $C_o$, when there is exactly one observation $O_i$ per view $V_j$. Some of observations $O_i$ can be empty $ varnothing$, i.e. not observed in the specific view $V_j$. . The world model is the set of contraints on views, observations and correspondences. For example, one the popular models are epipolar geometry and ridid motion assumption. . The correspondence is called ground truth or veridical if it satisfy the constraints posed be the world model. . We can now define a wide baseline stereo. . By wide baseline stereo we understand the process of establishing two or multi-view correspondences $C_o$ from observations $O_i$ and images $V_{j}$ and recovering the missing information about the views and estimatting the unknown parameters of the world model. . Most often in the current thesis we will be using the the following world model. The scene consists of 3 dimentional elements, and is rigid and static. The observations are the 2D projections to the camera plane by the projectice pinhole camera. The relationship between observations in different views is either epipolar geometry, or projective transform[Hartley2004]. Any moving object does not satisty the world model and therefore is considered an occlusion. We will call the &quot;baseline&quot; the distance between the camera centers. . For example, on image below, observations $O_i$ are blue circles and the correspondences $c_{jk}$ are shown as lines. The assumed object $X_i$ is a red circle. . . We will call &quot;wide multiple baseline stereo&quot; or WxBS [Mishkin2015WXBS] if the observations have different nature or the conditions under which observation were made are different. . The different between wide baseline stereo and short baseline stereo, or, simply stereo is the follwing. In stereo the baseline is small -- less then 1 meter -- and typically known and fixed. The task is to establish correspondences, which can be done by 1D search along the known epipolar lines. . In contrast, in wide baseline stereo the baseline is unknown, mostly unconstrained and the viewpoints of the cameras can vary drastically. . The wide baseline stereo, which also outputs the estimation of the latent objects, e.g. in form of 3d point world coordinates we would call rigid structure-from-motion (rigid SfM) or 3D reconstruction. We do not consider object shape approximation with voxels, meshes, etc in the current thesis. Nor we consider the recovery of scene albedo, illumination, and other appearance properties. . While the difference between SfM and WBS is often blurred and the terms are used interchangeably, we would consider WBS as a part of SfM pipeline prior to recovering 3d point cloud. . Other correspondence problems, as tracking, optical flow or establishing semantic correspondences could be defined using the terminilogy we established. . . References . (Gallego, Delbruck et al., 2020) Gallego Guillermo, Delbruck Tobi, Orchard Garrick Michael et al., ``Event-based Vision: A Survey&#39;&#39;, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. , number , pp. , 2020. . (Hartley and Zisserman, 2004) R.~I. Hartley and A. Zisserman, ``Multiple View Geometry in Computer Vision&#39;&#39;, 2004. . (Mishkin, Matas et al., 2015) D. Mishkin, J. Matas, M. Perdoch et al., ``WxBS: Wide Baseline Stereo Generalizations&#39;&#39;, BMVC, 2015. .",
            "url": "/wide-baseline-stereo-blog/2020/07/09/wxbs.html",
            "relUrl": "/2020/07/09/wxbs.html",
            "date": " • Jul 9, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "The Role of Wide Baseline Stereo in the Deep Learning World",
            "content": "Rise of Wide Multiple Baseline Stereo . The wide multiple baseline stereo (WxBS) is a process of establishing a sufficient number of pixel or region correspondences from two or more images depicting the same scene to estimate the geometric relationship between cameras, which produced these images. Typically, WxBS relies on the scene rigidity -- the assumption that there is no motion in the scene except the motion of the camera itself. The stereo problem is called wide multiple baseline if the images are significantly different in more than one aspect: viewpoint, illumination, time of acquisition, and so on. Historically, people were focused on the simpler problem with a single baseline, which was geometrical, i.e., viewpoint difference between cameras, and the area was known as wide baseline stereo. Nowadays, the field is mature and research is focused on solving more challenging multi-baseline problems. . WxBS is a building block of many popular computer vision applications, where spatial localization or 3D world understanding is required -- panorama stitching, 3D reconstruction, image retrieval, SLAM, etc. . If the wide baseline stereo is a new concept for you, I recommend checking the examplanation in simple terms. . . Where does wide baseline stereo come from? . As often happens, a new problem arises from the old -- narrow or short baseline stereo. In the narrow baseline stereo, images are taken from nearby positions, often exactly at the same time. One could find correspondence for the point $(x,y)$ from the image $I_1$ in the image $I_2$ by simply searching in some small window around $(x,y)$[Hannah1974ComputerMO,Moravec1980] or, assuming that camera pair is calibrated and the images are rectified -- by searching along the epipolar line[Hartley2004]. . . One of the first, if not the first, approaches to the wide baseline stereo problem was proposed by Schmid and Mohr [Schmid1995] in 1995. Given the difficulty of the wide multiple baseline stereo task at the moment, only a single geometrical -- baseline was considered, thus the name -- wide baseline stereo (WBS). The idea of Schmid and Mohr was to equip each keypoint with an invariant descriptor. This allowed establishing tentative correspondences between keypoints under viewpoint and illumination changes, as well as occlusions. One of the stepping stones was the corner detector by Harris and Stevens [Harris88], initially used for the application of tracking. It is worth a mention, that there were other good choices for the local feature detector at the time, starting with the Forstner [forstner1987fast], Moravec [Moravec1980] and Beaudet feature detectors [Hessian78]. . The Schmid and Mohr approach was later extended by Beardsley, Torr and Zisserman [Beardsley96] by adding RANSAC [RANSAC1981] robust geometry estimation and later refined by Pritchett and Zisserman [Pritchett1998,Pritchett1998b] in 1998. The general pipeline remains mostly the same until now [WBSTorr99,CsurkaReview2018,IMW2020], which is shown in Figure below. . . Let&#39;s write down the WxBS algorithm: . Compute interest points/regions in all images independently | For each interest point/region compute a descriptor of their neigborhood (local patch). | Establish tentative correspondences between interest points based on their descriptors. | Robustly estimate geometric relation between two images based on tentative correspondences with RANSAC. | The reasoning behind each step is described in this separate post. . Quick expansion . This algorithm significantly changed computer vision landscape for next forteen years. . Soon after the introduction of the WBS algorithm, it became clear that its quality significantly depends on the quality of each component, i.e., local feature detector, descriptor, and geometry estimation. Local feature detectors were designed to be as invariant as possible, backed up by the scale-space theory, most notable developed by Lindenberg [Lindeberg1993,Lindeberg1998,lindeberg2013scale]. A plethora of new detectors and descriptors were proposed in that time. We refer the interested reader to these two surveys: by Tuytelaars and Mikolajczyk [Tuytelaars2008] (2008) and by Csurka etal [CsurkaReview2018] (2018). Among the proposed local features is one of the most cited computer vision papers ever -- SIFT local feature [Lowe99,SIFT2004]. Besides the SIFT descriptor itself, . Lowe&#39;s paper incorporated several important steps, proposed earlier with his co-authors, to the matching pipeline. Specifically, they are quadratic fitting of the feature responses for precise keypoint localization [QuadInterp2002], using the Best-Bin-First kd-tree [aknn1997] as an approximate nearest neightbor search engine to speed-up the tentative correspondences generation, and using second-nearest neighbor (SNN) ratio to filter the tentative matches. It is worth noting that SIFT feature became popular only after Mikolajczyk benchmark paper [MikoDescEval2003,Mikolajczyk05] that showed its superiority to the rest of alternatives. . Robust geometry estimation was also a hot topic: a lot of improvements over vanilla RANSAC were proposed. For example, LO-RANSAC [LOransac2003] proposed an additional local optimization step into RANSAC to significantly decrease the number of required steps. PROSAC [PROSAC2005] takes into account the tentative correspondences matching score during sampling to speed up the procedure. DEGENSAC [Degensac2005] improved the quality of the geometry estimation in the presence of a dominant plane in the images, which is the typical case for urban images. We refer the interested reader to the survey by Choi etal [RANSACSurvey2009]. . Success of wide baseline stereo with SIFT features led to aplication of its components to other computer vision tasks, which were reformulated through wide baseline stereo lens: . Scalable image search. Sivic and Zisserman in famous &quot;Video Google&quot; paper[VideoGoogle2003] proposed to treat local features as &quot;visual words&quot; and use ideas from text processing for searching in image collections. Later even more WBS elements were re-introduced to image search, most notable -- spatial verification[Philbin07]: simplified RANSAC procedure to verify if visual word matches were spatially consistent. | . . Image classification was performed by placing some classifier (SVM, random forest, etc) on top of some encoding of the SIFT-like descriptors, extracted sparsely[Fergus03,CsurkaBoK2004] or densely[Lazebnik06]. | . . Object detection was formulated as relaxed wide baseline stereo problem[Chum2007Exemplar] or as classification of SIFT-like features inside a sliding window [HoG2005] | . . Semantic segmentation was performed by classicication of local region descriptors, typically, SIFT and color features and postprocessing afterwards[Superparsing2010]. | . Of course,wide baseline stereo was also used for its direct applications: . 3D reconstruction was based on camera poses and 3D points, estimated with help of SIFT features [PhotoTourism2006,RomeInDay2009,COLMAP2016] | . . SLAM(Simultaneous localization and mapping) [Se02,PTAM2007,Mur15] were based on fast version of local feature detectors and descriptors. . | Panorama stiching [Brown07] and, more generally, feature-based image registration[DualBootstrap2003] were initalized with a geometry obtained by WBS and then further optimized . | . Deep Learning Invasion: retreal to the geometrical fortress . In 2012 the deep learning-based AlexNet [AlexNet2012] approach beat all methods in image classification at the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). Soon after, Razavian et al.[Astounding2014] have shown that convolutional neural networks (CNNs) pre-trained on the Imagenet outperform more complex traditional solutions in image and scene classification, object detection and image search, see Figure below. The performance gap between deep leaning and &quot;classical&quot; solutions was large and quickly increasing. In addition, deep learning pipelines, be it off-the-shelf pretrained, fine-tuned or the end-to-end learned networks, are simple from the engineering perspective. That is why the deep learning algorithms quickly become the default option for lots of computer vision problems. . . However, there was still a domain, where deep learned solutions failed, sometimes spectacularly: geometry-related tasks. Wide baseline stereo [Melekhov2017relativePoseCnn], visual localization [PoseNet2015] and SLAM are still areas, where the classical wide baseline stereo dominates [sattler2019understanding,zhou2019learn,pion2020benchmarking]. . The full reasons why convolution neural network pipelines are struggling to perform tasks that are related to geometry, and how to fix that, are yet to be understood. The observations from the recent papers are following: . CNN-based pose predictions predictions are roughly equivalent to the retrieval of the most similar image from the training set and outputing its pose [sattler2019understanding]. This kind of behaviour is also observed in a related area: single-view 3D reconstruction performed by deep networks is essentially a retrieval of the most similar 3D model from the training set [Tatarchenko2019]. | Geometric and arithmetic operations are hard to represent via vanilla neural networks (i.e., matrix multiplication followed by non-linearity) and they may require specialized building blocks, approximating operations of algorithmic or geometric methods, e.g. spatial transformers [STN2015] and arithmetic units [NALU2018,NAU2020]. Even with such special-purpose components, the deep learning solutions require &quot;careful initialization, restricting parameter space, and regularizing for sparsity&quot; [NAU2020]. | Vanilla CNNs suffer from sensitivity to geometric transformations like scaling and rotation [GroupEqCNN2016] or even translation [MakeCNNShiftInvariant2019]. The sensitivity to translations might sound counter-intuitive, because the concolution operation by definition is translation-covariant. However, a typical CNN contains also zero-padding and downscaling operations, which break the covariance [MakeCNNShiftInvariant2019,AbsPositionCNN2020]. Unlike them, classical local feature detectors are grounded on scale-space [lindeberg2013scale] and image processing theories. Some of the classical methods deal with the issue by explicit geometric normalization of the patches before description. | CNNs predictions can be altered by a change in a small localized area [AdvPatch2017] or even a single pixel [OnePixelAttack2019], while the wide baseline stereo methods require the consensus of different independent regions. | . Today: assimilation and merging . Wide baseline stereo as a task: formulate differentiably and learn modules . This leads us to the following question -- is deep learning helping WxBS today? The answer is yes. After the quick interest in the black-box-style models, the current trend is to design deep learning solutions for the wide baseline stereo in a modular fashion [cv4action2019], resembling the one in Figure below. Such modules are learned separately. For example, the HardNet [HardNet2017] descriptor replaces SIFT local descriptor. The Hessian detector can be replaced by deep learned detectors like KeyNet [KeyNet2019] or the joint detector-descriptor [SuperPoint2017,R2D22019,D2Net2019]. The matching and filtering are performed by the SuperGlue [sarlin2019superglue] matching network, etc. There have been attempts to formulate the full pipeline solving problem like SLAM [gradslam2020] in a differentiable way, combining the advantages of structured and learning-based approaches. . . . Wide baseline stereo as a idea: consensus of local independent predictions . On the other hand, as an algorithm, wide baseline stereo is summarized into two main ideas . Image should be represented as set of local parts, robust to occlusion, and not influencing each other. | Decision should be based on spatial consensus of local feature correspondences. | One of modern revisit of wide baseline stereo ideas is Capsule Networks[CapsNet2011,CapsNet2017]. Unlike vanilla CNNs, capsule networks encode not only the intensity of feature response, but also its location. Geometric agreement between &quot;object parts&quot; is a requirement for outputing a confident prediction. . Similar ideas are now explored for ensuring adversarial robustness of CNNs[li2020extreme]. . Another way of using &quot;consensus of local independent predictions&quot; is used in Cross-transformers paper: spatial attention helps to select relevant feature for few-shot learning, see Figure below. . While wide multiple baseline stereo is a mature field now and does not attract even nearly as much attention as before, it continues to play an important role in computer vision. . . . References . [Hannah1974ComputerMO] M. J., ``Computer matching of areas in stereo images.&#39;&#39;, 1974. . [Moravec1980] Hans Peter Moravec, ``Obstacle Avoidance and Navigation in the Real World by a Seeing Robot Rover&#39;&#39;, 1980. . [Hartley2004] R.~I. Hartley and A. Zisserman, ``Multiple View Geometry in Computer Vision&#39;&#39;, 2004. . [Schmid1995] Schmid Cordelia and Mohr Roger, ``Matching by local invariants&#39;&#39;, , vol. , number , pp. , 1995. online . [Harris88] C. Harris and M. Stephens, ``A Combined Corner and Edge Detector&#39;&#39;, Fourth Alvey Vision Conference, 1988. . [forstner1987fast] W. F{ &quot;o}rstner and E. G{ &quot;u}lch, ``A fast operator for detection and precise location of distinct points, corners and centres of circular features&#39;&#39;, Proc. ISPRS intercommission conference on fast processing of photogrammetric data, 1987. . [Hessian78] P.R. Beaudet, ``Rotationally invariant image operators&#39;&#39;, Proceedings of the 4th International Joint Conference on Pattern Recognition, 1978. . [Beardsley96] P. Beardsley, P. Torr and A. Zisserman, ``3D model acquisition from extended image sequences&#39;&#39;, ECCV, 1996. . [RANSAC1981] Fischler Martin A. and Bolles Robert C., ``Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography&#39;&#39;, Commun. ACM, vol. 24, number 6, pp. 381--395, jun 1981. . [Pritchett1998] P. Pritchett and A. Zisserman, ``Wide baseline stereo matching&#39;&#39;, ICCV, 1998. . [Pritchett1998b] P. Pritchett and A. Zisserman, ``&quot;Matching and Reconstruction from Widely Separated Views&quot;&#39;&#39;, 3D Structure from Multiple Images of Large-Scale Environments, 1998. . [WBSTorr99] P. Torr and A. Zisserman, ``Feature Based Methods for Structure and Motion Estimation&#39;&#39;, Workshop on Vision Algorithms, 1999. . [CsurkaReview2018] {Csurka} Gabriela, {Dance} Christopher R. and {Humenberger} Martin, ``From handcrafted to deep local features&#39;&#39;, arXiv e-prints, vol. , number , pp. , 2018. . [IMW2020] Jin Yuhe, Mishkin Dmytro, Mishchuk Anastasiia et al., ``Image Matching across Wide Baselines: From Paper to Practice&#39;&#39;, arXiv preprint arXiv:2003.01587, vol. , number , pp. , 2020. . [Lindeberg1993] Lindeberg Tony, ``Detecting Salient Blob-like Image Structures and Their Scales with a Scale-space Primal Sketch: A Method for Focus-of-attention&#39;&#39;, Int. J. Comput. Vision, vol. 11, number 3, pp. 283--318, December 1993. . [Lindeberg1998] Lindeberg Tony, ``Feature Detection with Automatic Scale Selection&#39;&#39;, Int. J. Comput. Vision, vol. 30, number 2, pp. 79--116, November 1998. . [lindeberg2013scale] Lindeberg Tony, ``Scale-space theory in computer vision&#39;&#39;, , vol. 256, number , pp. , 2013. . [Tuytelaars2008] Tuytelaars Tinne and Mikolajczyk Krystian, ``Local Invariant Feature Detectors: A Survey&#39;&#39;, Found. Trends. Comput. Graph. Vis., vol. 3, number 3, pp. 177--280, July 2008. . [Lowe99] D. Lowe, ``Object Recognition from Local Scale-Invariant Features&#39;&#39;, ICCV, 1999. . [SIFT2004] Lowe David G., ``Distinctive Image Features from Scale-Invariant Keypoints&#39;&#39;, International Journal of Computer Vision (IJCV), vol. 60, number 2, pp. 91--110, 2004. . [QuadInterp2002] M. Brown and D. Lowe, ``Invariant Features from Interest Point Groups&#39;&#39;, BMVC, 2002. . [aknn1997] J.S. Beis and D.G. Lowe, ``Shape Indexing Using Approximate Nearest-Neighbour Search in High-Dimensional Spaces&#39;&#39;, CVPR, 1997. . [MikoDescEval2003] K. Mikolajczyk and C. Schmid, ``A Performance Evaluation of Local Descriptors&#39;&#39;, CVPR, June 2003. . [Mikolajczyk05] Mikolajczyk K., Tuytelaars T., Schmid C. et al., ``A Comparison of Affine Region Detectors&#39;&#39;, IJCV, vol. 65, number 1/2, pp. 43--72, 2005. . [LOransac2003] O. Chum, J. Matas and J. Kittler, ``Locally Optimized RANSAC&#39;&#39;, Pattern Recognition, 2003. . [PROSAC2005] O. Chum and J. Matas, ``Matching with PROSAC -- Progressive Sample Consensus&#39;&#39;, Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&#39;05) - Volume 1 - Volume 01, 2005. . [Degensac2005] O. Chum, T. Werner and J. Matas, ``Two-View Geometry Estimation Unaffected by a Dominant Plane&#39;&#39;, CVPR, 2005. . [RANSACSurvey2009] S. Choi, T. Kim and W. Yu, ``Performance Evaluation of RANSAC Family.&#39;&#39;, BMVC, 2009. . [VideoGoogle2003] J. Sivic and A. Zisserman, ``Video Google: A Text Retrieval Approach to Object Matching in Videos&#39;&#39;, ICCV, 2003. . [Philbin07] J. Philbin, O. Chum, M. Isard et al., ``Object Retrieval with Large Vocabularies and Fast Spatial Matching&#39;&#39;, CVPR, 2007. . [Fergus03] R. Fergus, P. Perona and A. Zisserman, ``Object Class Recognition by Unsupervised Scale-Invariant Learning&#39;&#39;, CVPR, 2003. . [CsurkaBoK2004] C.D. G. Csurka, J. Willamowski, L. Fan et al., ``Visual Categorization with Bags of Keypoints&#39;&#39;, ECCV, 2004. . [Lazebnik06] S. Lazebnik, C. Schmid and J. Ponce, ``Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories&#39;&#39;, CVPR, 2006. . [Chum2007Exemplar] O. {Chum} and A. {Zisserman}, ``An Exemplar Model for Learning Object Classes&#39;&#39;, CVPR, 2007. . [HoG2005] N. {Dalal} and B. {Triggs}, ``Histograms of oriented gradients for human detection&#39;&#39;, CVPR, 2005. . [Superparsing2010] J. Tighe and S. Lazebnik, ``SuperParsing: Scalable Nonparametric Image Parsing with Superpixels&#39;&#39;, ECCV, 2010. . [PhotoTourism2006] Snavely Noah, Seitz Steven M. and Szeliski Richard, ``Photo Tourism: Exploring Photo Collections in 3D&#39;&#39;, ToG, vol. 25, number 3, pp. 835–846, 2006. . [RomeInDay2009] Agarwal Sameer, Furukawa Yasutaka, Snavely Noah et al., ``Building Rome in a day&#39;&#39;, Communications of the ACM, vol. 54, number , pp. 105--112, 2011. . [COLMAP2016] J. Sch &quot;{o}nberger and J. Frahm, ``Structure-From-Motion Revisited&#39;&#39;, CVPR, 2016. . [Se02] Se S., G. D. and Little J., ``Mobile Robot Localization and Mapping with Uncertainty Using Scale-Invariant Visual Landmarks&#39;&#39;, IJRR, vol. 22, number 8, pp. 735--758, 2002. . [PTAM2007] G. {Klein} and D. {Murray}, ``Parallel Tracking and Mapping for Small AR Workspaces&#39;&#39;, IEEE and ACM International Symposium on Mixed and Augmented Reality, 2007. . [Mur15] Mur-Artal R., Montiel J. and Tard{ &#39;o}s J., ``ORB-Slam: A Versatile and Accurate Monocular Slam System&#39;&#39;, IEEE Transactions on Robotics, vol. 31, number 5, pp. 1147--1163, 2015. . [Brown07] Brown M. and Lowe D., ``Automatic Panoramic Image Stitching Using Invariant Features&#39;&#39;, IJCV, vol. 74, number , pp. 59--73, 2007. . [DualBootstrap2003] V. C., Tsai} {Chia-Ling and {Roysam} B., ``The dual-bootstrap iterative closest point algorithm with application to retinal image registration&#39;&#39;, IEEE Transactions on Medical Imaging, vol. 22, number 11, pp. 1379-1394, 2003. . [AlexNet2012] Alex Krizhevsky, Ilya Sutskever and Geoffrey E., ``ImageNet Classification with Deep Convolutional Neural Networks&#39;&#39;, 2012. . [Astounding2014] A. S., H. {Azizpour}, J. {Sullivan} et al., ``CNN Features Off-the-Shelf: An Astounding Baseline for Recognition&#39;&#39;, CVPRW, 2014. . [Melekhov2017relativePoseCnn] I. Melekhov, J. Ylioinas, J. Kannala et al., ``Relative Camera Pose Estimation Using Convolutional Neural Networks&#39;&#39;, , 2017. online . [PoseNet2015] A. Kendall, M. Grimes and R. Cipolla, ``PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization&#39;&#39;, ICCV, 2015. . [sattler2019understanding] T. Sattler, Q. Zhou, M. Pollefeys et al., ``Understanding the limitations of cnn-based absolute camera pose regression&#39;&#39;, CVPR, 2019. . [zhou2019learn] Q. Zhou, T. Sattler, M. Pollefeys et al., ``To Learn or Not to Learn: Visual Localization from Essential Matrices&#39;&#39;, ICRA, 2020. . [pion2020benchmarking] !! This reference was not found in biblio.bib !! . [Tatarchenko2019] M. Tatarchenko, S.R. Richter, R. Ranftl et al., ``What Do Single-View 3D Reconstruction Networks Learn?&#39;&#39;, CVPR, 2019. . [STN2015] M. Jaderberg, K. Simonyan and A. Zisserman, ``Spatial transformer networks&#39;&#39;, NeurIPS, 2015. . [NALU2018] A. Trask, F. Hill, S.E. Reed et al., ``Neural arithmetic logic units&#39;&#39;, NeurIPS, 2018. . [NAU2020] A. Madsen and A. Rosenberg, ``Neural Arithmetic Units&#39;&#39;, ICLR, 2020. . [GroupEqCNN2016] T. Cohen and M. Welling, ``Group equivariant convolutional networks&#39;&#39;, ICML, 2016. . [MakeCNNShiftInvariant2019] R. Zhang, ``Making convolutional networks shift-invariant again&#39;&#39;, ICML, 2019. . [AbsPositionCNN2020] M. Amirul, S. Jia and N. D., ``How Much Position Information Do Convolutional Neural Networks Encode?&#39;&#39;, ICLR, 2020. . [AdvPatch2017] T. Brown, D. Mane, A. Roy et al., ``Adversarial patch&#39;&#39;, NeurIPSW, 2017. . [OnePixelAttack2019] Su Jiawei, Vargas Danilo Vasconcellos and Sakurai Kouichi, ``One pixel attack for fooling deep neural networks&#39;&#39;, IEEE Transactions on Evolutionary Computation, vol. 23, number 5, pp. 828--841, 2019. . [cv4action2019] Zhou Brady, Kr{ &quot;a}henb{ &quot;u}hl Philipp and Koltun Vladlen, ``Does computer vision matter for action?&#39;&#39;, Science Robotics, vol. 4, number 30, pp. , 2019. . [HardNet2017] A. Mishchuk, D. Mishkin, F. Radenovic et al., ``Working Hard to Know Your Neighbor&#39;s Margins: Local Descriptor Learning Loss&#39;&#39;, NeurIPS, 2017. . [KeyNet2019] A. Barroso-Laguna, E. Riba, D. Ponsa et al., ``Key.Net: Keypoint Detection by Handcrafted and Learned CNN Filters&#39;&#39;, ICCV, 2019. . [SuperPoint2017] Detone D., Malisiewicz T. and Rabinovich A., ``Superpoint: Self-Supervised Interest Point Detection and Description&#39;&#39;, CVPRW Deep Learning for Visual SLAM, vol. , number , pp. , 2018. . [R2D22019] J. Revaud, ``R2D2: Repeatable and Reliable Detector and Descriptor&#39;&#39;, NeurIPS, 2019. . [D2Net2019] M. Dusmanu, I. Rocco, T. Pajdla et al., ``D2-Net: A Trainable CNN for Joint Detection and Description of Local Features&#39;&#39;, CVPR, 2019. . [sarlin2019superglue] P. Sarlin, D. DeTone, T. Malisiewicz et al., ``SuperGlue: Learning Feature Matching with Graph Neural Networks&#39;&#39;, CVPR, 2020. . [gradslam2020] J. Krishna Murthy, G. Iyer and L. Paull, ``gradSLAM: Dense SLAM meets Automatic Differentiation &#39;&#39;, ICRA, 2020 . . [CapsNet2011] G.E. Hinton, A. Krizhevsky and S.D. Wang, ``Transforming auto-encoders&#39;&#39;, ICANN, 2011. . [CapsNet2017] S. Sabour, N. Frosst and G.E. Hinton, ``Dynamic routing between capsules&#39;&#39;, NeurIPS, 2017. . [li2020extreme] Li Jianguo, Sun Mingjie and Zhang Changshui, ``Extreme Values are Accurate and Robust in Deep Networks&#39;&#39;, , vol. , number , pp. , 2020. online .",
            "url": "/wide-baseline-stereo-blog/2020/03/27/intro.html",
            "relUrl": "/2020/03/27/intro.html",
            "date": " • Mar 27, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi! My name is Dmytro Mishkin, I am computer vision researcher and consultant. This is blog series based on my on-going PhD thesis. My personal website is here. The best way to contact me is email and Twitter. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "/wide-baseline-stereo-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "/wide-baseline-stereo-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}