{
  
    
        "post0": {
            "title": "Benchmarking Image Retrieval for Visual Localization",
            "content": "I would like to share my thoughts on 3DV 2020 paper &quot;Benchmarking Image Retrieval for Visual Localization&quot; by Pion et.al. . . What is the paper about? . How one would approach visual localization? The most viable way to do it is hierarchical approach, similar to image retrieval with spatial verification. . You get the query image, retrieve the most similar images to it from some database by some efficient method, e.g. global descriptor search. Then given the top-k images you estimate the pose of the query image by doing two-view matching, or some other method. . The question is -- how much influence the quality of the image retrieval has? Should you spend more or less time improving it? That is the questions, paper trying to answer. . Authors design 3 re-localization systems. . &quot;Task 1&quot; system estimates the query image pose as average of the short-list images pose, weighted by the similarity to the query image. . | &quot;Task 2a&quot; system performs pairwise two-view matching between short-list images and query triangulates the query image pose using 3d map built from the successully matches images. . | &quot;Task 2b&quot; pre-builds the 3D map from the database images offline. At the inference time, local feature 2d-3d matching is done on the shortlist images. . | What is benchmarked? . The paper compares . SIFT-based DenseVLAD | . and CNN-based . NetVLAD | APGeM | DELG | . What is important (and adequately mentioned in the paper, although I would prefer the disclamer in the each figure) is that all CNN-based methods have different architectures AND training data. Basically, the paper uses author-released models. Thus one cannot say if APGeM is better or worse than NetVLAD as method, because they were trained on the very different data. However, I also understand that one cannot easily afford to re-implement and re-train everything. . As the sanity check paper provides the results on the Revisited Oxford and Paris image retrieval benchmark. . Summary of results . Paper contains a lot of information and I definitely recommend you to read it. Nevertheless, let me try to summarize paper messages and then my take on it. . For the task1 (similarity-weighted pose) there is no clear winner. (SIFT)-DenseVLAD works the best for the daytime datasets. Probably DenseVLAD is good because it is not invariant and if it can match images, they are really close -&gt; high pose accuracy. For the night both DeLG and AP-GeM are good. As paper guesses, that it because they are only ones, which were trained on night images as well. | There is almost no difference between CNN-based methods for the task2a and task2b (retrieval -&gt; local features matching). This indicates that the limit is the mostly in the number of images and local features. | . . My take-away messages . Image Relocalization seems to be is more real-world and engineering task, than image retrieval. . And that it why it actually ALREADY WORKS, because if there some weak spot, it is compensated by the system design. Thhe same conclusion from our IMC paper, experiment with ground truth -- if you have 1k images for the 3d model, you can use as bad features, as you want. The COLMAP will recover anyway . . The retrieval, on the other hand is more interesting to work on, because it is kind of deliberately hard and you can do some fancy stuff, which do not matter in the real world. . Task1 (global descriptor-only) system are quite useless now . Unless we are speaking about the quite dense image representation. I mean, top-accuracy is 35% vs almost 100% for those, which include local features. . Good news: it has a LOT of space for the improvement to work on. . For the task 2a and 2b, robust global descriptors are a way to do the retrieval, sorry VLAD. . The precision will come from the local features. Which I like a lot, because VLAD is more complex to train and initalize, I never liked it (nothing personal). . For the task2a and 2b we need new metrics, e.g. precisition @ X Mb memory footprint . Because otherwise, the task is easily solved by the brute force -- either by photo taking, or, at least with image syntesis, see 24/7 place recognition by view synthesis. . Such steps are already taken in the paper Learning and aggregating deep local descriptors for instance-level recognition -- see the table with memory footprint. . . That is how one could have an interesting research challenge, also having some grounds in the real-world -- to work in mobile phones. Otherwise, any method would work, if the database is dense enough. . Robust local features matter for illumination changes . It is a bit hidden in the Appendix, so go directly to the Figure 9. It clearly shows that localization performance is bounded by SIFT, if it is used for two view matching, making retrieval improvements irrelevant. When R2D2 or D2Net are used for matching instead, the overall results for night-time are much better. . . That is in line with my small visual benchmark I did recently. . https://twitter.com/ducha_aiki/status/1330495426865344515 . . That&#39;s all, folks! Now please, check the paper and the code they provided. .",
            "url": "/wide-baseline-stereo-blog/2020/11/25/review-of-retrieval-for-localization.html",
            "relUrl": "/2020/11/25/review-of-retrieval-for-localization.html",
            "date": " • Nov 25, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Revisiting Brown patch dataset and benchmark",
            "content": "In this post . Why one needs good development set? What is wrong with existing sets for local patch descriptor learning? | One should validate in the same way, as it is used in production. | Brown patch revisited -- implementation details | Local patch descriptors evaluation results. | Really quick intro into local patch descriptors . Local patch descriptor is the thing, which helps you to automatically decide, if two patches in the pair of images correspond to the same point in a real world, or not. It should be robust to illumination, viewpoint and other changes. . . There are lots of ways how to implement a local patch descriptor: engineered and learned. Local patch descriptor is the crucial component of the wide baseline stereo pipeline and a popular computer vision research topic. . Why do you need development set? . Good data is crucial for any machine learning problem -- everyone now knows that. One needs high quality training set for training a good model. One also needs good test set, to know, what is real performance. However, there is one more, often forgotten, crucial component -- validation or development set. We use it to decide hyperparameters and validate design choices we make. It should be different from both training and test sets, yet, be good predictor of test set performance. Moreover, it should allow fast iterations, so be not too small. . While such set is commonly called validation set, I do like Andrew Ng&#39;s term &quot;development&quot; set more - because it helps to develop your model. . Existing datasets for local patch descriptors . So, what are the development set options for local patch descriptors? . Brown PhotoTourism. . The most commonly and successfully used dataset for local descriptor learning is PhotoTourism, created in 2008. Here is its description by authors: . The dataset consists of corresponding patches sampled from 3D reconstructions of the Statue of Liberty (New York), Notre Dame (Paris) and Half Dome (Yosemite). . It also comes with evaluation protocol:patch pairs are labeled as &quot;same&quot; or &quot;different&quot; and the false positive rate at recall of 95% (FPR95) is reported. The variable, used to build ROC curve is descriptor distance between two patches. Advantages: . It contains local patches, extracted for two types of local feature detector -- DoG (SIFT) and Harris corners. | It is extracted from images, contraining non-planar structures and the geometrical noise present is caused by the local feature detector, not added artificially. | Descriptors, trained on the dataset, show very good performance [IMW2020], therefore the data itself is good. | . Disadvantages: . when used as a benchmark, it shows unrealistic results: SIFT is 40x worse than deep learned descriptor. In practice, the difference is much smaller. | . HPatches . HPatches, where H stands for the &quot;homography&quot; was proposed to overcome the problem of unrealisting metric and, seemingly, too easy data, used in Phototourism dataset. . It was constructed in a different way than a Phototourism. First, local features were detected in the &quot;reference&quot; image and then reprojected to other images in sequences. Reprojection is prossible, because all the images are photographies of the planes -- graffity, drawing, print, etc, or are all taken from the same position. After the reprojection, some amount of geometrical noise -- rotation, translation, scaling, was added to the local features and the patches were extracted. . This process is illustration on the picture below (both taken from the HPatches website). . . HPAtches also provide 3 testing protocol, evaluating mean average precision (mAP) for 3 different tasks: patch verification -- similar to Brown Phototourism, image matching and patch retrieval. The variable, used to build mAP is descriptor distance between two patches. . Advantages: . Unlike PhotoTourism patch verification, image matching and patch retrieval tasks are not saturated. | HPatches contains illumination split, allowing the evaluation of descriptor robustness to illumination changes. | . Disadvantages: . patches &quot;misregistration&quot; noise is of artificial nature, although paper claims that it has similar statistics | no non-planar structure | performance in HPaptches does not really correlate with the downstream performance [IMW2020] | . . AMOSPatches . AMOS patches is &quot;HPatches illumination on steroids, without geometrical noise&quot;. It has the same advantanges and disadvantages, as HPatches and is mostly focused on illumination and weather changes. . . PhotoSynth . PhotoSynth can be described and something in the middle between Phototour and HPatches. It contains patches, sampled from planar scenes, as well as from non-planar scenes. . At first glance, it should be great for the test and training purposes. However, there are several issues with it. First, pre-trained HardNetPS descriptor, released together with the dataset, works well on HPatches, but poor in practice[pultar2020improving]. . Second, a couple of colleagues has tried to train the descriptor on top of it, as it was significantly worse than the authors reference model. Moreover, there is no testing/training code protocol available together with dataset. . So, while PhotoSynth might be a good dataset in principle, it definitely needs more love and work. . . . Designing the evaluation protocol . Classical local descriptor matching consists of two parts: finding nearest neighbors and filtering unreliable ones based on some criterion. I have wrote a blogpost, describing the matching strategies in details. . The most used in practice criterion is the first to second nearest neighbor distance (Lowe&#39;s) ratio threshold for filtering false positive matches. It is shown in the figure below. . The intuition is simple: if two candidates are too similar, then the match is unreliable and it is better to drop it. . . Somehow, none of the local patch evaluation protocols does not take such filtering criterion in mind, although it greatly influences the overall performance. . So, let&#39;s do the following: . Take the patches, which are extracted from only two images. | For the each patch, calculate the descriptor distance to the correct match and to the hardnest (closest) non-match. Calculate the Lowe&#39;s ratio between this two. | Calculate accuracy for each of such triplets. If the correct match has smaller distance, score 1, if not - 0. | Sort the ratios from smallest to biggest and calculate mean average precision#Mean_average_precision) (mAP). | Brown PhotoTour Revisied: implementation details . We have designed the protocol, now time for data. We could spend several month collecting and cleaning it...or we can just re-use great Brown PhotoTourism dataset. Re-visiting labeling and/or evaluation protocol of the time-tested dataset is a great idea. . Just couple of examples: ImageNette created by Jeremy Howard from ImageNet, Revisited Oxford 5k by Filip Radenovic and so on. . For the protocol we designed above we need the information about the image id, where the patch was extracted from. Unfortunately, there is no such information in the Brown PhotoTourism, but there is suitable alternative -- the image id, where the reference patch was detected. What does it mean? . Suppose, we have 4 images and 5 keypoints. All the keypoints present in all images, which gives us 20 patches. 3 keypoints were first detected in Image 1 and 2 in image 2. That means that we will have 12 patches labeled image 1 and 8 patches labeled image 2. . So, we will have results for image 1 and image 2. Let&#39;s consider image 1. There are 12 patches, splitted in 3 &quot;classes&quot;, 4 patches in each class. . Then, for the each of those 12 patches we: . pick each of the corresponding patched as positives, so 3 positives. $P_1$, $P_2$, $P_3$ | find the closest negative N. | add triplets (A, $P_1$, N), (A, $P_2$, N), (A, $P_3$, N) to the evaluation. | . Repeat the same for the image 2. That mimics the two-view matching process as close, as possible, given the data available to us. . Installation . pip install brown_phototour_revisited . How to use . There is a single function, which does everything for you: full_evaluation. The original Brown benchmark consider evaluation, similar to cross-validation: train descriptor on one subset, evaluate on two others, repeat for all, so 6 evaluations are required. For the handcrafted descriptors, or those, that are trained on 3rd party datasets, only 3 evaluations are necessary. We are following it here as well. . However, if you need to run some tests separately, or reuse some functions -- we will cover the usage below. In the following example we will show how to use full_evaluation to evaluate SIFT descriptor as implemented in kornia. . # !pip install kornia . import torch import kornia from IPython.display import clear_output from brown_phototour_revisited.benchmarking import * patch_size = 65 model = kornia.feature.SIFTDescriptor(patch_size, rootsift=True).eval() descs_out_dir = &#39;data/descriptors&#39; download_dataset_to = &#39;data/dataset&#39; results_dir = &#39;data/mAP&#39; results_dict = {} results_dict[&#39;Kornia RootSIFT&#39;] = full_evaluation(model, &#39;Kornia RootSIFT&#39;, path_to_save_dataset = download_dataset_to, path_to_save_descriptors = descs_out_dir, path_to_save_mAP = results_dir, patch_size = patch_size, device = torch.device(&#39;cuda:0&#39;), distance=&#39;euclidean&#39;, backend=&#39;pytorch-cuda&#39;) clear_output() print_results_table(results_dict) . Mean Average Precision wrt Lowe SNN ratio criterion on UBC Phototour Revisited trained on liberty notredame liberty yosemite notredame yosemite tested on yosemite notredame liberty Kornia RootSIFT 56.70 47.71 48.09 . Results . So, let&#39;s check how it goes. The latest results and implementation are in the following notebooks: . Deep descriptors | Non-deep descriptors | . The results are the following: . Mean Average Precision wrt Lowe SNN ratio criterion on UBC Phototour Revisited trained on liberty notredame liberty yosemite notredame yosemite tested on yosemite notredame liberty Kornia RootSIFT 32px 58.24 49.07 49.65 HardNet 32px 70.64 70.31 61.93 59.56 63.06 61.64 SOSNet 32px 70.03 70.19 62.09 59.68 63.16 61.65 TFeat 32px 65.45 65.77 54.99 54.69 56.55 56.24 SoftMargin 32px 69.29 69.20 61.82 58.61 62.37 60.63 HardNetPS 32px 55.56 49.70 49.12 R2D2_center_grayscal 61.47 53.18 54.98 R2D2_MeanCenter_gray 62.73 54.10 56.17 Mean Average Precision wrt Lowe SNN ratio criterion on UBC Phototour Revisited trained on liberty notredame liberty yosemite notredame yosemite tested on yosemite notredame liberty Kornia SIFT 32px 58.47 47.76 48.70 OpenCV_SIFT 32px 53.16 45.93 46.00 Kornia RootSIFT 32px 58.24 49.07 49.65 OpenCV_RootSIFT 32px 53.50 47.16 47.37 OpenCV_LATCH 65px -- -- -- 37.26 -- 39.08 OpenCV_LUCID 32px 20.37 23.08 27.24 skimage_BRIEF 65px 52.68 44.82 46.56 Kornia RootSIFTPCA 3 60.73 60.64 50.80 50.24 52.46 52.02 MKD-concat-lw-32 32p 72.27 71.95 60.88 58.78 60.68 59.10 . So far - in agreement with IMC benchmark: SIFT and RootSIFT are good, but not the best, SOSNet and HardNet are the leaders, but within tens of percents, not by orders of magnitude. . . Disclaimer 1: don&#39;t trust this tables fully . I haven&#39;t (yet!) checked if all the deep descriptors models, trained on Brown, were trained with flip-rotation 90 degrees augmentation. In the code below I assume that they were, however, it might not be true -- and the comparison might not be completely fair. I will do my best to check it, but if you know that I have used wrong weights - please open an issue. Thank you. . Disclaimer 2: it is not &quot;benchmark&quot;. . The intended usage of the package is not to test and report the numbers in the paper. Instead think about is as cross-validation tool, helping the development. Thus, one CAN tune hyperparameters based on the benchmark results instead of doing so on HPatches. After you have finished tuning, please, evaluate your local descriptors on some downstream task like IMC image matching benchmark or visual localization. . Summary . It really pays off, to spend time designing a proper evaluation pipeline and gathering the data for it. If you can re-use existing work - great. But don&#39;t blindly trust anything, even super-popular and widely adopted benchmarks. You need always check if the the protocol and data makes sense for your use-case personally. . Thanks for the reading, see you soon! . Citation . If you use the benchmark/development set in an academic work, please cite it. . @misc{BrownRevisited2020, title={UBC PhotoTour Revisied}, author={Mishkin, Dmytro}, year={2020}, url = {https://github.com/ducha-aiki/brown_phototour_revisited} } . References . [IMW2020] Jin Yuhe, Mishkin Dmytro, Mishchuk Anastasiia et al., ``Image Matching across Wide Baselines: From Paper to Practice&#39;&#39;, arXiv preprint arXiv:2003.01587, vol. , number , pp. , 2020. . [pultar2020improving] Pultar Milan, ``Improving the HardNet Descriptor&#39;&#39;, arXiv ePrint:2007.09699, vol. , number , pp. , 2020. .",
            "url": "/wide-baseline-stereo-blog/2020/09/23/local-descriptors-validation.html",
            "relUrl": "/2020/09/23/local-descriptors-validation.html",
            "date": " • Sep 23, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "How to match images taken from really extreme viewpoints?",
            "content": "In this post . What to do, if you are in a desperate need of matching this particular image pair? | What are the limitations of the affine-covariant detectors like Hessian-Affine or HesAffNet? | ASIFT: brute-force affine view synthesis | Do as little as possible: MODS | What is the key factor of affine view synthesis? Ablation study | How to match images taken from really extreme viewpoints? . Standard wide-baseline stereo or 3d reconstruction pipelines work well in the many situations. Even if some image pair is not matched, it is usually not a problem. For example, one could match images from very different viewpoints, if there is a sequence of images in between, as shown in Figure below, from &quot;From Single Image Query to Detailed 3D Reconstruction&quot; paper[SingleImage3dRec2015]. . . However, that might not always be possible. For example, the number of pictures is limited because they historical and there is no way how one could go and take more without inventing a time machine. . What to do? One way would be to use affine features like Hessian-AffNet[AffNet2018] or MSER[MSER2002]. However, they help only up to some extent and what if the view, we need to match are more extreme? . . The image pair above is from &quot;Location recognition over large time lags dataset&quot; paper [LostInPast2015]. . The solution is to simulate real viewpoint change by affine or perspective warps of the current image. This idea was first proposed by Lepetit and Fua in 2006[AffineTree2006]. You can think about it as a special version of test-time augmentation, popular nowadays in deep learning. Later affine view synthesis for wide baseline stereo was extended and mathematically justified by Morel &amp; Yu in ASIFT paper[ASIFT2009]. They proved that perspective image warps are can be approximated by synthetic affine views. . What is wrong with affine-covariant local detectors? . One could say that the goal of affine-covariant detectors like MSER, Hessian-Affine or Hessian-AffNet is to detect the same region on a planar surface, regardless the camera angle change. It is true to some extent, as we demostrate on toy example below with Hessian-Affine feature. . . The problem arises, when the image content, e.g. 3 blobs on the figure below are situated close to each other, so under the tilt transform the merge into a single blob. So it is not the shape of region, which is detected incorrectly, but the center of the features themselves. For clarity, we omited affine shape estimation on the image below. . . ASIFT: brute-force affine view synthesis . So, to solve the problem explained above, Morel &amp; Yu [ASIFT2009] proposed to do a lot affine warps of each image, as shown on the Figure below, as match each view against all others, which is $O(n^2)$ complexity, where $n$ is number of views generated. . . The motivation do doing so it that assuming, original image to be a fronto-parallel one, to cover viewsphere really dense, as shown in the Figure below. . . This leads to impressive performance on a very challenging image pairs, see an example below . . In this section I have used great illustrations done by Mariano Rodríguez for his paper &quot;Fast Affine Invariant Image Matching&quot; [FastASIFT2018]. Please, checkout his blog. . MODS: do as little as possible . The main drawback of ASIFT algorithm is a huge computational cost: 82 views are generated regardless of the image pair difficulty. To overcome this, we proposed MODS[MODS2015] algorithm: Matching with On-Demand Synthesis. . . One starts with the fastest detector-descriptor without view synthesys and then uses more and more computationally expensive methods if needed. Moreover, by using affine-covariant detectors like MSER or Hessian-Affine, one could synthetise significantly less views, saving computations spent on local descriptor and matching. . This, together with FGINN matching strategy, specifically designed for the handling re-detections, MODS is able to match more challenging image pairs in less time than ASIFT. . . Why does affine synthesis help? . Despite that ASIFT and other view-synthesis based approaches are know more than decade, we are not aware of a study, why does affine synthesis helps in practice. Could one get a similar performance without view synthesis? Specificallly: . May it be that the most of improvements come from the fact that we have much more features? That is why we fix the number of features for all approaches. | Some regions from ASIFT, when reprojected to the original image, are quite narrow. Could we get them just by removing edge-like feature filtering, which is done in SIFT, Hessian and other detectors. Denoted +edge | Instead of doing affine view synthesis, one could directly use the same affine parameters to get the affine regions to describe, so the each keypoint would have several associated regions+descriptors. Denoted +MD | Using AffNet to directly estimated local affine shape without multiple descriptors. Denoted +AffNet | Combine (1), (2) and (3). | So, we did the study on HPatches Sequences dataset, the hardest image pairs (1-6) of viewpoint subset. The metric is similar to one used in the &quot;Image Matching across Wide Baselines: From Paper to Practice&quot; and CVPR 2020 RANSAC in 2020 - mean average accuracy of the estimated homography. . . We run Hessian detector with RootSIFT descriptor, FLANN matching and LO-RANSAC, as implemented in MODS. Features are sorted according the the detector response and their total number is clipped to 2048 or 8000 to ensure that the improvements do not come from just having more features. . Note, that we do not study, if view synthesis helps for the regular image pairs - it might actually hurt performance, similarly to affine features. Instead we are focusing on the case, when view synthesis definitely helps: matching obscure views of the mostly planar scenes. . 8000 feature budget . Results are in Figure below. Indeed, all of the factors: detecting more edge-like features, having multiple descriptors or better affine shape improve results over the plain Hessian detector, but even all of the combined are not good enough to match performance of the affine view synthesis + plain Hessian detector. . But the best setup is to use both Hessian-AffNet and view synthesis. . . 2048 feature budget . The picture is a bit different in a small feature budget: neither multiple-(affine)-descriptors per keypoint, nor allowing edge-like feature help. From other hand, affine view synthesis still improves results of the Hessian. And, again, the best performance is achieved with combination of view synthesis and AffNet shape estimation. . . Summary . Affine view synthesis helps for matching challenging image pairs and its improvement are not just because of more local features used. It can be done effective and efficient -- in the iterative MODS framework. . References . [SingleImage3dRec2015] J.L. Schonberger, F. Radenovic, O. Chum et al., ``From Single Image Query to Detailed 3D Reconstruction&#39;&#39;, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015. . [AffNet2018] D. Mishkin, F. Radenovic and J. Matas, ``Repeatability is Not Enough: Learning Affine Regions via Discriminability&#39;&#39;, ECCV, 2018. . [MSER2002] J. Matas, O. Chum, M. Urban et al., ``Robust Wide Baseline Stereo from Maximally Stable Extrema Regions&#39;&#39;, BMVC, 2002. . [LostInPast2015] Fernando Basura, Tommasi Tatiana and Tuytelaars Tinne, ``Location recognition over large time lags&#39;&#39;, Computer Vision and Image Understanding, vol. 139, number , pp. , 2015. . [AffineTree2006] Lepetit Vincent and Fua Pascal, ``Keypoint Recognition Using Randomized Trees&#39;&#39;, IEEE Trans. Pattern Anal. Mach. Intell., vol. 28, number 9, pp. , sep 2006. . [ASIFT2009] Morel Jean-Michel and Yu Guoshen, ``ASIFT: A New Framework for Fully Affine Invariant Image Comparison&#39;&#39;, SIAM J. Img. Sci., vol. 2, number 2, pp. , apr 2009. . [FastASIFT2018] Rodríguez Mariano, Delon Julie and Morel Jean-Michel, ``Fast Affine Invariant Image Matching&#39;&#39;, Image Processing On Line, vol. 8, number , pp. , 2018. . [MODS2015] Mishkin Dmytro, Matas Jiri and Perdoch Michal, ``MODS: Fast and robust method for two-view matching &#39;&#39;, Computer Vision and Image Understanding , vol. , number , pp. , 2015. .",
            "url": "/wide-baseline-stereo-blog/2020/08/06/affine-view-synthesis.html",
            "relUrl": "/2020/08/06/affine-view-synthesis.html",
            "date": " • Aug 6, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Patch extraction: devil in details",
            "content": "When working with local features one needs to pay attention to even a smallest details, or the whole process can be ruined. One of such details is how to extract the patch, which will be described by local descriptor such as SIFT or HardNet. . . Unfortunately, we cannot just extract patch from the image by cropping the patch and then resizing it. Or can we? Let&#39;s check. We will use two versions of image: original and 4x smaller one and would like to extract same-looking fixed size patch from both of them. The patch we want to crop is showed by oriented red circle. . . Aliasing . And here what we get by doing a simple crop and resize to 32x32 pixels. . . Doesn&#39;t look good. It is called &quot;aliasing&quot; - a problem, which arise when we are trying to downscale big images into small resolution. Specifically: the original image contains finer details, than we could represent in thumbnail, which leads to artifacts. . The solution: anti-aliasing . The solution, which follows out of sampling theorem is known: remove the details, which cannot be seens in small image first, then resample image to small size. . The simplest way to remove the fine details is to blur image with the Gaussian kernel. . Lets do it and compare the results. . By the way, you can try for yourself, all the required code is here, in kornia-examples . Performance . The problem is solved. Or is it? . The problem with properly antialiased patch extraction is that it is quite slow for two reasons. First, blurring a whole image is a costly operation. But, the worst part is that the required amount of blur depends on the patch size in the original image, or, in other words, keypoint scale. So for extracting, say 8000 patches, one needs to perform blurring 8000 times. Moreover, if one wants to extract elongated region and warp it to the square patch, the amount of blur in vertical and horizontal directions should be different! . What can be done? Well, instead of doing blurring 8000 times, one could create so called scale pyramid and then pick the level, which is the closest to optimal one, predicted by theorem. . . This is exactly, what kornia function extract_patches_from_pyramid does. . Also - I have a bit cheated with you above: the &quot;anti-aliased&quot; patches were actually extracted using the function above. . How it impacts local descriptor matching? . Let&#39;s do the toy example first - describe four patches we have in the example above with HardNet descriptor and calculate the distance between them. . . So the descriptor difference between antialiased patches is 0.09 and between naively extracted -- 0.44. 0.09 is not a big deal, but 0.44 is a lot, actually. . Let&#39;s move to the non-toy example from the paper devoted to this topic: &quot;A Few Things One Should Know About Feature Extraction, Description and Matching&quot;[PatchExtraction2014]. . The original data is lost I am too lazy to redo the experiments for the post, so I will just copy-past images with results. Here are abbrevations used in the paper: . OPE -- Optimal Patch Extraction. The most correct and slow way of extracting, including different amount of bluring in different directions. . | NBPE -- No-Blur Patch Extraction. The most naive way we started with . | PNBPE -- Pyramid, No-Blur Patch Extraction. The one, we described above - sampling patches from scale pyramid. . | PSPE Pyramid-Smoothing Patch Extraction. Pick the matching pyramid level and then add anisotropic blur missing. . | . As you can see, doing things optimally is quite slow. . Now let&#39;s see how it influences performance. . . It looks like that influence is smaller than we thought. But recall that the experiment above is for SIFT descriptor only. Doing pyramid helps for the small viewpoint change almost as good, as going fully optimal, but with increasing the viewpoint difference, such approximation degrades. Moreover, it influnces MSER detector much more that than Hessian-Affine. . How does it work with deep descriptors like HardNet or SoSNet? That is the question which not answered yet. Drop me a message if you want to do it yourself and we can do the follow-up post together. . References . [PatchExtraction2014] K. Lenc, J. Matas and D. Mishkin, ``A few things one should know about feature extraction, description and matching&#39;&#39;, Proceedings of the Computer Vision Winter Workshop, 2014. .",
            "url": "/wide-baseline-stereo-blog/2020/07/22/patch-extraction.html",
            "relUrl": "/2020/07/22/patch-extraction.html",
            "date": " • Jul 22, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Local affine features: useful side product",
            "content": "Keypoints are not just points . Wide baseline stereo matching often as perceived as establishing (key-)point correspondences between images. While this might be true for the some local features like SuperPoint [SuperPoint2017], typically it is more than that. . Specifically, detectors like DoG[Lowe99], Harris[Harris88], Hessian[Hessian78], KeyNet[KeyNet2019], ORB[ORB2011], and many others rate on scale-space provide at least 3 parameters: x, y, and scale. . Most of the local descriptors -- SIFT[Lowe99], HardNet[HardNet2017] and so on -- are not rotation invariant and those which are - mostly require complex matching function[RIFT2005], [sGLOH2], so the patch orientation has to be estimated anyway, in order to match reliably. This can be done by various methods: corners center of mass (ORB[ORB2011], dominant gradient orientation (SIFT)[Lowe99] or by some learned estimator (OriNets[OriNet2016],[AffNet2018]). Sometimes it is possible to rely on smartphone camera IMU or photographer and assume that images are upright[PerdochRetrieval2009]. . Thus, we can assume that if local descriptors match, this means the local feature scale and orientation also match, at least approximately -- see Figure below. Possible exceptions are cases, when the patch is symmetrical and orientation is ambiguous up to some symmetry group. . . In addition, one could assume that we observe the patch not from the fronto-parallel position and try to estimate local normal, or, more precisely, affine shape of the feature point, modeling it as an ellipse instead of circle. One could also think of affine shape estimation as finding the camera position, from where the patch is seen in some &quot;canonical&quot; view. . . This gives us 3 points correspondences from a single local feature match, see an example in Figure below. . . Why is it important and how to use it -- see in current post. How to esimate local affine features robustly -- in the next post. . Benefits of local affine features . Making descriptor job easier . The most straightforward benefit of using local affine features is that they increase the repeatability of the detector and potentially reduce appearance changes of a local patch caused by viewpoint difference. This makes possible matching more challenging image pairs. . . . The practice is a little bit more complicated. Our recent benchmark[IMW2020], which measure the accuracy of the output fundamental matrix, shows that the difference in using affine and similarity-covariant features is quite minor. Specifically, the relative difference between SIFT vs SIFT-Affine features is 0.5% and between Hessian-SIFT and Hessian-AffNet SIFT is 5.1%, see Table below. . . Therefore, if the benefit of local features would be to only improve descriptor extraction stage, it would be arguably not worth it. Luckily, there are more benefits, which are more pronounced. . Making RANSAC job easier . Let&#39;s recall how RANSAC works. . Randomly sample a minimally required number of tentative correspondences to fit the geometrical model of the scene: 4 for homography, 7 for epipolar geometry and estimate the model. | Calculate &quot;support&quot;: other correspondeces, which are consistent with the model. | Repeat steps (1), (2) and output the model which is supported with the most of correspondences. If you were lucky and have sampled all-inlier sample, meaning that all correspondences used to estimate the model were correct, you would have a correct model. | Reality is more complicated than I have just described, but the principle is the same. The most important part is the sampling and it is sensitive to inlier ratio $ nu$ - the percentage of the correct correspondences in the set. Lets denote the minimal number of correspondences required to estimate the model as m. To recover the correct model with the confidence p one needs to sample the number of correspondences, which is described by formula: . begin{equation} N = frac{ log{(1 - p)}}{ log{(1 - nu^{m})}} end{equation}Lets plot the how the number of required samples changes with inlier ratio for confidence equal 99%. Note the log scale on Y axis. Different lines are for different minimal sample size m. . . As you can see from the plot above, reducing the minimal sample size required for the model estimation even by 1 saves and order of magnitude of computation. In reality the benefit is a smaller, as modern RANSACs like GC-RANSAC[gcransac2018] and MAGSAC[magsac2019] could estimate the correct model from the sample containing outliers, but it is still huge, especially for low inlier rate cases. . Image retrieval . The ideal case would be to estimate a model from just a single sample and that is exactly what is done in spatial reranking paper &quot;Object retrieval with large vocabularies and fast spatial matching&quot; by Philbin et.al [Philbin07]. . Specifically, they are solving a particular object retrieval problem: given an image containing some object, return all the images from the database, which also containg the same object. . The inital list of images is formed by the descriptor distance and then is reranked. The authors propose to approximate a perspective change between two images as an affine image transformation, and count number of feature points, which are reprojected inside the second image. This number produces better ranking that the original short-list. . . Back to wide baseline stereo . While working for spatial re-ranking, 3-degrees of freedom camera model is too rough for the wide baseline stereo. Yet, going from 4 point correspondences (PC) to 2 affine correspondences (AC) for homogaphy and from 7 PC to 3 AC for the fundamental matrix would be huge benefit anyway for the robust model estimation. . Various variant of RANSAC working for local features were proposed in the last 15 years: Perdoch et.al[perd2006epipolar], Pritts et.al.[PrittsRANSAC2013], Barath and Kukelova [Barath2019ICCV], Rodríguez et.al[RANSACAffine2020]. . Finally, the systematic study of using is presented by Barath et.al[barath2020making] in &quot;Making Affine Correspondences Work in Camera Geometry Computation&quot; paper. Authors show that if used naively, affine correspondence lead to worse results, because they are more noisy than point correspondences. However, there is a bag of tricks presented in the paper, which allow to solve the noise issue and make the affine RANSAC working in practice, resulting in orders of magnitude faster computation. . . Moreover, for a special cases like autonomous driving, where the motion is mostly horizonal, one could even use 2 affine correspondes for both motion estimation and consistency check, significantly improving the efficiency of the outliers removal compared to the standard RANSAC loop[guan2020relative]. . . Besides the special case considerations, additional contraints can also come from running other algorithms, like monocular depth estimation. Such a constraint could reduce the required number of matches from two affine correspondences to a single one for calibrated camera case[OneACMonoDepth2020]. . . . Application-specific benefits . Besides the wide baseline stereo, local affine features and correspondences have other applications. I will briefly describe some of them here (to be updated). . Image rectification . Instead of matching local features between two images one might match them within a single image. Why would someone do it? This allows finding repeated pattern: think about windows, doors and so on. Typically they have the same physical size, therefore the diffrence in local features around them could tell us about the geometry of the scene and lens distortion. . . This is the idea of the series of works by Pritts and Chum. . . Surface normals estimation . Ivan Eichhardt and Levente Hajder have a series of works, exploiting the local affine correspondences for surface normals estimation[SurfaceNormals2019] . . Summary . Despite not being popular right now, treating keypoints as local affine features has a lot of advantages over the traditional treatment the local correspondence as the point correspondences. In the next post I will describe a way of estimating the local feature affine shape and orientation. . References . [SuperPoint2017] Detone D., Malisiewicz T. and Rabinovich A., ``Superpoint: Self-Supervised Interest Point Detection and Description&#39;&#39;, CVPRW Deep Learning for Visual SLAM, vol. , number , pp. , 2018. . [Lowe99] D. Lowe, ``Object Recognition from Local Scale-Invariant Features&#39;&#39;, ICCV, 1999. . [Harris88] C. Harris and M. Stephens, ``A Combined Corner and Edge Detector&#39;&#39;, Fourth Alvey Vision Conference, 1988. . [Hessian78] P.R. Beaudet, ``Rotationally invariant image operators&#39;&#39;, Proceedings of the 4th International Joint Conference on Pattern Recognition, 1978. . [KeyNet2019] A. Barroso-Laguna, E. Riba, D. Ponsa et al., ``Key.Net: Keypoint Detection by Handcrafted and Learned CNN Filters&#39;&#39;, ICCV, 2019. . [ORB2011] E. Rublee, V. Rabaud, K. Konolidge et al., ``ORB: An Efficient Alternative to SIFT or SURF&#39;&#39;, ICCV, 2011. . [HardNet2017] A. Mishchuk, D. Mishkin, F. Radenovic et al., ``Working Hard to Know Your Neighbor&#39;s Margins: Local Descriptor Learning Loss&#39;&#39;, NeurIPS, 2017. . [RIFT2005] {Lazebnik} S., {Schmid} C. and {Ponce} J., ``A sparse texture representation using local affine regions&#39;&#39;, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 27, number 8, pp. 1265-1278, 2005. . [sGLOH2] {Bellavia} F. and {Colombo} C., ``Rethinking the sGLOH Descriptor&#39;&#39;, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 40, number 4, pp. 931-944, 2018. . [OriNet2016] K. M., Y. Verdie, P. Fua et al., ``Learning to Assign Orientations to Feature Points&#39;&#39;, CVPR, 2016. . [AffNet2018] D. Mishkin, F. Radenovic and J. Matas, ``Repeatability is Not Enough: Learning Affine Regions via Discriminability&#39;&#39;, ECCV, 2018. . [PerdochRetrieval2009] M. {Perd&#39;och}, O. {Chum} and J. {Matas}, ``Efficient representation of local geometry for large scale object retrieval&#39;&#39;, CVPR, 2009. . [IMW2020] Jin Yuhe, Mishkin Dmytro, Mishchuk Anastasiia et al., ``Image Matching across Wide Baselines: From Paper to Practice&#39;&#39;, arXiv preprint arXiv:2003.01587, vol. , number , pp. , 2020. . [gcransac2018] D. Barath and J. Matas, ``Graph-Cut RANSAC&#39;&#39;, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. . [magsac2019] J.N. Daniel Barath, ``MAGSAC: marginalizing sample consensus&#39;&#39;, CVPR, 2019. . [Philbin07] J. Philbin, O. Chum, M. Isard et al., ``Object Retrieval with Large Vocabularies and Fast Spatial Matching&#39;&#39;, CVPR, 2007. . [perd2006epipolar] M. Perd&#39;och, J. Matas and O. Chum, ``Epipolar geometry from two correspondences&#39;&#39;, ICPR, 2006. . [PrittsRANSAC2013] J. {Pritts}, O. {Chum} and J. {Matas}, ``Approximate models for fast and accurate epipolar geometry estimation&#39;&#39;, 2013 28th International Conference on Image and Vision Computing New Zealand (IVCNZ 2013), 2013. . [Barath2019ICCV] D. Barath and Z. Kukelova, ``Homography From Two Orientation- and Scale-Covariant Features&#39;&#39;, ICCV, 2019. . [RANSACAffine2020] M. {Rodríguez}, G. {Facciolo}, R. G. et al., ``Robust estimation of local affine maps and its applications to image matching&#39;&#39;, 2020 IEEE Winter Conference on Applications of Computer Vision (WACV), 2020. . [barath2020making] Barath Daniel, Polic Michal, Förstner Wolfgang et al., ``Making Affine Correspondences Work in Camera Geometry Computation&#39;&#39;, arXiv preprint arXiv:2007.10032, vol. , number , pp. , 2020. . [guan2020relative] Guan Banglei, Zhao Ji, Barath Daniel et al., ``Relative Pose Estimation for Multi-Camera Systems from Affine Correspondences&#39;&#39;, arXiv preprint arXiv:2007.10700, vol. , number , pp. , 2020. . [OneACMonoDepth2020] D.B. Ivan Eichhardt, ``Relative Pose from Deep Learned Depth and a Single Affine Correspondence&#39;&#39;, ECCV, 2020. . [SurfaceNormals2019] {Baráth} D., {Eichhardt} I. and {Hajder} L., ``Optimal Multi-View Surface Normal Estimation Using Affine Correspondences&#39;&#39;, IEEE Transactions on Image Processing, vol. 28, number 7, pp. 3301-3311, 2019. .",
            "url": "/wide-baseline-stereo-blog/2020/07/17/affine-correspondences.html",
            "relUrl": "/2020/07/17/affine-correspondences.html",
            "date": " • Jul 17, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "WxBS: Wide Multiple Baseline Stereo as a task",
            "content": "Definition of WxBS . Let us denote observations $O_{i}, i=1..n$, each of which belongs to one of the views $V_{j}, j=1..m$, $m leq n$. . Observation consist of spatial information and the &quot;descriptor&quot;. View contains the information, which is shared and the same for a group of observations. . For example, a single observation can be an RGB pixel. Its spatial information is the pixel coordinates and the &quot;descriptor&quot; is RGB value. The view then is the image, with information about the camera pose, camera intrinsics, sensor and the time of the photo. Some of this information can be unknown to the user, i.e. hidden variable. . Another example could an event camera[EventCameraSurvey2020]. In that case the observation contains the pixel coordinates and the descriptor is the sign of the intensity change. The view will contain the information about the sensor, camera pose and the single observation inside it, because every event has an unique timestamp. . Observations and views can be of different nature and dimentionality. E.g. $V_1$, $V_2$ -- RGB images, $V_3$ -- point cloud from a laser scaner, $V_4$ -- image from a thermal camera, and so on. . An unordered pair of observations $(O_{i},O_{k})$ forms a correspondence $c_{ik}$ if they are belong to different views $V_{j}$. The group of observations is called multivew correspondence $C_o$, when there is exactly one observation $O_i$ per view $V_j$. Some of observations $O_i$ can be empty $ varnothing$, i.e. not observed in the specific view $V_j$. . The world model is the set of contraints on views, observations and correspondences. For example, one the popular models are epipolar geometry and ridid motion assumption. . The correspondence is called ground truth or veridical if it satisfy the constraints posed be the world model. . We can now define a wide baseline stereo. . By wide baseline stereo we understand the process of establishing two or multi-view correspondences $C_o$ from observations $O_i$ and images $V_{j}$ and recovering the missing information about the views and estimatting the unknown parameters of the world model. . Most often in the current thesis we will be using the the following world model. The scene consists of 3 dimentional elements, and is rigid and static. The observations are the 2D projections to the camera plane by the projectice pinhole camera. The relationship between observations in different views is either epipolar geometry, or projective transform[Hartley2004]. Any moving object does not satisty the world model and therefore is considered an occlusion. We will call the &quot;baseline&quot; the distance between the camera centers. . For example, on image below, observations $O_i$ are blue circles and the correspondences $c_{jk}$ are shown as lines. The assumed object $X_i$ is a red circle. . . We will call &quot;wide multiple baseline stereo&quot; or WxBS [Mishkin2015WXBS] if the observations have different nature or the conditions under which observation were made are different. . The different between wide baseline stereo and short baseline stereo, or, simply stereo is the follwing. In stereo the baseline is small -- less then 1 meter -- and typically known and fixed. The task is to establish correspondences, which can be done by 1D search along the known epipolar lines. . In contrast, in wide baseline stereo the baseline is unknown, mostly unconstrained and the viewpoints of the cameras can vary drastically. . The wide baseline stereo, which also outputs the estimation of the latent objects, e.g. in form of 3d point world coordinates we would call rigid structure-from-motion (rigid SfM) or 3D reconstruction. We do not consider object shape approximation with voxels, meshes, etc in the current thesis. Nor we consider the recovery of scene albedo, illumination, and other appearance properties. . While the difference between SfM and WBS is often blurred and the terms are used interchangeably, we would consider WBS as a part of SfM pipeline prior to recovering 3d point cloud. . Other correspondence problems, as tracking, optical flow or establishing semantic correspondences could be defined using the terminilogy we established. . . References . (Gallego, Delbruck et al., 2020) Gallego Guillermo, Delbruck Tobi, Orchard Garrick Michael et al., ``Event-based Vision: A Survey&#39;&#39;, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. , number , pp. , 2020. . (Hartley and Zisserman, 2004) R.~I. Hartley and A. Zisserman, ``Multiple View Geometry in Computer Vision&#39;&#39;, 2004. . (Mishkin, Matas et al., 2015) D. Mishkin, J. Matas, M. Perdoch et al., ``WxBS: Wide Baseline Stereo Generalizations&#39;&#39;, BMVC, 2015. .",
            "url": "/wide-baseline-stereo-blog/2020/07/09/wxbs.html",
            "relUrl": "/2020/07/09/wxbs.html",
            "date": " • Jul 9, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "The Role of Wide Baseline Stereo in the Deep Learning World",
            "content": "What is the wide baseline stereo? . Imagine you have a nice photo you took in autumn and would like to take one in summer, from exactly the same position. How would you achieve that? You come to the place and start to compare what you see on camera screen and on printed photo. Specifically, you probably would try to locate the same objects, e.g. &quot;that high lamppost&quot; or &quot;this wallclock&quot;. Then one would estimate difference how they are arranged on the old photo and camera screen, e.g. the lamppost is occluding the clock on the tower&quot;. That would give you an idea of how you should move your camera, i.e. relative camera pose. . Now, what if you don&#39;t have that photo as is, but can have only a text description of it instead? In that case, it is likely, that you would try to list some features and objects one can see in the photo, together with some disntinctive descriptions, like &quot;long staircase on the left side&quot; and &quot;Nice building with dark roof and two towers&quot;. You would also tell, where this objects are in the photo, e.g.&quot;The lamp posts are at the left, the closer to the viewer is in front of the left tower with clock. Tower with the clock is separate from the building itself&quot;. . Then, when arriving on the spot, you would try to find those objects, match them to the description you have and, again, try to estimate the where you should go. . What I have just described is known as wide baseline stereo problem and the common way solving it. . Compute interest points/regions (&quot;trees&quot;, &quot;statues&quot;, etc) in all images independently . | For each interest point/region compute a descriptor of their neigborhood (&quot;statue with blue left ear&quot;). . | Establish tentative correspondences between interest points based on their descriptors. . | Robustly estimate geometric relation between two images based on tentative correspondences. &quot;If the camera is here, then this tree should be behind the statue&quot;. . | Rise of Wide Baseline Stereo . A bit more formally, the wide baseline stereo (WBS) is a process of establishing correspondences between pixels and/or regions between images depicting the same object or scene and estimation geometric relationship between the cameras, which produced that images. It is the building block of many popular computer vision application, where spatial localization or 3D world understanding is required. . . Where does wide baseline stereo come from? . As it often happens, the new arised from the older problem -- stereo matching, or as we will call it -- narrow or short baseline stereo. In the narrow baseline stereo images are taken from the nearby positions and not differ much in the orientation either. One could find correspondence for the point $(x,y)$ from the image $I_1$ in the image $I_2$ by simply searching in some small window around $(x,y)$[Hannah1974ComputerMO,Moravec1980] or, assuming that camera pair is calibrated -- by searching along the epipolar line[Hartley2004]. . . One of the first succesful approaches to the wide baseline stereo problem was proposed by Schmid and Mohr[Schmid1995] in 1995. One of the stepping stones was the corner detector by Harris and Stevens [Harris88], initially developed for the application of tracking. . It was later extended by Beardsley, Torr and Zisserman[Beardsley96] by adding RANSAC robust geometry estimation and later refined by Pritchett and Zisserman [Pritchett1998,Pritchett1998b] in 1998. The general pipeline remains mostly the same until now [WBSTorr99,CsurkaReview2018]. The currently adopted version of the wide baseline stereo algorithm is shown below. . . Let&#39;s repeat the WBS algorithm again: . Compute interest points/regions in all images independently | For each interest point/region compute a descriptor of their neigborhood (local patch). | Establish tentative correspondences between interest points based on their descriptors. | Robustly estimate geometric relation between two images based on tentative correspondences with RANSAC. | The reason of steps 1 and 2 done on the both images separately is that in general wide baseline stereo is not limited to pairs of images, but rather to a collections of them. If all the steps are done pairwise, then the computational complexity is $O(n^2)$. The more steps done seperately - the more efficient algorithm is. . Quick expansion . This algorithm significantly changed computer vision landscape for next forteen years. . Soon after introducing the algorithm, there it become clear that its quality significantly depends on quality of each component, i.e. local feature detector, descriptor, and geometry estimation. Pleora of new detectors and descriptors were proposed, with the most cited computer vision paper ever SIFT local feature[Lowe99]. . It is worth noting, that SIFT became popular only after Mikolajczyk benchmark paper [MikoDescEval2003,Mikolajczyk05], showed it superiority to the rest of alternatives. . Robust geometry estimation was also a hot topic: a lot of improvements over vanilla RANSAC were proposed: LO-RANSAC[LOransac2003], DEGENSAC[Degensac2005], MLESAC[MLESAC00] . Success of wide baseline stereo with SIFT features led to aplication of its components to other computer vision tasks, which were reformulated through wide baseline stereo lens: . Scalable image search. Sivic and Zisserman in famous &quot;Video Google&quot; paper[VideoGoogle2003] proposed to treat local features as &quot;visual words&quot; and use ideas from text processing for searching in image collections. Later even more WBS elements were re-introduced to image search, most notable -- spatial verification[Philbin07]: simplified RANSAC procedure to verify if visual word matches were spatially consistent. | . . Image classification was performed by placing some classifier (SVM, random forest, etc) on top of some encoding of the SIFT-like descriptors, extracted sparsely[Fergus03,CsurkaBoK2004] or densely[Lazebnik06]. | . . Object detection was formulated as relaxed wide baseline stereo problem[Chum2007Exemplar] or as classification of SIFT-like features inside a sliding window [HoG2005] | . . Semantic segmentation was performed by classicication of local region descriptors, typically, SIFT and color features and postprocessing afterwards[Superparsing2010]. | . Of course,wide baseline stereo was also used for its direct applications: . 3D reconstruction was based on camera poses and 3D points, estimated with help of SIFT features [PhotoTourism2006,RomeInDay2009,COLMAP2016] | . . SLAM(Simultaneous localization and mapping) [Se02,PTAM2007,Mur15] were based on fast version of local feature detectors and descriptors. . | Panorama stiching [Brown07] and, more generally, feature-based image registration[DualBootstrap2003] were initalized with a geometry obtained by WBS and then further optimized . | . Deep Learning Invasion: retreal to the geometrical fortress . In 2012 deep learning-based AlexNet[AlexNet2012] approach beat all the methods in image classification. Soon after, Razavian et.al[Astounding2014] have shown that convolutional neural networks (CNNs) pre-trained on the Imagenet outperform more complex traditional solutions in image and scene classification, object detection and image search. Deep learning solutions, be it pretrained or end-to-end learned networks quickly become the default solution for the most of computer vision problems. . . However, there was still an area, where deep learned solutions failed, sometimes spectacularly: geometry-related tasks. Wide baseline stereo[Melekhov2017relativePoseCnn], visual localization[PoseNet2015]}, SLAM are still areas, where the classical wide baseline stereo dominates[sattler2019understanding,zhou2019learn]. . The full reasons why convolution pipelines are failing for geometrical tasks are yet to understand, but the current hypothesis are the following: . CNN-based pose predictions predictions are roughly equivalent to retrieval of most similar image from the training set and outputing its pose.[sattler2019understanding] This phenomenum is also observed in related area: single-view 3D reconstruction[Tatarchenko2019]. | Geometric and arithmetic operations are hard to represent via vanilla neural networks (i.e. matrix multiplication with non-linearity) and they may require specialized building blocks, resembling operations of algorithmic or geometric methods, e.g. spatial transformers[STN2015] and arithmetic units[NALU2018,NAU2020]. Even with special structure such networks require &quot;careful initialization, restricting parameter space, and regularizing for sparsity&quot;[NAU2020]. | Vanilla CNNs are not covariant to even simple geometric transformation like translation [MakeCNNShiftInvariant2019], scaling and especially rotation [GroupEqCNN2016]. Unlike them, WBS baseline is grounded on scale-space theory [lindeberg2013scale] and local patches are geometrically normalilzed before description. | Predictions of the CNNs can be altered by change in a small localized area [AdvPatch2017] or even single pixel [OnePixelAttack2019], while the wide baseline stereo methods require the consensus of different independent regions. | . Today: assimilation and merging . Wide baseline stereo as a task: formulate differentiably and learn modules . Wide baseline stereo as a task is solved today typically by using learned components as a replacement of specific blocks in WBS algorithm[jin2020image] ,e.g. local descriptor like HardNet[HardNet2017], detectors like KeyNet[KeyNet2019], joint detector-descriptor[SuperPoint2017] matching and filtering like SuperGlue[sarlin2019superglue], etc. There are also attempts to formulate the whole downstream task pipeline like SLAM[gradslam2020] in a differentiable way, combining advantages of structured and learning-based approaches. . . . Wide baseline stereo as a idea: consensus of local independent predictions . On the other hand, as an algorithm, wide baseline stereo is summarized into two main ideas . Image should be represented as set of local parts, robust to occlusion, and not influencing each other. | Decision should be based on spatial consensus of local feature correspondences. | One of modern revisit of wide baseline stereo ideas is Capsule Networks[CapsNet2011,CapsNet2017]. Unlike CNNs, they encode not only intensity of feature responce, but also its location and require a geometric agreement between object parts for outputing a confident prediction. . Similar ideas are now explored for ensuring adversarial robustness of CNNs[li2020extreme]. . Another way of using &quot;consensus of local independent predictions&quot; is used in Cross-transformers paper: spatial attention helps to select relevant feature for few-shot learning, see Figure below. . While wide baseline stereo is far from the mainstream now, it continues to play an important role in computer vision. . . . References . [Hannah1974ComputerMO] M. J., ``Computer matching of areas in stereo images.&#39;&#39;, 1974. . [Moravec1980] Hans Peter Moravec, ``Obstacle Avoidance and Navigation in the Real World by a Seeing Robot Rover&#39;&#39;, 1980. . [Hartley2004] R.~I. Hartley and A. Zisserman, ``Multiple View Geometry in Computer Vision&#39;&#39;, 2004. . [Schmid1995] Schmid Cordelia and Mohr Roger, ``Matching by local invariants&#39;&#39;, , vol. , number , pp. , 1995. online . [Harris88] C. Harris and M. Stephens, ``A Combined Corner and Edge Detector&#39;&#39;, Fourth Alvey Vision Conference, 1988. . [Beardsley96] P. Beardsley, P. Torr and A. Zisserman, ``3D model acquisition from extended image sequences&#39;&#39;, ECCV, 1996. . [Pritchett1998] P. Pritchett and A. Zisserman, ``Wide baseline stereo matching&#39;&#39;, ICCV, 1998. . [Pritchett1998b] P. Pritchett and A. Zisserman, ``&quot;Matching and Reconstruction from Widely Separated Views&quot;&#39;&#39;, 3D Structure from Multiple Images of Large-Scale Environments, 1998. . [WBSTorr99] P. Torr and A. Zisserman, ``Feature Based Methods for Structure and Motion Estimation&#39;&#39;, Workshop on Vision Algorithms, 1999. . [CsurkaReview2018] {Csurka} Gabriela, {Dance} Christopher R. and {Humenberger} Martin, ``From handcrafted to deep local features&#39;&#39;, arXiv e-prints, vol. , number , pp. , 2018. . [Lowe99] D. Lowe, ``Object Recognition from Local Scale-Invariant Features&#39;&#39;, ICCV, 1999. . [MikoDescEval2003] K. Mikolajczyk and C. Schmid, ``A Performance Evaluation of Local Descriptors&#39;&#39;, CVPR, June 2003. . [Mikolajczyk05] Mikolajczyk K., Tuytelaars T., Schmid C. et al., ``A Comparison of Affine Region Detectors&#39;&#39;, IJCV, vol. 65, number 1/2, pp. 43--72, 2005. . [LOransac2003] O. Chum, J. Matas and J. Kittler, ``Locally Optimized RANSAC&#39;&#39;, Pattern Recognition, 2003. . [Degensac2005] O. Chum, T. Werner and J. Matas, ``Two-View Geometry Estimation Unaffected by a Dominant Plane&#39;&#39;, CVPR, 2005. . [MLESAC00] Torr P.H.S. and Zisserman A., ``MLESAC: A New Robust Estimator with Application to Estimating Image Geometry&#39;&#39;, CVIU, vol. 78, number , pp. 138--156, 2000. . [VideoGoogle2003] J. Sivic and A. Zisserman, ``Video Google: A Text Retrieval Approach to Object Matching in Videos&#39;&#39;, ICCV, 2003. . [Philbin07] J. Philbin, O. Chum, M. Isard et al., ``Object Retrieval with Large Vocabularies and Fast Spatial Matching&#39;&#39;, CVPR, 2007. . [Fergus03] R. Fergus, P. Perona and A. Zisserman, ``Object Class Recognition by Unsupervised Scale-Invariant Learning&#39;&#39;, CVPR, 2003. . [CsurkaBoK2004] C.D. G. Csurka, J. Willamowski, L. Fan et al., ``Visual Categorization with Bags of Keypoints&#39;&#39;, ECCV, 2004. . [Lazebnik06] S. Lazebnik, C. Schmid and J. Ponce, ``Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories&#39;&#39;, CVPR, 2006. . [Chum2007Exemplar] O. {Chum} and A. {Zisserman}, ``An Exemplar Model for Learning Object Classes&#39;&#39;, CVPR, 2007. . [HoG2005] N. {Dalal} and B. {Triggs}, ``Histograms of oriented gradients for human detection&#39;&#39;, CVPR, 2005. . [Superparsing2010] J. Tighe and S. Lazebnik, ``SuperParsing: Scalable Nonparametric Image Parsing with Superpixels&#39;&#39;, ECCV, 2010. . [PhotoTourism2006] Snavely Noah, Seitz Steven M. and Szeliski Richard, ``Photo Tourism: Exploring Photo Collections in 3D&#39;&#39;, ToG, vol. 25, number 3, pp. 835–846, 2006. . [RomeInDay2009] Agarwal Sameer, Furukawa Yasutaka, Snavely Noah et al., ``Building Rome in a day&#39;&#39;, Communications of the ACM, vol. 54, number , pp. 105--112, 2011. . [COLMAP2016] J. Sch &quot;{o}nberger and J. Frahm, ``Structure-From-Motion Revisited&#39;&#39;, CVPR, 2016. . [Se02] Se S., G. D. and Little J., ``Mobile Robot Localization and Mapping with Uncertainty Using Scale-Invariant Visual Landmarks&#39;&#39;, IJRR, vol. 22, number 8, pp. 735--758, 2002. . [PTAM2007] G. {Klein} and D. {Murray}, ``Parallel Tracking and Mapping for Small AR Workspaces&#39;&#39;, IEEE and ACM International Symposium on Mixed and Augmented Reality, 2007. . [Mur15] Mur-Artal R., Montiel J. and Tard{ &#39;o}s J., ``ORB-Slam: A Versatile and Accurate Monocular Slam System&#39;&#39;, IEEE Transactions on Robotics, vol. 31, number 5, pp. 1147--1163, 2015. . [Brown07] Brown M. and Lowe D., ``Automatic Panoramic Image Stitching Using Invariant Features&#39;&#39;, IJCV, vol. 74, number , pp. 59--73, 2007. . [DualBootstrap2003] V. C., Tsai} {Chia-Ling and {Roysam} B., ``The dual-bootstrap iterative closest point algorithm with application to retinal image registration&#39;&#39;, IEEE Transactions on Medical Imaging, vol. 22, number 11, pp. 1379-1394, 2003. . [AlexNet2012] Alex Krizhevsky, Ilya Sutskever and Geoffrey E., ``ImageNet Classification with Deep Convolutional Neural Networks&#39;&#39;, 2012. . [Astounding2014] A. S., H. {Azizpour}, J. {Sullivan} et al., ``CNN Features Off-the-Shelf: An Astounding Baseline for Recognition&#39;&#39;, CVPRW, 2014. . [Melekhov2017relativePoseCnn] I. Melekhov, J. Ylioinas, J. Kannala et al., ``Relative Camera Pose Estimation Using Convolutional Neural Networks&#39;&#39;, , 2017. online . [PoseNet2015] A. Kendall, M. Grimes and R. Cipolla, ``PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization&#39;&#39;, ICCV, 2015. . [sattler2019understanding] T. Sattler, Q. Zhou, M. Pollefeys et al., ``Understanding the limitations of cnn-based absolute camera pose regression&#39;&#39;, CVPR, 2019. . [zhou2019learn] Q. Zhou, T. Sattler, M. Pollefeys et al., ``To Learn or Not to Learn: Visual Localization from Essential Matrices&#39;&#39;, ICRA, 2020. . [Tatarchenko2019] M. Tatarchenko, S.R. Richter, R. Ranftl et al., ``What Do Single-View 3D Reconstruction Networks Learn?&#39;&#39;, CVPR, 2019. . [STN2015] M. Jaderberg, K. Simonyan and A. Zisserman, ``Spatial transformer networks&#39;&#39;, NeurIPS, 2015. . [NALU2018] A. Trask, F. Hill, S.E. Reed et al., ``Neural arithmetic logic units&#39;&#39;, NeurIPS, 2018. . [NAU2020] A. Madsen and A. Rosenberg, ``Neural Arithmetic Units&#39;&#39;, ICLR, 2020. . [MakeCNNShiftInvariant2019] R. Zhang, ``Making convolutional networks shift-invariant again&#39;&#39;, ICML, 2019. . [GroupEqCNN2016] T. Cohen and M. Welling, ``Group equivariant convolutional networks&#39;&#39;, ICML, 2016. . [lindeberg2013scale] Lindeberg Tony, ``Scale-space theory in computer vision&#39;&#39;, , vol. 256, number , pp. , 2013. . [AdvPatch2017] T. Brown, D. Mane, A. Roy et al., ``Adversarial patch&#39;&#39;, NeurIPSW, 2017. . [OnePixelAttack2019] Su Jiawei, Vargas Danilo Vasconcellos and Sakurai Kouichi, ``One pixel attack for fooling deep neural networks&#39;&#39;, IEEE Transactions on Evolutionary Computation, vol. 23, number 5, pp. 828--841, 2019. . [jin2020image] Jin Yuhe, Mishkin Dmytro, Mishchuk Anastasiia et al., ``Image Matching across Wide Baselines: From Paper to Practice&#39;&#39;, arXiv preprint arXiv:2003.01587, vol. , number , pp. , 2020. . [HardNet2017] A. Mishchuk, D. Mishkin, F. Radenovic et al., ``Working Hard to Know Your Neighbor&#39;s Margins: Local Descriptor Learning Loss&#39;&#39;, NeurIPS, 2017. . [KeyNet2019] A. Barroso-Laguna, E. Riba, D. Ponsa et al., ``Key.Net: Keypoint Detection by Handcrafted and Learned CNN Filters&#39;&#39;, ICCV, 2019. . [SuperPoint2017] Detone D., Malisiewicz T. and Rabinovich A., ``Superpoint: Self-Supervised Interest Point Detection and Description&#39;&#39;, CVPRW Deep Learning for Visual SLAM, vol. , number , pp. , 2018. . [sarlin2019superglue] P. Sarlin, D. DeTone, T. Malisiewicz et al., ``SuperGlue: Learning Feature Matching with Graph Neural Networks&#39;&#39;, CVPR, 2020. . [gradslam2020] J. Krishna Murthy, G. Iyer and L. Paull, ``gradSLAM: Dense SLAM meets Automatic Differentiation &#39;&#39;, ICRA, 2020 . . [CapsNet2011] G.E. Hinton, A. Krizhevsky and S.D. Wang, ``Transforming auto-encoders&#39;&#39;, ICANN, 2011. . [CapsNet2017] S. Sabour, N. Frosst and G.E. Hinton, ``Dynamic routing between capsules&#39;&#39;, NeurIPS, 2017. . [li2020extreme] Li Jianguo, Sun Mingjie and Zhang Changshui, ``Extreme Values are Accurate and Robust in Deep Networks&#39;&#39;, , vol. , number , pp. , 2020. online .",
            "url": "/wide-baseline-stereo-blog/2020/03/27/intro.html",
            "relUrl": "/2020/03/27/intro.html",
            "date": " • Mar 27, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi! My name is Dmytro Mishkin, I am computer vision researcher. This is blog series based on my on-going PhD thesis. The best way to contact me is Twitter . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "/wide-baseline-stereo-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "/wide-baseline-stereo-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}