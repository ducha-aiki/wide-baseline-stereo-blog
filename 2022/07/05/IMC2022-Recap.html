<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Image Matching Challenge 2022 Recap</h1><p class="page-description">Look, LoFTRs and SuperGlue everywhere</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-07-05T00:00:00-05:00" itemprop="datePublished">
        Jul 5, 2022
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      5 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/ducha-aiki/wide-baseline-stereo-blog/tree/master/_notebooks/2022-07-05-IMC2022-Recap.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/wide-baseline-stereo-blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/ducha-aiki/wide-baseline-stereo-blog/master?filepath=_notebooks%2F2022-07-05-IMC2022-Recap.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/wide-baseline-stereo-blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/ducha-aiki/wide-baseline-stereo-blog/blob/master/_notebooks/2022-07-05-IMC2022-Recap.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/wide-baseline-stereo-blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
          <div class="px-2">
  <a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Fducha-aiki%2Fwide-baseline-stereo-blog%2Fblob%2Fmaster%2F_notebooks%2F2022-07-05-IMC2022-Recap.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/wide-baseline-stereo-blog/assets/badges/deepnote.svg" alt="Launch in Deepnote"/>
  </a>
</div>

        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-07-05-IMC2022-Recap.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="What-has-been-changed-in-the-challenge-itself?-Kaggle!">What has been changed in the challenge itself? Kaggle!<a class="anchor-link" href="#What-has-been-changed-in-the-challenge-itself?-Kaggle!"> </a></h2><p>Yes, it is the main change, compared to previous years. How it works:</p>
<ul>
<li>Participants submit notebooks that run offline</li>
<li>Test set remains private (not visible to participants) : very difficult to cheat (big concern at Kaggle)</li>
<li>Allows for quick iterations</li>
</ul>
<p>That allowed us to get 25x more participants and 150x more submissions</p>
<p><img src="/wide-baseline-stereo-blog/images/copied_from_nb/2022-06-29-IMC2022_files/att_00001.png" alt="" title="We have 14 186 submissions from 642 teams this year" /></p>
<p>There are other differences compared to the previous years.</p>
<ol>
<li><p>No multiview track. Unlike previous competitions with multivew and stereo tracks, this year we focused on stereo only. There are many reasons for it, but the main is technical difficulty - to run and evaluate this under reasonable time limit. Having a pair of images as an input also could potentially broaden the family of methods used by participants.</p>
</li>
<li><p>New dataset and new metric. Unlike previous years, where our GT data was scale-less, now we have a metric ground truth, so we can evaluate translation error more in line with visual re-localization literature. We also used non-public dataset from Google, not available anywhere online. This was also only possible because of <a href="https://www.kaggle.com/docs/competitions#notebooks-only-FAQ">Kaggle Code competition</a> nature.</p>
</li>
<li><p>Time limit. I have mentioned this already, but it is important. The submission should finish in 9 hours on Kaggle GPU virtual instance. This forced teams to think, what to add, and what not. Simple example would be a semantic segmentation masking - teams decided that benefit is not worth computational overhead it brings.</p>
</li>
</ol>
<p><img src="/wide-baseline-stereo-blog/images/copied_from_nb/2022-06-29-IMC2022_files/att_00002.png" alt="" title="Inliers from LoFTR and KeyNetAffNetHardnet from kornia on one of 3 image pairs" /></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Top-solutions-2022-overview">Top solutions-2022 overview<a class="anchor-link" href="#Top-solutions-2022-overview"> </a></h2><p>Unlike previous competitions, almost all the top-performing teams have converged to almost the same pipeline, although different in implementation details, but based on the same ideas. Let me condence it is for you, and if you are interested in participants own write-ups, just scroll down, I will provide all the links after the post.</p>
<p>The only top-participant, who did not apply any pre/post-processing is 2nd place winner hkchkc. It is based on novel matcher (let's call it LoFTR 2.0), which he did not want to disclose yet. Anyway, check <a href="https://www.kaggle.com/competitions/image-matching-challenge-2022/discussion/329317">his write-up here</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So, the general pipeline is shown below. The following steps are present in the most of solutions:</p>
<p><img src="/wide-baseline-stereo-blog/images/copied_from_nb/2022-06-29-IMC2022_files/general-scheme-1c.png" alt="" title="Common pipeline among the top-performers of IMC-2022" /></p>
<ol>
<li><p>Obtaining initial pool of matches, using a combination of off-the-shelf models. Such models are LoFTR, SuperGlue, QuadTree Attention LoFTR and DKM. Some of the teams also applied test-time scale augmentation (TTA) and concatenated tentative correspondence, which came from matching differently-resize images.</p>
</li>
<li><p>The next step is an estimation of the co-visible area between two images, similar to <a href="https://arxiv.org/abs/2112.04846">ScaleNet</a> idea, but totally handcrafted and based on tentative matches. The approach is to cluster the matches using  K-means or DBSCAN and then find the bounding box in each image, which contains the most of the matches. This can lead to potentially several "co-visible areas". One may take a step further and estimate the homography between each matching region. An alternative to clustering (from 9th place) is to use MAGSAC++ with few iterations and coarse threshold to reject the most of outliers, and get the bounding box .</p>
</li>
</ol>
<ol>
<li><p>Each of such co-visible and matching regions is then cropped and resized to a predefined image size. Then the base-matcher, or other set of matching algorithms are applied for this "zoomed-in" image pair. All the correspondences are then reprojected into original image pair coordinates and contatenated with original correspondences. <strong>Important</strong>: "zoomed-in" is used to entend original tentative correspondences, not replace them, otherwise first failure would kill the matching process.</p>
</li>
<li><p>The correspondences are (optionally) filtered with some kind of non-maxima-suppression, like <a href="https://github.com/BAILOOL/ANMS-Codes">ANMS</a> or radius-based NMS. The intuition here is to select the fewer set of more precise matches. It is interesting, that no team run any kind of filtering network as OANet.</p>
</li>
<li><p>MAGSAC++ from OpenCV (as recommended in <a href="https://ducha-aiki.github.io/wide-baseline-stereo-blog/2021/05/17/OpenCV-New-RANSACs.html">my evaluation</a>) is applied to find fundamental matrix.</p>
</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Tricks-which-improve-results">Tricks which improve results<a class="anchor-link" href="#Tricks-which-improve-results"> </a></h2><ol>
<li><p><a href="https://www.kaggle.com/competitions/image-matching-challenge-2022/discussion/328865">Switching img1 and img2 for LoFTR-like methods</a>, because the method is asymmetric.</p>
</li>
<li><p>Running RANSAC and local feature extraction in separate threads. Although competition specific, this also may help to speed-up some semi-real-time SLAM methods.</p>
</li>
</ol>
<h2 id="Works-with-tuning">Works with tuning<a class="anchor-link" href="#Works-with-tuning"> </a></h2><ol>
<li>Some teams successfully used <a href="https://arxiv.org/abs/2003.08348">two-view keypoint refinement</a>, while others <a href="https://www.kaggle.com/competitions/image-matching-challenge-2022/discussion/328982">didn't make it work</a>.</li>
</ol>
<p><img src="/wide-baseline-stereo-blog/images/copied_from_nb/2022-06-29-IMC2022_files/att_00005.png" alt="" title="Illustration from Multi-View Optimization of Local Feature Geometry" /></p>
<ol>
<li>Unlike <a href="https://ieeexplore.ieee.org/abstract/document/9511155">MAGSAC++</a>, which works out of the box, <a href="https://arxiv.org/abs/2106.10240v2">VSAC</a> could bring significant time improvements, but needs a lot of hyperparameter tuning, otherwise epipolar geometry quality suffers.</li>
</ol>
<h2 id="Rare-findings">Rare findings<a class="anchor-link" href="#Rare-findings"> </a></h2><ol>
<li><p>Normalizing positional embeddings used in LoFTR-like methods. See <a href="https://www.kaggle.com/competitions/image-matching-challenge-2022/discussion/329317">solution 2</a>.</p>
</li>
<li><p>When doing TTA, run and cache SuperPoint prior to SuperGlue matching, this saves a lot of time. Also, SuperGlue whould be run on the extracted, not reprojected keypoints.</p>
</li>
<li><p>Resize method matters a little bit: <a href="https://www.kaggle.com/competitions/image-matching-challenge-2022/discussion/328803#1809862">Lanczos</a> seems to work the best.</p>
</li>
<li><p>Coordinates refinement by ECO-TR (improved fast version of <a href="https://github.com/ubc-vision/COTR">COTR</a>, not yet available online)</p>
</li>
</ol>
<p><img src="/wide-baseline-stereo-blog/images/copied_from_nb/2022-06-29-IMC2022_files/att_00006.png" alt="" title="Refinement by ECO-TR" /></p>
<h2 id="Does-not-work">Does not work<a class="anchor-link" href="#Does-not-work"> </a></h2><ol>
<li><p>Adding tentative matches from local features without learned matcher, such as DISK, ALike, etc. Team 1 mentioned that KeyNetAffNetHardNet was the one, which do imporove results, but at the cost of huge computational overhead, so it doesn't worth it.</p>
</li>
<li><p>Semantic segmentation masking (sky, people, etc).</p>
</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Final-thoughts">Final thoughts<a class="anchor-link" href="#Final-thoughts"> </a></h2><p>Sooo...first think I want to say, is that going Kaggle was definitely worth it. We learned a lot, challenge got visibility, and many new improvements came out of of the challenge.</p>
<p>Second, is that our training set was too small and and too different from the test set, that is why it was almost useless. I don't have any solution yet for this.</p>
<p>Thresholds on the pose accuracy were probably too loose, as you can see from the results.</p>
<p>On the lessons from the solution themselves:</p>
<ol>
<li><p>2-stage approach of first finding covisible region, and then zooming-in into it seems to be way to go. What is also important, is to delay the decision until the very end.</p>
</li>
<li><p>It is better to first fix recall problem, i.e. to get as many tentative correspondences as possible, e.g. from different methods, and only the filter them. Modern RANSACs can recover the pose from a small inlier ratio, but never from small number of correct correspondences themselves.</p>
</li>
<li><p>Learned methods like LoFTR are very sensitive on the input image size, we probably have to do something about it.</p>
</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="List-of-all-top-solution-write-ups">List of all top solution write-ups<a class="anchor-link" href="#List-of-all-top-solution-write-ups"> </a></h2><p>It is <a href="https://www.kaggle.com/competitions/image-matching-challenge-2022/discussion/329650">here</a>, created by Addison Howard.</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="ducha-aiki/wide-baseline-stereo-blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/wide-baseline-stereo-blog/2022/07/05/IMC2022-Recap.html" hidden></a>
</article>
