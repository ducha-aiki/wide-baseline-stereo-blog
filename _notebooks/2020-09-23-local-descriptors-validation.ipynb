{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Revisiting Brown patch dataset and benchmark\"\n",
    "> \"How to create useful development set\"\n",
    "- toc: false\n",
    "- image: images/brown_phototour_revisited.jpg\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- hide: false\n",
    "- search_exclude: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this post\n",
    "\n",
    "1. Why one needs good development set? What is wrong with existing sets for local patch descriptor learning?\n",
    "2. One should validate in the same way, as it is used in production.\n",
    "3. Brown patch revisited -- implementation details\n",
    "4. Local patch descriptors evaluation results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Really quick intro into local patch descriptors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local patch descriptor is the thing, which helps you to automatically decide, if two patches in the pair of images correspond to the same point in a real world, or not. It should be robust to illumination, viewpoint and other changes. \n",
    "\n",
    "![](2020-09-16-local-descriptors-validation_files/att_00003.png \"The task of local descriptor (neural network here) is to decide if two patches belong to the same point, or nor. Image taken from SoSNet decriptor blogpost by Vassileios Balntas https://medium.com/scape-technologies/mapping-the-world-part-4-sosnet-to-the-rescue-5383671713e7\")\n",
    "\n",
    "There are lots of ways how to implement a local patch descriptor: engineered and learned. \n",
    "Local patch descriptor is the crucial component of the [wide baseline stereo pipeline](https://ducha-aiki.github.io/wide-baseline-stereo-blog/2020/03/27/intro.html) and a popular computer vision research topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do you need development set?\n",
    "\n",
    "Good data is crucial for any machine learning problem -- everyone now knows that. \n",
    "One needs high quality training set for training a good model. One also needs good test set, to know, what is _real_ performance. However, there is one more, often forgotten, crucial component -- **validation** or **development** set. We use it to decide hyperparameters and validate design choices we make. It should be different from both training and test sets, yet, be good predictor of test set performance.\n",
    "Moreover, it should allow fast iterations, so be not too small. \n",
    "\n",
    "While such set is commonly called [validation set](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets), I do like Andrew Ng's term \"[development](https://cs230.stanford.edu/files/C2M1.pdf)\" set more - because it helps to *develop* your model.\n",
    "\n",
    "\n",
    "# Existing datasets for local patch descriptors\n",
    "So, what are the development set options for local patch descriptors?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brown PhotoTourism.\n",
    "\n",
    "![](2020-09-16-local-descriptors-validation_files/brown_patches.png \"Patches from 3 subsets of Brown Phototourism dataset\")\n",
    "\n",
    "The most commonly and successfully used dataset for local descriptor learning is PhotoTourism, created in 2008. Here is its [description by authors](http://matthewalunbrown.com/patchdata/patchdata.html):\n",
    "\n",
    "\n",
    ">The dataset consists of corresponding patches sampled from 3D reconstructions of the Statue of Liberty (New York), Notre Dame (Paris) and Half Dome (Yosemite).\n",
    "\n",
    "It also comes with evaluation protocol: patch pairs are labeled as \"same\" or \"different\" and the false positive rate at recall of 95% (FPR95) is reported. The variable, used to build ROC curve is descriptor distance between two patches.\n",
    "\n",
    "Advantages:\n",
    "\n",
    " - It contains local patches, extracted for two types of local feature detector -- DoG (SIFT) and Harris corners.\n",
    " - It is extracted from images, contraining non-planar structures and the geometrical noise present is caused by the local feature detector, not added artificially.  \n",
    " - Descriptors, trained on the dataset, show very good performance \\cite{IMW2020}, therefore the data itself is good.\n",
    "\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    " - when used as a benchmark, it shows unrealistic results: SIFT is 40x worse than deep learned descriptor. In practice, the difference is much smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HPatches\n",
    "\n",
    "[HPatches](https://github.com/hpatches/hpatches-dataset), where H stands for the \"[homography](https://en.wikipedia.org/wiki/Homography)\" was proposed to overcome the problem of unrealisting metric and, seemingly, too easy data, used in Phototourism dataset.\n",
    "\n",
    "It was constructed in a different way than a Phototourism. First, local features were detected in the \"reference\" image and then reprojected to other images in sequences. Reprojection is prossible, because all the images are photographies of the planes  -- graffity, drawing, print, etc, or are all taken from the same position.\n",
    "After the reprojection, some amount of geometrical noise -- rotation, translation, scaling, was added to the local features and the patches were extracted. \n",
    "\n",
    "This process is illustration on the picture below (both taken from the [HPatches website](https://github.com/hpatches/hpatches-dataset)).\n",
    "\n",
    "HPAtches also provide 3 testing protocol, evaluating mean average precision (mAP) for 3 different tasks: patch verification  -- similar to Brown Phototourism, image matching and patch retrieval. The variable, used to build mAP is descriptor distance between two patches.\n",
    "\n",
    "Advantages:\n",
    "\n",
    " - Unlike PhotoTourism patch verification, image matching and patch retrieval tasks are not saturated.\n",
    " - HPatches contains illumination split, allowing the evaluation of descriptor robustness to illumination changes.\n",
    " \n",
    "Disadvantages:\n",
    "\n",
    " - patches \"misregistration\" noise is of artificial nature, although paper claims that it has similar statistics\n",
    " - no non-planar structure\n",
    " - performance in HPaptches does not really correlate with the downstream performance \\cite{IMW2020}\n",
    "\n",
    "\n",
    "![](2020-09-16-local-descriptors-validation_files/images_hard.png \"Visualization of the hard patches locations in the target images.\")\n",
    "![](2020-09-16-local-descriptors-validation_files/patches_hard.png \"Extracted hard patches from the example sequence.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AMOSPatches\n",
    "\n",
    "[AMOS patches](https://github.com/pultarmi/AMOS_patches) is \"HPatches illumination on steroids, without geometrical noise\". It has the same advantanges and disadvantages, as HPatches and is mostly focused on illumination and weather changes.\n",
    "\n",
    "\n",
    "![](2020-09-16-local-descriptors-validation_files/att_00002.png \"Some images, contributing to AMOSPatches\")\n",
    "![](2020-09-16-local-descriptors-validation_files/amos_patches_small.png \"Some patches from AMOS Patches dataset\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PhotoSynth\n",
    "\n",
    "\n",
    "[PhotoSynth](https://github.com/rmitra/PS-Dataset) can be described and something in the middle between Phototour and HPatches. It contains patches, sampled from planar scenes, as well as from non-planar scenes. \n",
    "\n",
    "At first glance, it should be great for the test and training purposes. However, there are several issues with it.\n",
    "First, pre-trained HardNetPS descriptor, released together with the dataset, works well on HPatches, but poor in practice\\cite{pultar2020improving}. \n",
    "\n",
    "Second, a couple of colleagues has tried to train the descriptor on top of it, as it was significantly worse than the authors reference model. Moreover, there is no testing/training code protocol available together with dataset.\n",
    "\n",
    "So, while PhotoSynth might be a good dataset in principle, it definitely needs more love and work.\n",
    "\n",
    "![image.png](2020-09-16-local-descriptors-validation_files/att_00000.png \"Images, contributed to PS dataset\")\n",
    "\n",
    "![image.png](2020-09-16-local-descriptors-validation_files/att_00001.png \"Patches, samples from PS dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing the evaluation protocol\n",
    "\n",
    "Classical local descriptor matching consists of two parts: finding nearest neighbors and filtering unreliable ones based on some criterion. \n",
    "I have wrote a [blogpost, describing the matching strategies in details](https://medium.com/@ducha.aiki/how-to-match-to-learn-or-not-to-learn-part-2-1ab52ede2022).\n",
    "\n",
    "\n",
    "The most used in practice criterion is the first to second nearest neighbor distance (Lowe's) ratio threshold for filtering false positive matches. It is shown in the figure below. \n",
    "\n",
    "The intuition is simple: if two candidates are too similar, then the match is unreliable and it is better to drop it.\n",
    "\n",
    "![](2020-09-16-local-descriptors-validation_files/att_00004.png \"Second nearest ratio strategy. Features from img1 (blue circles) are matched to features from img2 (red squares). For each point in img1 we calculate two nearest neighbors and check their distance ratio . If both are too similar (>0.8, bottom at Figure), then the match is discarded. Only confident matches are kept. Right graph is from SIFT paper, justification of such strategy.\")\n",
    "\n",
    "Somehow, none of the local patch evaluation protocols does not take such filtering criterion in mind, although it greatly influences the overall performance. \n",
    "\n",
    "So, let's do the following:\n",
    "\n",
    "1. Take the patches, which are extracted from only two images. \n",
    "2. For the each patch, calculate the descriptor distance to the correct match and to the hardnest (closest) non-match. Calculate the Lowe's ratio between this two.\n",
    "3. Calculate accuracy for each of such triplets. If the correct match has smaller distance, score 1, if not - 0.\n",
    "4. Sort the ratios from smallest to biggest and calculate [mean average precision](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision) (mAP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brown PhotoTour Revisied: implementation details\n",
    "\n",
    "\n",
    "We have designed the protocol, now time for data. We could spend several month collecting and cleaning it...or we can just re-use great Brown PhotoTourism dataset.\n",
    "\n",
    "For the protocol we designed above we need the information about the image id, where the patch was extracted from. Unfortunately, there is no such information in the Brown PhotoTourism, but there is suitable alternative -- the image id, where the reference patch was detected. What does it mean?\n",
    "\n",
    "Suppose, we have 4 images and 5 keypoints. All the keypoints present in all images, which gives us 20 patches.\n",
    "3 keypoints were first detected in Image 1 and 2 in image 2.  \n",
    "That means that we will have 12 patches labeled image 1 and 8 patches labeled image 2. \n",
    "\n",
    "So, we will have results for image 1 and image 2. Let's consider image 1. There are 12 patches, splitted in 3 \"classes\", 4 patches in each class. \n",
    "\n",
    "Then, for the each of those 12 patches we:\n",
    "\n",
    "- pick each of the corresponding patched as positives, so 3 positives. $P_1$, $P_2$, $P_3$\n",
    "- find the closest negative N. \n",
    "- add triplets (A, $P_1$, N), (A, $P_2$, N), (A, $P_3$, N) to the evaluation.\n",
    "\n",
    "Repeat the same for the image 2. \n",
    "That mimics the two-view matching process as close, as possible, given the data available to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "So, let's check how it goes. The latest results and implementation are in the following notebooks:\n",
    "\n",
    "- [Deep descriptors](https://github.com/ducha-aiki/brown_phototour_revisited/blob/master/examples/evaluate_deep_descriptors.ipynb)\n",
    "- [Non-deep descriptors](https://github.com/ducha-aiki/brown_phototour_revisited/blob/master/examples/evaluate_non_deep_descriptors.ipynb)\n",
    "\n",
    "The results are the following:\n",
    "\n",
    "\n",
    "    ------------------------------------------------------------------------------\n",
    "    Mean Average Precision wrt Lowe SNN ratio criterion on UBC Phototour Revisited\n",
    "    ------------------------------------------------------------------------------\n",
    "    trained on       liberty notredame  liberty yosemite  notredame yosemite\n",
    "    tested  on           yosemite           notredame            liberty\n",
    "    ------------------------------------------------------------------------------\n",
    "    Kornia RootSIFT 32px   58.24              49.07               49.65 \n",
    "    HardNet 32px       70.64  70.31        61.93  59.56        63.06  61.64\n",
    "    SOSNet 32px        70.03  70.19        62.09  59.68        63.16  61.65\n",
    "    TFeat 32px         65.45  65.77        54.99  54.69        56.55  56.24\n",
    "    SoftMargin 32px    69.29  69.20        61.82  58.61        62.37  60.63\n",
    "    HardNetPS 32px         55.56              49.70               49.12 \n",
    "    R2D2_center_grayscal   61.47              53.18               54.98 \n",
    "    R2D2_MeanCenter_gray   62.73              54.10               56.17 \n",
    "    ------------------------------------------------------------------------------\n",
    "    \n",
    "    ------------------------------------------------------------------------------\n",
    "    Mean Average Precision wrt Lowe SNN ratio criterion on UBC Phototour Revisited\n",
    "    ------------------------------------------------------------------------------\n",
    "    trained on       liberty notredame  liberty yosemite  notredame yosemite\n",
    "    tested  on           yosemite           notredame            liberty\n",
    "    ------------------------------------------------------------------------------\n",
    "    Kornia RootSIFT 32px   58.24              49.07               49.65 \n",
    "    Kornia RootSIFT 41px   57.83              48.48               49.01 \n",
    "    Kornia SIFT 32px       58.47              47.76               48.70 \n",
    "    Kornia SIFT 41px       58.14              47.30               48.30 \n",
    "    OpenCV_SIFT 32px       53.16              45.93               46.00 \n",
    "    OpenCV_SIFT 41px       54.10              46.09               46.29 \n",
    "    OpenCV_RootSIFT 32px   53.50              47.16               47.37 \n",
    "    OpenCV_RootSIFT 41px   54.19              47.20               47.37 \n",
    "    OpenCV_LATCH 65px  -----  -----        -----  37.26        -----  39.08\n",
    "    OpenCV_LUCID 32px      20.37              23.08               27.24 \n",
    "    skimage_BRIEF 65px     52.68              44.82               46.56 \n",
    "    ------------------------------------------------------------------------------\n",
    "    \n",
    "So far - in agreement with IMC benchmark: SIFT and RootSIFT are good, but not the best, SOSNet and HardNet are the leaders, but within tens of percents, not by orders of magnitude.\n",
    "\n",
    "\n",
    "![](2020-09-16-local-descriptors-validation_files/att_00005.png \"Image Matching Benchmark results, from https://arxiv.org/abs/2003.01587\")\n",
    "    \n",
    "    \n",
    "### Disclaimer 1: don't trust this tables fully\n",
    "\n",
    "\n",
    "I haven't (yet!) checked if all the deep descriptors models, trained on Brown, were trained with flip-rotation 90 degrees augmentation. In the code below I assume that they were, however, it might not be true -- and the comparison might not be completely fair. I will do my best to check it, but if you know that I have used wrong weights - please [open an issue](https://github.com/ducha-aiki/brown_phototour_revisited/issues). Thank you. \n",
    "\n",
    "\n",
    "### Disclaimer 2: it is not \"benchmark\".\n",
    "\n",
    "\n",
    "The intended usage of the package is not to test and report the numbers in the paper. Instead think about is as cross-validation tool, helping the development. Thus, one CAN tune hyperparameters based on the benchmark results  instead of doing so on [HPatches](https://github.com/hpatches/hpatches-benchmark). After you have finished tuning, please, evaluate your local descriptors on some downstream task like [IMC image matching benchmark](https://github.com/vcg-uvic/image-matching-benchmark) or [visual localization](https://www.visuallocalization.net/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n",
    "\n",
    "If you use the benchmark/development set in an academic work, please cite it.\n",
    "\n",
    "    @misc{BrownRevisited2020,\n",
    "      title={UBC PhotoTour Revisied},\n",
    "      author={Mishkin, Dmytro},\n",
    "      year={2020},\n",
    "      url = {https://github.com/ducha-aiki/brown_phototour_revisited}\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[<a id=\"cit-IMW2020\" href=\"#call-IMW2020\">IMW2020</a>] Jin Yuhe, Mishkin Dmytro, Mishchuk Anastasiia <em>et al.</em>, ``_Image Matching across Wide Baselines: From Paper to Practice_'', arXiv preprint arXiv:2003.01587, vol. , number , pp. ,  2020.\n",
    "\n",
    "[<a id=\"cit-pultar2020improving\" href=\"#call-pultar2020improving\">pultar2020improving</a>] Pultar Milan, ``_Improving the HardNet Descriptor_'', arXiv ePrint:2007.09699, vol. , number , pp. ,  2020.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "key",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": true,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
