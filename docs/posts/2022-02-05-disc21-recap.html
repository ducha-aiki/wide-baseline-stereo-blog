<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2022-02-05">
<meta name="description" content="Can you detect edited copies?">

<title>Image Similarity Challenge 2021 recap – Wide baseline stereo meets deep learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-6bd9cfa162949bde0a231f530c97869d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-GE2NZRSZBN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-GE2NZRSZBN', { 'anonymize_ip': true});
</script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Wide baseline stereo meets deep learning</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About Me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/ducha_aiki"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/ducha_aiki"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Image Similarity Challenge 2021 recap</h1>
                  <div>
        <div class="description">
          Can you detect edited copies?
        </div>
      </div>
                </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 5, 2022</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="what-is-image-similarity-challenge" class="level2">
<h2 class="anchored" data-anchor-id="what-is-image-similarity-challenge">What is Image Similarity Challenge?</h2>
<p>Image Similarity Challenge is benchmark of the methods, which should find the original in the database (if any) by the edited query image. The main difference to the classical instance retrieval task (datasets <a href="https://www.robots.ox.ac.uk/~vgg/data/oxbuildings/">Oxford5k</a>, <a href="https://github.com/cvdfoundation/google-landmark">GLD</a>, etc) is that we should NOT retrieve the different photos of the given instance. Instead only version of the same photos are valid. In other words, we are looking for the image forgery and copyright infringement.</p>
<p><img src="2022-02-05-DISC21-recap_files/att_00000.png" title="Example of the query and reference images in the ISC" class="img-fluid"></p>
<section id="image-similarity-dataset-disc" class="level3">
<h3 class="anchored" data-anchor-id="image-similarity-dataset-disc">Image Similarity Dataset (DISC)</h3>
<p>The dataset is described in the “<a href="https://arxiv.org/pdf/2106.09672.pdf">The 2021 Image Similarity Dataset and Challenge</a>” paper. It is created by processing YCC100M and DeepFake Detection Challenge images with manually in GIMP editor and automatically with help of <a href="https://arxiv.org/abs/2201.06494">AugLy</a> library.</p>
<p><img src="2022-02-05-DISC21-recap_files/att_00001.png" title="More example of the image transformations used in DISC. Taken from DISC paper" class="img-fluid"></p>
<p>Besides the public part of the test set (Phase I of the competition), there is also final test set (Phase II) was also composed with use of adversarial attack methods and so-called “air-gap attack”. Air-gap, according to the paper is</p>
<blockquote class="blockquote">
<p>We asked manual editors to perform “air-gap” transformations, where the picture is displayed or printed out, and re-captured with a camera or mobile phone. It is a quite common image attack, that is hard to reproduce automatically or even with an image editor like the GIMP. Figure 8 shows an example air-gap augmentation (the zebra). The effect is a combination of over-exposure, geometric framing and blurriness on the picture.</p>
</blockquote>
<p><img src="2022-02-05-DISC21-recap_files/att_00002.png" title="Test set Phase 2 transformations. Taken from DISC paper" class="img-fluid"></p>
<p>The paper also provides several baselines, mostly based on global and local descriptors</p>
<p><img src="2022-02-05-DISC21-recap_files/att_00003.png" title="Baselines, provided by the challenge organizers" class="img-fluid"></p>
</section>
</section>
<section id="tracks-descriptor-track-and-matching-track" class="level2">
<h2 class="anchored" data-anchor-id="tracks-descriptor-track-and-matching-track">Tracks: Descriptor track and Matching track</h2>
<p>The competition had 2 tracks.</p>
<ol type="1">
<li><p><strong>Descriptor</strong> track expects participant to submit global image descriptors, up to 256-dim, which should be compared by L2 distance. Matching image pairs should have small descriptor distances, while non-matching - large.</p></li>
<li><p><strong>Matching</strong> track, where the model can do any pairwise (but only pairwise!) comparisons to get the similarity score between two images.</p></li>
</ol>
<p>Below I will overview the winning solutions, presented at the <a href="https://sites.google.com/view/isc2021">NeurIPS2021 workshop</a>. But first let me show you a couple of slides from the <a href="https://drive.google.com/file/d/1Jcw9NWpBJ65A3RM5MaHB9-AKG_rM6-Wt/view">organizer’s analisys</a>.</p>
<p>The easiest image transformations are geometrical ones, except cropping the image to a very small size. The hardest ones are those, which involve combination of several images. The global descriptors are very sensitive to image rotatio and vertical flip (not really surprising).</p>
<p><img src="2022-02-05-DISC21-recap_files/att_00004.png" title="Easy-to-hard transformations. From organaizer's analysis" class="img-fluid"></p>
<p>The models are more or less reasonably robust to adversarial attacks, except really strong ones.</p>
<p><img src="2022-02-05-DISC21-recap_files/att_00005.png" title="Models sensitivity to the adversarial attacks" class="img-fluid"></p>
<p><img src="2022-02-05-DISC21-recap_files/att_00006.png" title="Examples of the adversarial attacks" class="img-fluid"></p>
<p>Finally organizers found that have not done enough de-duplication in the dataset, meaning that models were able to discover the duplicates in the dataset. While some of this annotation errors were genuine, others were in the gray zone.</p>
<p>For example, images, which were taken from the different positions, but the background object DOES look identical. Another example is the camera burst, where the part of the content, which is not the same across the burst images ended up cropped out by annotators or automatic procedure.</p>
<p><img src="2022-02-05-DISC21-recap_files/att_00007.png" title="Images taken from different position, but with the object far away" class="img-fluid"></p>
<p><img src="2022-02-05-DISC21-recap_files/att_00008.png" title="Image bursts, which are not de-duplicated" class="img-fluid"></p>
</section>
<section id="descriptor-track" class="level2">
<h2 class="anchored" data-anchor-id="descriptor-track">Descriptor track</h2>
<section id="st-place-contrastive-learning-with-large-memory-bank-and-negative-embedding-subtraction-for-accurate-copy-detection" class="level3">
<h3 class="anchored" data-anchor-id="st-place-contrastive-learning-with-large-memory-bank-and-negative-embedding-subtraction-for-accurate-copy-detection">1st place: Contrastive Learning with Large Memory Bank and Negative Embedding Subtraction for Accurate Copy Detection</h3>
<p>by Shuhei Yokoo aka lyakaap.</p>
<p>The method takes ImageNet-21k-pretrained EfficientNetv2 from <a href="https://fastai.github.io/timmdocs/">timm</a> and trains it with contrastive loss, gradually increasing the augmentations intensity, as well as image size. Final augmentations are quite strong, see the image below. The loss is the simple constrastive loss with memory bank for the hard negatives.</p>
<p><img src="2022-02-05-DISC21-recap_files/att_00009.png" title="Examples of augnentations by lyakaap team" class="img-fluid"></p>
<p><img src="2022-02-05-DISC21-recap_files/att_00010.png" title="Ablation study of the different steps" class="img-fluid"></p>
<p>Finally, Shuhei Yokoo proposes novel postprocessing technique - to subtract hard negative embedding from all database image embeddings. It added nice final boost to the submission.</p>
<p><a href="https://arxiv.org/pdf/2112.04323.pdf">arXiv</a> <a href="https://github.com/lyakaap/ISC21-Descriptor-Track-1st">github</a></p>
</section>
<section id="nd-place-producing-augmentation-invariant-embeddings-from-real-life-imagery" class="level3">
<h3 class="anchored" data-anchor-id="nd-place-producing-augmentation-invariant-embeddings-from-real-life-imagery">2nd place: Producing augmentation-invariant embeddings from real-life imagery</h3>
<p>by Sergio Manuel Papadakis and Sanjay Addicam</p>
<p>The method relies on the GeM descriptors ( trained EfficientNetv1, EfficientNetv2, NfNet) with ArcFace loss. ArcFace loss assumes classification, not contrastive setup and hard to train with &gt;1M classes. That is why the method gradually increases the number of classes (images used to train) and the classifier head is re-initialized with centroids, obtained from backbone descriptors from the previous stage. Multiple models are combined by Concat -&gt; PCA -&gt; L2Norm.</p>
<p><img src="2022-02-05-DISC21-recap_files/att_00011.png" title="2nd place model architecture" class="img-fluid"></p>
<p>Finally, the embeddings are modified, to take into account the most similar images from the database, similar to the 1st place, but in a bit more complicated way.</p>
<p><a href="https://arxiv.org/pdf/2112.03415.pdf">arXiv</a>, <a href="https://github.com/socom20/facebook-image-similarity-challenge-2021">github</a></p>
</section>
<section id="rd-place-bag-of-tricks-and-a-strong-baseline-for-image-copy-detection" class="level3">
<h3 class="anchored" data-anchor-id="rd-place-bag-of-tricks-and-a-strong-baseline-for-image-copy-detection">3rd place: Bag of Tricks and A Strong baseline for Image Copy Detection</h3>
<p>by Wenhao Wang, Weipu Zhang, Yifan Sun, Yi Yang</p>
<p>The method uses ResNet50 backbone, pretrained in augmentation-supervised fashion (<a href="https://arxiv.org/abs/2103.03230">BarlowTwins</a>, unlike 1st and 2nd place, which used supervised pretrained EfficientNetv2. The model is trained with a combination of triplet and cross entropy losses with GeM pooling and WaveBlock. Finally, authors also present hard-negative aware embedding de-normalization called “descriptor stretching” – different variant of the similar idea, also used in 1st and 2nd place post-processing.</p>
<p><img src="2022-02-05-DISC21-recap_files/att_00013.png" title="Training pipeline of the 3rd place in descriptor track" class="img-fluid"></p>
<p>Note that before descriptor stretching, results are much weaker than in 1st and 2nd place, but after the post-processing, the margin shrinks</p>
<p><img src="2022-02-05-DISC21-recap_files/att_00014.png" title="The ablation studye. Supervised or Unsupervised denotes the supervised pre-training or unsupervised pre-training is used, respectively. Des-Str denotes adding descriptor stretching strategy. Det denotes adding overlay detection YOLOv5. Multi denotes using multi-scale testing." class="img-fluid"></p>
<p><a href="https://arxiv.org/abs/2111.08004">arXiv</a>, <a href="https://github.com/WangWenhao0716/ISC-Track2-Submission">github</a></p>
</section>
</section>
<section id="matching-track" class="level2">
<h2 class="anchored" data-anchor-id="matching-track">Matching track</h2>
<section id="st-place-d2lv-a-data-driven-and-local-verification-approach-for-image-copy-detection" class="level3">
<h3 class="anchored" data-anchor-id="st-place-d2lv-a-data-driven-and-local-verification-approach-for-image-copy-detection">1st place D2LV: A Data-Driven and Local-Verification Approach for Image Copy Detection</h3>
<p>by Wenhao Wang, Yifan Sun, Weipu Zhang, Yi Yang (3rd place in descriptor track).</p>
<p>The method relies on several ideas:</p>
<ul>
<li>Training specialist models on the different augmentations, e.g.&nbsp;“basic”, “basic+superdark”, “basic+superblur”, etc.</li>
<li>Different ways of splitting the image into the subparts: by YOLOv5 object detection, by 4x4 even split, by rotating the original image, etc, each of which is then “matched” by the global descriptor similarity to the right-hand image. Local-to-global means “we aplit query and match vs reference image”, whereas “global-to-local” - - “we match whole query image to splitted reference”.</li>
</ul>
<p><img src="2022-02-05-DISC21-recap_files/att_00015.png" title="Winning solution pipeline" class="img-fluid"></p>
<p><img src="2022-02-05-DISC21-recap_files/att_00016.png" title="The ablation study D2LV. Supervised and Unsupervised denotes the supervised pre-training or unsupervised pre-training is used, respectively. Global-local -- using global-local matching strategy, and Both denotes using global-local and local-global matching strategy. Adv Aug denotes using different kinds of advanced augmentations. Multi-Tricks denotes using multi-scale testing and other men- tioned tricks." class="img-fluid"></p>
<p><a href="https://arxiv.org/pdf/2111.07090.pdf">arXiv</a> <a href="https://github.com/WangWenhao0716/ISC-Track1-Submission">github</a></p>
</section>
<section id="nd-place-2nd-place-solution-to-facebook-ai-image-similarity-challenge-matching-track" class="level3">
<h3 class="anchored" data-anchor-id="nd-place-2nd-place-solution-to-facebook-ai-image-similarity-challenge-matching-track">2nd place: 2nd Place Solution to Facebook AI Image Similarity Challenge: Matching Track</h3>
<p>by SeungKee Jeon.</p>
<p>The paper uses a simple approach of concatenating query and reference image and feeding such collage into the vision transformer (ViT).</p>
<p>ViT embeddings are also used to create a short-list for the matching, trained with contrastive loss (SimCLR).</p>
<p>Similarly to the 1st place, SeungKee Jeon also used image-to-sub-images (1 -&gt; 2x2, 1-&gt;3x3) splitting technique at test time.</p>
<p><img src="2022-02-05-DISC21-recap_files/att_00017.png" title="Matching Track model training pipeline. Reference and query image is concatenated to form as one image and ask ViT network to predict if query image used the reference image. Label is 1 if train image A is used as reference image and its augmented image is used as query image. Label is 0 if train image A is used as reference image and augmented version of other train image C is used as query image." class="img-fluid"></p>
<p><a href="https://arxiv.org/abs/2111.09113">arXiv</a> <a href="https://github.com/seungkee/2nd-place-solution-to-Facebook-Image-Similarity-Matching-Track">github</a></p>
</section>
<section id="rd-place-a-global-and-local-dual-retrieval-solution-to-facebook-ai-image-similarity-challenge" class="level3">
<h3 class="anchored" data-anchor-id="rd-place-a-global-and-local-dual-retrieval-solution-to-facebook-ai-image-similarity-challenge">3rd place: A Global and Local Dual Retrieval Solution to Facebook AI Image Similarity Challenge</h3>
<p>by Xinlong Sun, Yangyang Qin, Xuyuan Xu, Guoping Gong, Yang Fang, Yexin Wang</p>
<p>This method is probably the most hand-engineered.</p>
<p><img src="2022-02-05-DISC21-recap_files/att_00018.png" title="Overall pipeline, which consist of separate overlay-detection step, image retrieval with global EsViT and local descriptors - SIFT and finally their combination" class="img-fluid"></p>
<ol type="1">
<li>The images are pre-processed with overlay-detection – out-of-the-box YOLOv5. Detected overlays are treated as a separate images.</li>
<li>The global descriptor EsViT (Swin-B) is pretrained in self-supervised fashion on ImageNet, then with contrastive loss on the challange dataset, with hard-negative memory bank.</li>
</ol>
<p><img src="2022-02-05-DISC21-recap_files/att_00019.png" class="img-fluid"></p>
<ol start="3" type="1">
<li>Resize images to 300px -&gt; SIFT features (~600 per image) -&gt; 600M descriptors. Use brute-force search in faiss with fp16 precision. That takes 165Gb memory, fit with 8 V100 GPUs. Threshold the descriptor distance first, the number of matched features second.</li>
</ol>
<p>According to the ablation study, SIFT was significantly worse, than global EsViT descriptor in Phase I, but significantly boosted overall score in Phase 2.</p>
<p><img src="2022-02-05-DISC21-recap_files/att_00020.png" title="Ablation study. G-emb means global embedding, F means finetune, and P means using Preprocessing Module." class="img-fluid"></p>
<p><a href="https://arxiv.org/abs/2112.02373">arXiv</a>, <a href="https://github.com/sun-xl/ISC2021">github</a></p>
</section>
</section>
<section id="conclusion-my-speculations" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-my-speculations">Conclusion &amp; my speculations</h2>
<p>Edited image copy detection is a challenging task, which is, nevertheless solvable to some practical extent with existing techniques. It is interesting, the all 3 winners in descriptor track come up with a similar idea of contextualized database image embeddings with hard negatives.</p>
<p>I also don’t believe that the supervised/unsupervised pretraining and transformer/CNN architecture matter as much, as participants were reporting – precisely because different teams succesfully used different architectures and pretraining. Once one optimized the training procedure for the selected architecture, the differences would probably disappear.</p>
<p>The matching track pipelines are quite complex and computationally demanding. Nevertheless, using local features (or at least sub-image matching) noticably improves the score. My personal favorite is the 2nd place in the matching track for its originality and simplicity.</p>
<p>What would be interesting, is to check how much one looses when using visual-words tf-idf encoding for the local features matching, instead of keeping original descriptors.</p>
<p>Challenge organizers also announced the join paper with winners, overviewing the competition and the best approaches some when in February (i.e.&nbsp;soon). Stay tuned :)</p>


</section>

<p>Everything you (didn’t) want to know about image matching</p></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>