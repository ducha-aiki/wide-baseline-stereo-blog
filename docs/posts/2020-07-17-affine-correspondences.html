<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2020-07-17">
<meta name="description" content="Why to throw away an important information?">

<title>Local affine features: useful side product – Wide baseline stereo meets deep learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-6bd9cfa162949bde0a231f530c97869d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-GE2NZRSZBN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-GE2NZRSZBN', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Wide baseline stereo meets deep learning</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About Me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/ducha_aiki"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/ducha_aiki"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Local affine features: useful side product</h1>
                  <div>
        <div class="description">
          Why to throw away an important information?
        </div>
      </div>
                </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 17, 2020</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="keypoints-are-not-just-points" class="level2">
<h2 class="anchored" data-anchor-id="keypoints-are-not-just-points">Keypoints are not just points</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2020-07-17-affine-correspondences_files/att_00000.png" class="img-fluid figure-img"></p>
<figcaption>image.png</figcaption>
</figure>
</div>
<p>Wide baseline stereo matching often as perceived as establishing (key-)point correspondences between images. While this might be true for the some local features like SuperPoint , typically it is more than that.</p>
<p>Specifically, detectors like DoG, Harris, Hessian, KeyNet, ORB, and many others rate on scale-space provide at least 3 parameters: x, y, and scale.</p>
<p>Most of the local descriptors – SIFT, HardNet and so on – are not rotation invariant and those which are - mostly require complex matching function, , so the patch orientation has to be estimated anyway, in order to match reliably. This can be done by various methods: corners center of mass (ORB, dominant gradient orientation (SIFT) or by some learned estimator (OriNets,). Sometimes it is possible to rely on smartphone camera IMU or photographer and assume that images are upright.</p>
<p>Thus, we can assume that if local descriptors match, this means the local feature scale and orientation also match, at least approximately – see Figure below. Possible exceptions are cases, when the patch is symmetrical and orientation is ambiguous up to some symmetry group.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2020-07-17-affine-correspondences_files/matches_patches.png" class="img-fluid figure-img"></p>
<figcaption>Selected matching SIFT keypoints and corresponding patches. One can see that not only patch centers correspond to each other, but also other pixels, although less precise. Image pair from Sacre Coeur IMW dataset.</figcaption>
</figure>
</div>
<p>In addition, one could assume that we observe the patch not from the fronto-parallel position and try to estimate local normal, or, more precisely, affine shape of the feature point, modeling it as an ellipse instead of circle. One could also think of affine shape estimation as finding the camera position, from where the patch is seen in some “canonical” view.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2020-07-17-affine-correspondences_files/affinematches_patches.png" class="img-fluid figure-img"></p>
<figcaption>Selected matching SIFT-AffNet keypoints and corresponding patches. One can see that not only patch centers correspond to each other, but also other pixels, although less precise. Image pair from Sacre Coeur IMW dataset.</figcaption>
</figure>
</div>
<p>This gives us 3 points correspondences from a single local feature match, see an example in Figure below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2020-07-17-affine-correspondences_files/laf-check-illustration.png" class="img-fluid figure-img"></p>
<figcaption>Local affine correspondences. While centers of both regions A and B are correct point matches, only A is a correct affine correspondence.</figcaption>
</figure>
</div>
<p>Why is it important and how to use it – see in current post. How to esimate local affine features robustly – in the next post.</p>
</section>
<section id="benefits-of-local-affine-features" class="level1">
<h1>Benefits of local affine features</h1>
<section id="making-descriptor-job-easier" class="level2">
<h2 class="anchored" data-anchor-id="making-descriptor-job-easier">Making descriptor job easier</h2>
<p>The most straightforward benefit of using local affine features is that they increase the repeatability of the detector and potentially reduce appearance changes of a local patch caused by viewpoint difference. This makes possible matching more challenging image pairs.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2020-07-17-affine-correspondences_files/test.png" class="img-fluid figure-img"></p>
<figcaption>Hessian features + HardNet matches + RANSAC inliers, Right: HessAffNet features + HardNet matches + RANSAC inliers. Image pair from Tanks &amp; Temples. Epipolar lines are shown in cyan.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2020-07-17-affine-correspondences_files/att_00009.png" class="img-fluid figure-img"></p>
<figcaption>Repeatability and the number of correspondences. AffNet compared with the de facto standard Baumberg iteration according to the Mikolajczyk protocol. Left – images with illumination differences, right – with viewpoint and scale changes. SS – patch from the scale-space pyramid at the level of the detection, image – from the original image; 19 and 33 are patch sizes.</figcaption>
</figure>
</div>
<p>The practice is a little bit more complicated. Our recent benchmark, which measure the accuracy of the output fundamental matrix, shows that the difference in using affine and similarity-covariant features is quite minor. Specifically, the relative difference between SIFT vs SIFT-Affine features is 0.5% and between Hessian-SIFT and Hessian-AffNet SIFT is 5.1%, see Table below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2020-07-17-affine-correspondences_files/att_00011.png" class="img-fluid figure-img"></p>
<figcaption>Test – Stereo results with 8k features. We report: NF – Number of Features; NI – Number of Inliers produced by RANSAC; and mAA@10°.</figcaption>
</figure>
</div>
<p>Therefore, if the benefit of local features would be to only improve descriptor extraction stage, it would be arguably not worth it. Luckily, there are more benefits, which are more pronounced.</p>
</section>
<section id="making-ransac-job-easier" class="level2">
<h2 class="anchored" data-anchor-id="making-ransac-job-easier">Making RANSAC job easier</h2>
<p>Let’s recall how RANSAC works.</p>
<ol type="1">
<li>Randomly sample a minimally required number of tentative correspondences to fit the geometrical model of the scene: 4 for homography, 7 for epipolar geometry and estimate the model.</li>
<li>Calculate “support”: other correspondeces, which are consistent with the model.</li>
<li>Repeat steps (1), (2) and output the model which is supported with the most of correspondences. If you were lucky and have sampled all-inlier sample, meaning that all correspondences used to estimate the model were correct, you would have a correct model.</li>
</ol>
<p>Reality is more complicated than I have just described, but the principle is the same. The most important part is the sampling and it is sensitive to inlier ratio <span class="math inline">\(\nu\)</span> - the percentage of the correct correspondences in the set. Lets denote the minimal number of correspondences required to estimate the model as <strong>m</strong>. To recover the correct model with the confidence <strong>p</strong> one needs to sample the number of correspondences, which is described by formula:</p>
<p><span class="math display">\[\begin{equation}
N = \frac{\log{(1 - p)}}{\log{(1 - \nu^{m})}}
\end{equation}\]</span></p>
<p>Lets plot the how the number of required samples changes with inlier ratio for confidence equal 99%. Note the log scale on Y axis. Different lines are for different minimal sample size <strong>m</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2020-07-17-affine-correspondences_files/att_00016.png" class="img-fluid figure-img"></p>
<figcaption>Number of samples to find correct model as a function of inlier ratio.</figcaption>
</figure>
</div>
<p>As you can see from the plot above, reducing the minimal sample size required for the model estimation even by 1 saves and order of magnitude of computation. In reality the benefit is a smaller, as modern RANSACs like GC-RANSAC and MAGSAC could estimate the correct model from the sample containing outliers, but it is still huge, especially for low inlier rate cases.</p>
<section id="image-retrieval" class="level3">
<h3 class="anchored" data-anchor-id="image-retrieval">Image retrieval</h3>
<p>The ideal case would be to estimate a model from just a single sample and that is exactly what is done in spatial reranking paper “<strong>Object retrieval with large vocabularies and fast spatial matching</strong>” by Philbin et.al .</p>
<p>Specifically, they are solving a particular object retrieval problem: given an image containing some object, return all the images from the database, which also containg the same object.</p>
<p>The inital list of images is formed by the descriptor distance and then is reranked. The authors propose to approximate a perspective change between two images as an affine image transformation, and count number of feature points, which are reprojected inside the second image. This number produces better ranking that the original short-list.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2020-07-17-affine-correspondences_files/att_00012.png" class="img-fluid figure-img"></p>
<figcaption>Figure from Philbin et.al</figcaption>
</figure>
</div>
</section>
<section id="back-to-wide-baseline-stereo" class="level3">
<h3 class="anchored" data-anchor-id="back-to-wide-baseline-stereo">Back to wide baseline stereo</h3>
<p>While working for spatial re-ranking, 3-degrees of freedom camera model is too rough for the wide baseline stereo. Yet, going from 4 point correspondences (PC) to 2 affine correspondences (AC) for homogaphy and from 7 PC to 3 AC for the fundamental matrix would be huge benefit anyway for the robust model estimation.</p>
<p>Various variant of RANSAC working for local features were proposed in the last 15 years: Perdoch et.al, Pritts et.al., Barath and Kukelova , Rodríguez et.al.</p>
<p>Finally, the systematic study of using is presented by Barath et.al in “Making Affine Correspondences Work in Camera Geometry Computation” paper. Authors show that if used naively, affine correspondence lead to worse results, because they are more noisy than point correspondences. However, there is a bag of tricks presented in the paper, which allow to solve the noise issue and make the affine RANSAC working in practice, resulting in orders of magnitude faster computation.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2020-07-17-affine-correspondences_files/att_00013.png" class="img-fluid figure-img"></p>
<figcaption>Figure from Making Affine Correspondences Work in Camera Geometry Computation</figcaption>
</figure>
</div>
<p>Moreover, for a special cases like autonomous driving, where the motion is mostly horizonal, one could even use 2 affine correspondes for both motion estimation and consistency check, significantly improving the efficiency of the outliers removal compared to the standard RANSAC loop.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2020-07-17-affine-correspondences_files/att_00001.png" class="img-fluid figure-img"></p>
<figcaption>Empirical cumulative error distributions for KITTI sequence 00. Figure from Relative Pose Estimation for Multi-Camera Systems from Affine Correspondences.</figcaption>
</figure>
</div>
<p>Besides the special case considerations, additional contraints can also come from running other algorithms, like monocular depth estimation. Such a constraint could reduce the required number of matches from two affine correspondences to a single one for calibrated camera case.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2020-07-17-affine-correspondences_files/att_00002.png" class="img-fluid figure-img"></p>
<figcaption>Figure from Relative Pose from Deep Learned Depth and a Single Affine Correspondence</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2020-07-17-affine-correspondences_files/att_00003.png" class="img-fluid figure-img"></p>
<figcaption>Figure from Relative Pose from Deep Learned Depth and a Single Affine Correspondence</figcaption>
</figure>
</div>
</section>
</section>
<section id="application-specific-benefits" class="level2">
<h2 class="anchored" data-anchor-id="application-specific-benefits">Application-specific benefits</h2>
<p>Besides the wide baseline stereo, local affine features and correspondences have other applications. I will briefly describe some of them here (to be updated).</p>
<section id="image-rectification" class="level3">
<h3 class="anchored" data-anchor-id="image-rectification">Image rectification</h3>
<p>Instead of matching local features between two images one might match them within a single image. Why would someone do it? This allows finding repeated pattern: think about windows, doors and so on. Typically they have the same physical size, therefore the diffrence in local features around them could tell us about the geometry of the scene and lens distortion.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2020-07-17-affine-correspondences_files/att_00008.png" class="img-fluid figure-img"></p>
<figcaption>Repeated patterns detection with MSER andRootSIFT local features. Figure from the Minimal Solvers for Rectifying from Radially-Distorted Conjugate Translations, PAMI 2020 paper.</figcaption>
</figure>
</div>
<p>This is the idea of the series of works by Pritts and Chum.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2020-07-17-affine-correspondences_files/att_00007.png" class="img-fluid figure-img"></p>
<figcaption>Figure from the ‘Minimal Solvers for Rectifying from Radially-Distorted Conjugate Translations’, PAMI 2020 paper.</figcaption>
</figure>
</div>
</section>
<section id="surface-normals-estimation" class="level3">
<h3 class="anchored" data-anchor-id="surface-normals-estimation">Surface normals estimation</h3>
<p>Ivan Eichhardt and Levente Hajder have a series of works, exploiting the local affine correspondences for surface normals estimation</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2020-07-17-affine-correspondences_files/att_00015.png" class="img-fluid figure-img"></p>
<figcaption>Estimated surface normals. Figure from Optimal Multi-View Surface Normal Estimation Using Affine Correspondences</figcaption>
</figure>
</div>
</section>
</section>
</section>
<section id="summary" class="level1">
<h1>Summary</h1>
<p>Despite not being popular right now, treating keypoints as local affine features has a lot of advantages over the traditional treatment the local correspondence as the point correspondences. In the next post I will describe a way of estimating the local feature affine shape and orientation.</p>
</section>
<section id="references" class="level1">
<h1>References</h1>
<p>[<a id="cit-SuperPoint2017" href="#call-SuperPoint2017">SuperPoint2017</a>] Detone D., Malisiewicz T. and Rabinovich A., ``<em>Superpoint: Self-Supervised Interest Point Detection and Description</em>’’, CVPRW Deep Learning for Visual SLAM, vol.&nbsp;, number , pp.&nbsp;, 2018.</p>
<p>[<a id="cit-Lowe99" href="#call-Lowe99">Lowe99</a>] D. Lowe, ``<em>Object Recognition from Local Scale-Invariant Features</em>’’, ICCV, 1999.</p>
<p>[<a id="cit-Harris88" href="#call-Harris88">Harris88</a>] C. Harris and M. Stephens, ``<em>A Combined Corner and Edge Detector</em>’’, Fourth Alvey Vision Conference, 1988.</p>
<p>[<a id="cit-Hessian78" href="#call-Hessian78">Hessian78</a>] P.R. Beaudet, ``<em>Rotationally invariant image operators</em>’’, Proceedings of the 4th International Joint Conference on Pattern Recognition, 1978.</p>
<p>[<a id="cit-KeyNet2019" href="#call-KeyNet2019">KeyNet2019</a>] A. Barroso-Laguna, E. Riba, D. Ponsa <em>et al.</em>, ``<em>Key.Net: Keypoint Detection by Handcrafted and Learned CNN Filters</em>’’, ICCV, 2019.</p>
<p>[<a id="cit-ORB2011" href="#call-ORB2011">ORB2011</a>] E. Rublee, V. Rabaud, K. Konolidge <em>et al.</em>, ``<em>ORB: An Efficient Alternative to SIFT or SURF</em>’’, ICCV, 2011.</p>
<p>[<a id="cit-HardNet2017" href="#call-HardNet2017">HardNet2017</a>] A. Mishchuk, D. Mishkin, F. Radenovic <em>et al.</em>, ``<em>Working Hard to Know Your Neighbor’s Margins: Local Descriptor Learning Loss</em>’’, NeurIPS, 2017.</p>
<p>[<a id="cit-RIFT2005" href="#call-RIFT2005">RIFT2005</a>] {Lazebnik} S., {Schmid} C. and {Ponce} J., ``<em>A sparse texture representation using local affine regions</em>’’, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol.&nbsp;27, number 8, pp.&nbsp;1265-1278, 2005.</p>
<p>[<a id="cit-sGLOH2" href="#call-sGLOH2">sGLOH2</a>] {Bellavia} F. and {Colombo} C., ``<em>Rethinking the sGLOH Descriptor</em>’’, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol.&nbsp;40, number 4, pp.&nbsp;931-944, 2018.</p>
<p>[<a id="cit-OriNet2016" href="#call-OriNet2016">OriNet2016</a>] K. M., Y. Verdie, P. Fua <em>et al.</em>, ``<em>Learning to Assign Orientations to Feature Points</em>’’, CVPR, 2016.</p>
<p>[<a id="cit-AffNet2018" href="#call-AffNet2018">AffNet2018</a>] D. Mishkin, F. Radenovic and J. Matas, ``<em>Repeatability is Not Enough: Learning Affine Regions via Discriminability</em>’’, ECCV, 2018.</p>
<p>[<a id="cit-PerdochRetrieval2009" href="#call-PerdochRetrieval2009">PerdochRetrieval2009</a>] M. {Perd’och}, O. {Chum} and J. {Matas}, ``<em>Efficient representation of local geometry for large scale object retrieval</em>’’, CVPR, 2009.</p>
<p>[<a id="cit-IMW2020" href="#call-IMW2020">IMW2020</a>] Jin Yuhe, Mishkin Dmytro, Mishchuk Anastasiia <em>et al.</em>, ``<em>Image Matching across Wide Baselines: From Paper to Practice</em>’’, arXiv preprint arXiv:2003.01587, vol.&nbsp;, number , pp.&nbsp;, 2020.</p>
<p>[<a id="cit-gcransac2018" href="#call-gcransac2018">gcransac2018</a>] D. Barath and J. Matas, ``<em>Graph-Cut RANSAC</em>’’, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.</p>
<p>[<a id="cit-magsac2019" href="#call-magsac2019">magsac2019</a>] J.N. Daniel Barath, ``<em>MAGSAC: marginalizing sample consensus</em>’’, CVPR, 2019.</p>
<p>[<a id="cit-Philbin07" href="#call-Philbin07">Philbin07</a>] J. Philbin, O. Chum, M. Isard <em>et al.</em>, ``<em>Object Retrieval with Large Vocabularies and Fast Spatial Matching</em>’’, CVPR, 2007.</p>
<p>[<a id="cit-perd2006epipolar" href="#call-perd2006epipolar">perd2006epipolar</a>] M. Perd’och, J. Matas and O. Chum, ``<em>Epipolar geometry from two correspondences</em>’’, ICPR, 2006.</p>
<p>[<a id="cit-PrittsRANSAC2013" href="#call-PrittsRANSAC2013">PrittsRANSAC2013</a>] J. {Pritts}, O. {Chum} and J. {Matas}, ``<em>Approximate models for fast and accurate epipolar geometry estimation</em>’’, 2013 28th International Conference on Image and Vision Computing New Zealand (IVCNZ 2013), 2013.</p>
<p>[<a id="cit-Barath2019ICCV" href="#call-Barath2019ICCV">Barath2019ICCV</a>] D. Barath and Z. Kukelova, ``<em>Homography From Two Orientation- and Scale-Covariant Features</em>’’, ICCV, 2019.</p>
<p>[<a id="cit-RANSACAffine2020" href="#call-RANSACAffine2020">RANSACAffine2020</a>] M. {Rodríguez}, G. {Facciolo}, R. G. <em>et al.</em>, ``<em>Robust estimation of local affine maps and its applications to image matching</em>’’, 2020 IEEE Winter Conference on Applications of Computer Vision (WACV), 2020.</p>
<p>[<a id="cit-barath2020making" href="#call-barath2020making">barath2020making</a>] Barath Daniel, Polic Michal, Förstner Wolfgang <em>et al.</em>, ``<em>Making Affine Correspondences Work in Camera Geometry Computation</em>’’, arXiv preprint arXiv:2007.10032, vol.&nbsp;, number , pp.&nbsp;, 2020.</p>
<p>[<a id="cit-guan2020relative" href="#call-guan2020relative">guan2020relative</a>] Guan Banglei, Zhao Ji, Barath Daniel <em>et al.</em>, ``<em>Relative Pose Estimation for Multi-Camera Systems from Affine Correspondences</em>’’, arXiv preprint arXiv:2007.10700, vol.&nbsp;, number , pp.&nbsp;, 2020.</p>
<p>[<a id="cit-OneACMonoDepth2020" href="#call-OneACMonoDepth2020">OneACMonoDepth2020</a>] D.B. Ivan Eichhardt, ``<em>Relative Pose from Deep Learned Depth and a Single Affine Correspondence</em>’’, ECCV, 2020.</p>
<p>[<a id="cit-SurfaceNormals2019" href="#call-SurfaceNormals2019">SurfaceNormals2019</a>] {Baráth} D., {Eichhardt} I. and {Hajder} L., ``<em>Optimal Multi-View Surface Normal Estimation Using Affine Correspondences</em>’’, IEEE Transactions on Image Processing, vol.&nbsp;28, number 7, pp.&nbsp;3301-3311, 2019.</p>


</section>

<p>Everything you (didn’t) want to know about image matching</p></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>