[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hi! My name is Dmytro Mishkin, I am computer vision researcher and consultant. This is blog series was originally based on my PhD thesis, and now just continuing on the same topic. My personal website is here. The best way to contact me is email and Twitter.\n\n\nEverything you (didn’t) want to know about image matching"
  },
  {
    "objectID": "posts/2021-05-14-imc2020-competition-recap.html",
    "href": "posts/2021-05-14-imc2020-competition-recap.html",
    "title": "Image Matching Challenge 2020 Recap",
    "section": "",
    "text": "Image Matching Challenge is an on-going benchmark of the local features, matching methods and RANSACs, held since 2019. Its main idea is to measure a downstream metric of the image matching pipeline, such as camera pose accuracy after a careful hyperparameter tuning.\n\n\n\nimage.png\n\n\nBefore the challenge we have benchmarked existing popular and recent methods, such as (Root)SIFT, ORB, HardNet, SuperPoint, R2D2, D2Net, etc, and published a paper called “Image matching across wide baselines: From paper to practice”.\nThe best results were obtained by a combinination of difference-of-Gaussians (DoG) local feature detector, commonly referred as SIFT detector, with deep learned patch descriptor such as HardNet, SOSNet, or AffNet-HardNet.\nLet’s check what was proposed by the challenge participants.\nEverything you (didn’t) want to know about image matching"
  },
  {
    "objectID": "posts/2021-05-14-imc2020-competition-recap.html#what-is-image-matching-challenge",
    "href": "posts/2021-05-14-imc2020-competition-recap.html#what-is-image-matching-challenge",
    "title": "Image Matching Challenge 2020 Recap",
    "section": "",
    "text": "Image Matching Challenge is an on-going benchmark of the local features, matching methods and RANSACs, held since 2019. Its main idea is to measure a downstream metric of the image matching pipeline, such as camera pose accuracy after a careful hyperparameter tuning.\n\n\n\nimage.png\n\n\nBefore the challenge we have benchmarked existing popular and recent methods, such as (Root)SIFT, ORB, HardNet, SuperPoint, R2D2, D2Net, etc, and published a paper called “Image matching across wide baselines: From paper to practice”.\nThe best results were obtained by a combinination of difference-of-Gaussians (DoG) local feature detector, commonly referred as SIFT detector, with deep learned patch descriptor such as HardNet, SOSNet, or AffNet-HardNet.\nLet’s check what was proposed by the challenge participants."
  },
  {
    "objectID": "posts/2021-05-14-imc2020-competition-recap.html#top-solutions-2020-and-follow-ups",
    "href": "posts/2021-05-14-imc2020-competition-recap.html#top-solutions-2020-and-follow-ups",
    "title": "Image Matching Challenge 2020 Recap",
    "section": "Top solutions-2020 and follow-ups",
    "text": "Top solutions-2020 and follow-ups\n\nSuperGlue\n\nSuperGlue is an attention-based graph neural network for matching local features, taking into account both geometry (keypoint location) and appearance (descriptor). Unlike previous works, e.g. CNe, or OANet below, it does not “scoring-and-cleaning” already established tentative correspondences. Instead, it establishes the correpondences given the local features from two images. SuperGlue won IMC-2020, as well as two other competitions at CVPR 2020. Its inference implementation is available here.\n\n\nOANet\n OANet is an specialized neural network architecture for “scoring-and-cleaning” already established tentative correspondences. OANet was run on top of DoG-HardNet local features. Pytorch version of OANet is available here\n\n\nAdaLAM\n\nAdaLAM is a handcrafted algorithm for tentative correspondence cleaning, which work comparably or even better than a learning-based approaches. It is based on two core assumptions:\n\nKeypoints, which are near from each other are probably corresponding to the neaighboring keypoints in the other images\nIf keypoints are in correspondence, it means that their orientation and scale are also in correspondence. Check my post Local affine features: useful side product for an explanation\n\nThe implementation is avilable here. As DoG+HardNet were used as a local features.\n\n\nDISK\n\nDISK is local feature, which has two main differences from the rest of competitors (SuperPoint, R2D2):\n\nDISK is trained with a reinforcement-learning objective\nDISK has UNet-like architecture, unlike VGG-style for the rest of the features.\n\nIts implementation is available here.\n\n\nHyNet\n HyNet is an next stage in the L2Net-HardNet-SOSNet series of local patch descriptors. It is different from the previous works in two ways:\n\nBatchNorm and ReLU in the HardNet architecture are replaced with FRN and TLU respecively.\nDuring training, distance to the negative (non-matching) \\(d(\\theta)\\) and positive (matching) \\(s(\\theta)\\) samples are calculated in a different way for better learning, see image below. u and v denote descriptors is\n\n\nHyNet submission also used semantic segmentation network to remove the keypoints from the non-matchable areas, such as sky and water.\n\n\nHardNet8\n\nHardNet8 is another improvement of the HardNet architecture:\n\nDeeper and wider network\nThe output is compressed with a PCA.\nThe training set and hyperparameters are carefully selected.\n\nIt is available in kornia"
  },
  {
    "objectID": "posts/2021-05-14-imc2020-competition-recap.html#challenge",
    "href": "posts/2021-05-14-imc2020-competition-recap.html#challenge",
    "title": "Image Matching Challenge 2020 Recap",
    "section": "2021 challenge",
    "text": "2021 challenge\nThis year challenge brings 2 new datasets: PragueParks and GoogleUrban.\n\nThe PragueParks dataset\n\nThe PragueParks dataset contains images from video sequences captured by the organizers with an iPhone 11, in 2021. The iPhone 11 has two cameras, with normal and wide lenses, both of which were used. Note that while the video is high quality, some of the frames suffer from motion blur. These videos were then processed by the commercial 3D reconstruction software RealityCapture, which is orders of magnitude faster than COLMAP, while delivering a comparable output in terms of accuracy. Similarly to we did for the “PhotoTourism” dataset, this data is then subsampled in order to generate the subsets used for evaluation.\nThe dataset contains small-scale scenes like tree, pond, wooden and metal sculptures with different level of zoom, lots of vegetation, and no people. The distribution of its camera poses differs from Phototourism.\n\n\nThe GoogleUrban dataset\n\nThe GoogleUrban dataset contains images used by Google to evaluate localization algorithms, such as those in Google’s Visual Positioning System, which powers Live View on millions on mobile devices. They are obtained from videos collected from different cell phones, on many countries all over the world, often years apart. They contain poses, but not depth maps. Please note that due to legal reasons, this data is released with a restricted license, and must be deleted by the end of the challenge."
  },
  {
    "objectID": "posts/2021-05-14-imc2020-competition-recap.html#submit-your-solution",
    "href": "posts/2021-05-14-imc2020-competition-recap.html#submit-your-solution",
    "title": "Image Matching Challenge 2020 Recap",
    "section": "Submit your solution!",
    "text": "Submit your solution!\nYou can check out the tutorial on how to submit to IMC 2021 in the post Submitting to Image Matching Challenge 2021. Good luck!"
  },
  {
    "objectID": "posts/2020-07-17-affine-correspondences.html",
    "href": "posts/2020-07-17-affine-correspondences.html",
    "title": "Local affine features: useful side product",
    "section": "",
    "text": "image.png\n\n\nWide baseline stereo matching often as perceived as establishing (key-)point correspondences between images. While this might be true for the some local features like SuperPoint , typically it is more than that.\nSpecifically, detectors like DoG, Harris, Hessian, KeyNet, ORB, and many others rate on scale-space provide at least 3 parameters: x, y, and scale.\nMost of the local descriptors – SIFT, HardNet and so on – are not rotation invariant and those which are - mostly require complex matching function, , so the patch orientation has to be estimated anyway, in order to match reliably. This can be done by various methods: corners center of mass (ORB, dominant gradient orientation (SIFT) or by some learned estimator (OriNets,). Sometimes it is possible to rely on smartphone camera IMU or photographer and assume that images are upright.\nThus, we can assume that if local descriptors match, this means the local feature scale and orientation also match, at least approximately – see Figure below. Possible exceptions are cases, when the patch is symmetrical and orientation is ambiguous up to some symmetry group.\n\nIn addition, one could assume that we observe the patch not from the fronto-parallel position and try to estimate local normal, or, more precisely, affine shape of the feature point, modeling it as an ellipse instead of circle. One could also think of affine shape estimation as finding the camera position, from where the patch is seen in some “canonical” view.\n\nThis gives us 3 points correspondences from a single local feature match, see an example in Figure below.\n\nWhy is it important and how to use it – see in current post. How to esimate local affine features robustly – in the next post.\nEverything you (didn’t) want to know about image matching"
  },
  {
    "objectID": "posts/2020-07-17-affine-correspondences.html#keypoints-are-not-just-points",
    "href": "posts/2020-07-17-affine-correspondences.html#keypoints-are-not-just-points",
    "title": "Local affine features: useful side product",
    "section": "",
    "text": "image.png\n\n\nWide baseline stereo matching often as perceived as establishing (key-)point correspondences between images. While this might be true for the some local features like SuperPoint , typically it is more than that.\nSpecifically, detectors like DoG, Harris, Hessian, KeyNet, ORB, and many others rate on scale-space provide at least 3 parameters: x, y, and scale.\nMost of the local descriptors – SIFT, HardNet and so on – are not rotation invariant and those which are - mostly require complex matching function, , so the patch orientation has to be estimated anyway, in order to match reliably. This can be done by various methods: corners center of mass (ORB, dominant gradient orientation (SIFT) or by some learned estimator (OriNets,). Sometimes it is possible to rely on smartphone camera IMU or photographer and assume that images are upright.\nThus, we can assume that if local descriptors match, this means the local feature scale and orientation also match, at least approximately – see Figure below. Possible exceptions are cases, when the patch is symmetrical and orientation is ambiguous up to some symmetry group.\n\nIn addition, one could assume that we observe the patch not from the fronto-parallel position and try to estimate local normal, or, more precisely, affine shape of the feature point, modeling it as an ellipse instead of circle. One could also think of affine shape estimation as finding the camera position, from where the patch is seen in some “canonical” view.\n\nThis gives us 3 points correspondences from a single local feature match, see an example in Figure below.\n\nWhy is it important and how to use it – see in current post. How to esimate local affine features robustly – in the next post."
  },
  {
    "objectID": "posts/2020-07-17-affine-correspondences.html#making-descriptor-job-easier",
    "href": "posts/2020-07-17-affine-correspondences.html#making-descriptor-job-easier",
    "title": "Local affine features: useful side product",
    "section": "Making descriptor job easier",
    "text": "Making descriptor job easier\nThe most straightforward benefit of using local affine features is that they increase the repeatability of the detector and potentially reduce appearance changes of a local patch caused by viewpoint difference. This makes possible matching more challenging image pairs.\n\n\nThe practice is a little bit more complicated. Our recent benchmark, which measure the accuracy of the output fundamental matrix, shows that the difference in using affine and similarity-covariant features is quite minor. Specifically, the relative difference between SIFT vs SIFT-Affine features is 0.5% and between Hessian-SIFT and Hessian-AffNet SIFT is 5.1%, see Table below.\n\nTherefore, if the benefit of local features would be to only improve descriptor extraction stage, it would be arguably not worth it. Luckily, there are more benefits, which are more pronounced."
  },
  {
    "objectID": "posts/2020-07-17-affine-correspondences.html#making-ransac-job-easier",
    "href": "posts/2020-07-17-affine-correspondences.html#making-ransac-job-easier",
    "title": "Local affine features: useful side product",
    "section": "Making RANSAC job easier",
    "text": "Making RANSAC job easier\nLet’s recall how RANSAC works.\n\nRandomly sample a minimally required number of tentative correspondences to fit the geometrical model of the scene: 4 for homography, 7 for epipolar geometry and estimate the model.\nCalculate “support”: other correspondeces, which are consistent with the model.\nRepeat steps (1), (2) and output the model which is supported with the most of correspondences. If you were lucky and have sampled all-inlier sample, meaning that all correspondences used to estimate the model were correct, you would have a correct model.\n\nReality is more complicated than I have just described, but the principle is the same. The most important part is the sampling and it is sensitive to inlier ratio \\(\\nu\\) - the percentage of the correct correspondences in the set. Lets denote the minimal number of correspondences required to estimate the model as m. To recover the correct model with the confidence p one needs to sample the number of correspondences, which is described by formula:\n\\[\\begin{equation}\nN = \\frac{\\log{(1 - p)}}{\\log{(1 - \\nu^{m})}}\n\\end{equation}\\]\nLets plot the how the number of required samples changes with inlier ratio for confidence equal 99%. Note the log scale on Y axis. Different lines are for different minimal sample size m.\n\nAs you can see from the plot above, reducing the minimal sample size required for the model estimation even by 1 saves and order of magnitude of computation. In reality the benefit is a smaller, as modern RANSACs like GC-RANSAC and MAGSAC could estimate the correct model from the sample containing outliers, but it is still huge, especially for low inlier rate cases.\n\nImage retrieval\nThe ideal case would be to estimate a model from just a single sample and that is exactly what is done in spatial reranking paper “Object retrieval with large vocabularies and fast spatial matching” by Philbin et.al .\nSpecifically, they are solving a particular object retrieval problem: given an image containing some object, return all the images from the database, which also containg the same object.\nThe inital list of images is formed by the descriptor distance and then is reranked. The authors propose to approximate a perspective change between two images as an affine image transformation, and count number of feature points, which are reprojected inside the second image. This number produces better ranking that the original short-list.\n\n\n\nBack to wide baseline stereo\nWhile working for spatial re-ranking, 3-degrees of freedom camera model is too rough for the wide baseline stereo. Yet, going from 4 point correspondences (PC) to 2 affine correspondences (AC) for homogaphy and from 7 PC to 3 AC for the fundamental matrix would be huge benefit anyway for the robust model estimation.\nVarious variant of RANSAC working for local features were proposed in the last 15 years: Perdoch et.al, Pritts et.al., Barath and Kukelova , Rodríguez et.al.\nFinally, the systematic study of using is presented by Barath et.al in “Making Affine Correspondences Work in Camera Geometry Computation” paper. Authors show that if used naively, affine correspondence lead to worse results, because they are more noisy than point correspondences. However, there is a bag of tricks presented in the paper, which allow to solve the noise issue and make the affine RANSAC working in practice, resulting in orders of magnitude faster computation.\n\nMoreover, for a special cases like autonomous driving, where the motion is mostly horizonal, one could even use 2 affine correspondes for both motion estimation and consistency check, significantly improving the efficiency of the outliers removal compared to the standard RANSAC loop.\n\nBesides the special case considerations, additional contraints can also come from running other algorithms, like monocular depth estimation. Such a constraint could reduce the required number of matches from two affine correspondences to a single one for calibrated camera case."
  },
  {
    "objectID": "posts/2020-07-17-affine-correspondences.html#application-specific-benefits",
    "href": "posts/2020-07-17-affine-correspondences.html#application-specific-benefits",
    "title": "Local affine features: useful side product",
    "section": "Application-specific benefits",
    "text": "Application-specific benefits\nBesides the wide baseline stereo, local affine features and correspondences have other applications. I will briefly describe some of them here (to be updated).\n\nImage rectification\nInstead of matching local features between two images one might match them within a single image. Why would someone do it? This allows finding repeated pattern: think about windows, doors and so on. Typically they have the same physical size, therefore the diffrence in local features around them could tell us about the geometry of the scene and lens distortion.\n\nThis is the idea of the series of works by Pritts and Chum.\n\n\n\nSurface normals estimation\nIvan Eichhardt and Levente Hajder have a series of works, exploiting the local affine correspondences for surface normals estimation"
  },
  {
    "objectID": "posts/2020-09-23-local-descriptors-validation.html",
    "href": "posts/2020-09-23-local-descriptors-validation.html",
    "title": "Revisiting Brown patch dataset and benchmark",
    "section": "",
    "text": "Why one needs good development set? What is wrong with existing sets for local patch descriptor learning?\nOne should validate in the same way, as it is used in production.\nBrown patch revisited – implementation details\nLocal patch descriptors evaluation results.\nEverything you (didn’t) want to know about image matching"
  },
  {
    "objectID": "posts/2020-09-23-local-descriptors-validation.html#really-quick-intro-into-local-patch-descriptors",
    "href": "posts/2020-09-23-local-descriptors-validation.html#really-quick-intro-into-local-patch-descriptors",
    "title": "Revisiting Brown patch dataset and benchmark",
    "section": "Really quick intro into local patch descriptors",
    "text": "Really quick intro into local patch descriptors\nLocal patch descriptor is the thing, which helps you to automatically decide, if two patches in the pair of images correspond to the same point in a real world, or not. It should be robust to illumination, viewpoint and other changes.\n\nThere are lots of ways how to implement a local patch descriptor: engineered and learned. Local patch descriptor is the crucial component of the wide baseline stereo pipeline and a popular computer vision research topic."
  },
  {
    "objectID": "posts/2020-09-23-local-descriptors-validation.html#why-do-you-need-development-set",
    "href": "posts/2020-09-23-local-descriptors-validation.html#why-do-you-need-development-set",
    "title": "Revisiting Brown patch dataset and benchmark",
    "section": "Why do you need development set?",
    "text": "Why do you need development set?\nGood data is crucial for any machine learning problem – everyone now knows that. One needs high quality training set for training a good model. One also needs good test set, to know, what is real performance. However, there is one more, often forgotten, crucial component – validation or development set. We use it to decide hyperparameters and validate design choices we make. It should be different from both training and test sets, yet, be good predictor of test set performance. Moreover, it should allow fast iterations, so be not too small.\nWhile such set is commonly called validation set, I do like Andrew Ng’s term “development” set more - because it helps to develop your model."
  },
  {
    "objectID": "posts/2020-09-23-local-descriptors-validation.html#designing-the-evaluation-protocol",
    "href": "posts/2020-09-23-local-descriptors-validation.html#designing-the-evaluation-protocol",
    "title": "Revisiting Brown patch dataset and benchmark",
    "section": "Designing the evaluation protocol",
    "text": "Designing the evaluation protocol\nClassical local descriptor matching consists of two parts: finding nearest neighbors and filtering unreliable ones based on some criterion. I have wrote a blogpost, describing the matching strategies in details.\nThe most used in practice criterion is the first to second nearest neighbor distance (Lowe’s) ratio threshold for filtering false positive matches. It is shown in the figure below.\nThe intuition is simple: if two candidates are too similar, then the match is unreliable and it is better to drop it.\n\nSomehow, none of the local patch evaluation protocols does not take such filtering criterion in mind, although it greatly influences the overall performance.\nSo, let’s do the following:\n\nTake the patches, which are extracted from only two images.\nFor the each patch, calculate the descriptor distance to the correct match and to the hardnest (closest) non-match. Calculate the Lowe’s ratio between this two.\nCalculate accuracy for each of such triplets. If the correct match has smaller distance, score 1, if not - 0.\nSort the ratios from smallest to biggest and calculate mean average precision (mAP)."
  },
  {
    "objectID": "posts/2020-09-23-local-descriptors-validation.html#brown-phototour-revisied-implementation-details",
    "href": "posts/2020-09-23-local-descriptors-validation.html#brown-phototour-revisied-implementation-details",
    "title": "Revisiting Brown patch dataset and benchmark",
    "section": "Brown PhotoTour Revisied: implementation details",
    "text": "Brown PhotoTour Revisied: implementation details\nWe have designed the protocol, now time for data. We could spend several month collecting and cleaning it…or we can just re-use great Brown PhotoTourism dataset. Re-visiting labeling and/or evaluation protocol of the time-tested dataset is a great idea.\nJust couple of examples: ImageNette created by Jeremy Howard from ImageNet, Revisited Oxford 5k by Filip Radenovic and so on.\nFor the protocol we designed above we need the information about the image id, where the patch was extracted from. Unfortunately, there is no such information in the Brown PhotoTourism, but there is suitable alternative – the image id, where the reference patch was detected. What does it mean?\nSuppose, we have 4 images and 5 keypoints. All the keypoints present in all images, which gives us 20 patches. 3 keypoints were first detected in Image 1 and 2 in image 2.\nThat means that we will have 12 patches labeled image 1 and 8 patches labeled image 2.\nSo, we will have results for image 1 and image 2. Let’s consider image 1. There are 12 patches, splitted in 3 “classes”, 4 patches in each class.\nThen, for the each of those 12 patches we:\n\npick each of the corresponding patched as positives, so 3 positives. \\(P_1\\), \\(P_2\\), \\(P_3\\)\nfind the closest negative N.\nadd triplets (A, \\(P_1\\), N), (A, \\(P_2\\), N), (A, \\(P_3\\), N) to the evaluation.\n\nRepeat the same for the image 2. That mimics the two-view matching process as close, as possible, given the data available to us."
  },
  {
    "objectID": "posts/2020-09-23-local-descriptors-validation.html#installation",
    "href": "posts/2020-09-23-local-descriptors-validation.html#installation",
    "title": "Revisiting Brown patch dataset and benchmark",
    "section": "Installation",
    "text": "Installation\npip install brown_phototour_revisited"
  },
  {
    "objectID": "posts/2020-09-23-local-descriptors-validation.html#how-to-use",
    "href": "posts/2020-09-23-local-descriptors-validation.html#how-to-use",
    "title": "Revisiting Brown patch dataset and benchmark",
    "section": "How to use",
    "text": "How to use\nThere is a single function, which does everything for you: full_evaluation. The original Brown benchmark consider evaluation, similar to cross-validation: train descriptor on one subset, evaluate on two others, repeat for all, so 6 evaluations are required. For the handcrafted descriptors, or those, that are trained on 3rd party datasets, only 3 evaluations are necessary. We are following it here as well.\nHowever, if you need to run some tests separately, or reuse some functions – we will cover the usage below. In the following example we will show how to use full_evaluation to evaluate SIFT descriptor as implemented in kornia.\n# !pip install kornia\nimport torch\nimport kornia\nfrom IPython.display import clear_output\nfrom brown_phototour_revisited.benchmarking import *\npatch_size = 65 \n\nmodel = kornia.feature.SIFTDescriptor(patch_size, rootsift=True).eval()\n\ndescs_out_dir = 'data/descriptors'\ndownload_dataset_to = 'data/dataset'\nresults_dir = 'data/mAP'\n\nresults_dict = {}\nresults_dict['Kornia RootSIFT'] = full_evaluation(model,\n                                'Kornia RootSIFT',\n                                path_to_save_dataset = download_dataset_to,\n                                path_to_save_descriptors = descs_out_dir,\n                                path_to_save_mAP = results_dir,\n                                patch_size = patch_size, \n                                device = torch.device('cuda:0'), \n                           distance='euclidean',\n                           backend='pytorch-cuda')\nclear_output()\nprint_results_table(results_dict)\n------------------------------------------------------------------------------\nMean Average Precision wrt Lowe SNN ratio criterion on UBC Phototour Revisited\n------------------------------------------------------------------------------\ntrained on       liberty notredame  liberty yosemite  notredame yosemite\ntested  on           yosemite           notredame            liberty\n------------------------------------------------------------------------------\nKornia RootSIFT        56.70              47.71               48.09 \n------------------------------------------------------------------------------"
  },
  {
    "objectID": "posts/2020-09-23-local-descriptors-validation.html#results",
    "href": "posts/2020-09-23-local-descriptors-validation.html#results",
    "title": "Revisiting Brown patch dataset and benchmark",
    "section": "Results",
    "text": "Results\nSo, let’s check how it goes. The latest results and implementation are in the following notebooks:\n\nDeep descriptors\nNon-deep descriptors\n\nThe results are the following:\n------------------------------------------------------------------------------\nMean Average Precision wrt Lowe SNN ratio criterion on UBC Phototour Revisited\n------------------------------------------------------------------------------\ntrained on       liberty notredame  liberty yosemite  notredame yosemite\ntested  on           yosemite           notredame            liberty\n------------------------------------------------------------------------------\nKornia RootSIFT 32px   58.24              49.07               49.65 \nHardNet 32px       70.64  70.31        61.93  59.56        63.06  61.64\nSOSNet 32px        70.03  70.19        62.09  59.68        63.16  61.65\nTFeat 32px         65.45  65.77        54.99  54.69        56.55  56.24\nSoftMargin 32px    69.29  69.20        61.82  58.61        62.37  60.63\nHardNetPS 32px         55.56              49.70               49.12 \nR2D2_center_grayscal   61.47              53.18               54.98 \nR2D2_MeanCenter_gray   62.73              54.10               56.17 \n------------------------------------------------------------------------------\n\n------------------------------------------------------------------------------\nMean Average Precision wrt Lowe SNN ratio criterion on UBC Phototour Revisited\n------------------------------------------------------------------------------\ntrained on       liberty notredame  liberty yosemite  notredame yosemite\ntested  on           yosemite           notredame            liberty\n------------------------------------------------------------------------------\nKornia SIFT 32px       58.47              47.76               48.70 \nOpenCV_SIFT 32px       53.16              45.93               46.00 \nKornia RootSIFT 32px   58.24              49.07               49.65 \nOpenCV_RootSIFT 32px   53.50              47.16               47.37 \nOpenCV_LATCH 65px  -----  -----        -----  37.26        -----  39.08\nOpenCV_LUCID 32px      20.37              23.08               27.24 \nskimage_BRIEF 65px     52.68              44.82               46.56 \nKornia RootSIFTPCA 3 60.73  60.64        50.80  50.24        52.46  52.02\nMKD-concat-lw-32 32p 72.27  71.95        60.88  58.78        60.68  59.10\n------------------------------------------------------------------------------\nSo far - in agreement with IMC benchmark: SIFT and RootSIFT are good, but not the best, SOSNet and HardNet are the leaders, but within tens of percents, not by orders of magnitude.\n\n\nDisclaimer 1: don’t trust this tables fully\nI haven’t (yet!) checked if all the deep descriptors models, trained on Brown, were trained with flip-rotation 90 degrees augmentation. In the code below I assume that they were, however, it might not be true – and the comparison might not be completely fair. I will do my best to check it, but if you know that I have used wrong weights - please open an issue. Thank you.\n\n\nDisclaimer 2: it is not “benchmark”.\nThe intended usage of the package is not to test and report the numbers in the paper. Instead think about is as cross-validation tool, helping the development. Thus, one CAN tune hyperparameters based on the benchmark results instead of doing so on HPatches. After you have finished tuning, please, evaluate your local descriptors on some downstream task like IMC image matching benchmark or visual localization."
  },
  {
    "objectID": "posts/2020-09-23-local-descriptors-validation.html#summary",
    "href": "posts/2020-09-23-local-descriptors-validation.html#summary",
    "title": "Revisiting Brown patch dataset and benchmark",
    "section": "Summary",
    "text": "Summary\nIt really pays off, to spend time designing a proper evaluation pipeline and gathering the data for it. If you can re-use existing work - great. But don’t blindly trust anything, even super-popular and widely adopted benchmarks. You need always check if the the protocol and data makes sense for your use-case personally.\nThanks for the reading, see you soon!"
  },
  {
    "objectID": "posts/2020-09-23-local-descriptors-validation.html#citation",
    "href": "posts/2020-09-23-local-descriptors-validation.html#citation",
    "title": "Revisiting Brown patch dataset and benchmark",
    "section": "Citation",
    "text": "Citation\nIf you use the benchmark/development set in an academic work, please cite it.\n@misc{BrownRevisited2020,\n  title={UBC PhotoTour Revisied},\n  author={Mishkin, Dmytro},\n  year={2020},\n  url = {https://github.com/ducha-aiki/brown_phototour_revisited}\n}"
  },
  {
    "objectID": "posts/2021-02-11-wxbs-step-by-step.html",
    "href": "posts/2021-02-11-wxbs-step-by-step.html",
    "title": "WxBS step by step",
    "section": "",
    "text": "The wide baseline stereo problem is commonly addressed by a family of algorithms, the general structure of which is shown in Figure above. We will be referring to it as the WxBS pipeline or the WxBS algorithm. Let us describe it in more detail and discuss the reasoning behind each block.\n\nA set of the local features (Also known as keypoints, local regions, distinguished regions, salient regions, salient points, etc.) is detected in each image independently. In automated systems the local features are usually low level structures like corners, blobs and so on. However, they can also be more high level semantic structures, as we used in the example in intro: “a long staircase on the left side”, “the top of the lampost” and so on. An important detail is that detection is typically done in each image separately. Why is it the case? If the task is to match only a single image pair, that would be an unnecessary restriction. It is even benefitial to process the images jointly, as a human would do, by placing both images side-by-side and looking at them back and forth. However, the wide baseline stereo task rarely arises by itself, more often it is only a part of a bigger system, e.g. visual localization or 3D recontruction from the collection of images. Therefore, one needs to match an image to not the one, but multiple other images. That is why it is benefitial to perform feature extraction only once per image and then load the stored results. Moreover, independent feature extraction is a task which is easy to parallelize and that is typically done in most of libraries and frameworks for the WxBS. One could be wondering if the local feature detection process is necessary at all? Indeed, it is possible to avoid feature detection and consider all the pixels as “detections”. The problem with such approach is the high computational and memory complexity – even a small 800x600 image contains half a million pixels, which need to be matched to half a million pixels in another image.\nA region around the local feature to be described is selected. If one considers a keypoint to be literally a point, it is impractical to distinguish between them based only on coordinates and, maybe, the single RGB value of the pixel. On the other extreme, part of the image, which really far from the current keypoint helps little to nothing in terms of finding a correspondence. Thus, a reasonable trade-off needs to be made. Keypoint therefore can be think of as the “reference point of the distinguished region”, e.g. a center of the blob. It worth mention that some detectors return a region by default, so this step is omitted, or, to be precise, included into step 1 “local features detection”. However, it is useful to have it discussed separately .\nA patch around each local feature is described with a local feature descriptor, i.e. converted to a compact format. Such procedure also should be robust to changes in acquisition conditions so that descriptors related to the same 3D points are similar and dissimilar otherwise. The local feature descriptors are then used for the efficient generation of tentative correspondences. Could one skip this stage? Yes, but as with the local feature detection, the skipping is not desirable from a computational point of view – the benefits are discussed in the next stage – matching. Local feature detection, measurement region selection and description together convert an image into a sparse representation, which is suitable for the correspondence search. Such representation is more robust to the acquisition conditions and can be further indexed if used for image retrieval.\nTentative correspondences between detected features are established and then filtered. The simplest and common way to generate tentative correspondences is to perform a nearest neighbor search in the descriptor space. The commonly used descriptors are the binary or float point vectors, which allows to employ various algorithms for approximate nearest neighbor search and trade a small amount of accuracy for orders of magnitude speed-up. Such correspondences need to be filtered, that is why the are called “tentative” or “putative” – a significant percantage of them is incorrect. There are many reasons for that – imprerfection of the local feature detector, descriptor, matching process and simply the fact that some parts of the scene are visible only on one image, but not another.\nThe geometric relationship between the two images is recovered, which is the final goal of the whole process. In addition, the tentative correspondences, which are not consistent with the found geometry, are called outliers and are discarded. The most common way of robust model estimation in the presense of outliers is called RANSAC – random sample consensus. There are other methods as well, e.g. re-weighted least squares, but RANSAC predominantely used in practice.\nEverything you (didn’t) want to know about image matching"
  },
  {
    "objectID": "posts/2021-02-11-wxbs-step-by-step.html#delving-deeper-into-wxbs-algorithm-steps",
    "href": "posts/2021-02-11-wxbs-step-by-step.html#delving-deeper-into-wxbs-algorithm-steps",
    "title": "WxBS step by step",
    "section": "",
    "text": "The wide baseline stereo problem is commonly addressed by a family of algorithms, the general structure of which is shown in Figure above. We will be referring to it as the WxBS pipeline or the WxBS algorithm. Let us describe it in more detail and discuss the reasoning behind each block.\n\nA set of the local features (Also known as keypoints, local regions, distinguished regions, salient regions, salient points, etc.) is detected in each image independently. In automated systems the local features are usually low level structures like corners, blobs and so on. However, they can also be more high level semantic structures, as we used in the example in intro: “a long staircase on the left side”, “the top of the lampost” and so on. An important detail is that detection is typically done in each image separately. Why is it the case? If the task is to match only a single image pair, that would be an unnecessary restriction. It is even benefitial to process the images jointly, as a human would do, by placing both images side-by-side and looking at them back and forth. However, the wide baseline stereo task rarely arises by itself, more often it is only a part of a bigger system, e.g. visual localization or 3D recontruction from the collection of images. Therefore, one needs to match an image to not the one, but multiple other images. That is why it is benefitial to perform feature extraction only once per image and then load the stored results. Moreover, independent feature extraction is a task which is easy to parallelize and that is typically done in most of libraries and frameworks for the WxBS. One could be wondering if the local feature detection process is necessary at all? Indeed, it is possible to avoid feature detection and consider all the pixels as “detections”. The problem with such approach is the high computational and memory complexity – even a small 800x600 image contains half a million pixels, which need to be matched to half a million pixels in another image.\nA region around the local feature to be described is selected. If one considers a keypoint to be literally a point, it is impractical to distinguish between them based only on coordinates and, maybe, the single RGB value of the pixel. On the other extreme, part of the image, which really far from the current keypoint helps little to nothing in terms of finding a correspondence. Thus, a reasonable trade-off needs to be made. Keypoint therefore can be think of as the “reference point of the distinguished region”, e.g. a center of the blob. It worth mention that some detectors return a region by default, so this step is omitted, or, to be precise, included into step 1 “local features detection”. However, it is useful to have it discussed separately .\nA patch around each local feature is described with a local feature descriptor, i.e. converted to a compact format. Such procedure also should be robust to changes in acquisition conditions so that descriptors related to the same 3D points are similar and dissimilar otherwise. The local feature descriptors are then used for the efficient generation of tentative correspondences. Could one skip this stage? Yes, but as with the local feature detection, the skipping is not desirable from a computational point of view – the benefits are discussed in the next stage – matching. Local feature detection, measurement region selection and description together convert an image into a sparse representation, which is suitable for the correspondence search. Such representation is more robust to the acquisition conditions and can be further indexed if used for image retrieval.\nTentative correspondences between detected features are established and then filtered. The simplest and common way to generate tentative correspondences is to perform a nearest neighbor search in the descriptor space. The commonly used descriptors are the binary or float point vectors, which allows to employ various algorithms for approximate nearest neighbor search and trade a small amount of accuracy for orders of magnitude speed-up. Such correspondences need to be filtered, that is why the are called “tentative” or “putative” – a significant percantage of them is incorrect. There are many reasons for that – imprerfection of the local feature detector, descriptor, matching process and simply the fact that some parts of the scene are visible only on one image, but not another.\nThe geometric relationship between the two images is recovered, which is the final goal of the whole process. In addition, the tentative correspondences, which are not consistent with the found geometry, are called outliers and are discarded. The most common way of robust model estimation in the presense of outliers is called RANSAC – random sample consensus. There are other methods as well, e.g. re-weighted least squares, but RANSAC predominantely used in practice."
  },
  {
    "objectID": "posts/2023-01-04-unsuperpoint-family.html",
    "href": "posts/2023-01-04-unsuperpoint-family.html",
    "title": "Un-SuperPoint family: who are they?",
    "section": "",
    "text": "If you work in image matching, you know SuperPoint. If you don’t - that is one of the most successful modern local feature, developed by Daniel DeTone, Tomasz Malisiewicz and Andrew Rabinovich in 2017.\nIdea was simple and genious: we know that corners are good keypoints. Let’s train a basic corner (in wide meaning) on unlimited synthetic data. Then we somehow adapt it to realworld images.\n\nThe adaptation is done under augmentation supervision. In other word - we believe that our detector is already good, but noisy and we will cancel this noise by running the same detector on multiple augmented version of the same image, which gives us pseudo-ground truth. The training is done via optimizing cross-entropy loss, which leads to very peaky response map compared to other alternatives such as R2D2.\n\nThat’s it. The resulting detector (and descriptor) was fast enough on GPU and great for SLAM purposes. It was especially good on indoor images with lots of textureless areas.\nOn outdoor images it was not so great, however, as it was shown later, the problem was more in descriptor and matching, not detector. SuperGlue matching on top of SuperPoint local features won 3 competitions at CVPR 2020, including our Image Matching Challenge.\nSo, it was huge success. However, not without drawbacks. The main drawback is that training code was never released. That is why people tried independently re-implement SuperPoint, as well as present new, supposedly better versions of it.\nUnfortunately, none of this version was properly evaluated, so we have no idea how they work in reality. Let me fill this gap and present a small eval of the SuperPoint children.\nI’ll first do a short review of how are they different, and the benchmark will be in the last section.\n\n\nBesides the description, I will show how the detections are different between implementations. I will use two images. One is realworld photo taken by myself on Xiaomi phone, and another one is synthetic from OpenCV Blob detection tutorial. It is kind of adversarial, as SuperPoint is not designed to fire or not fire on circular patterns. Well, that is exactly why I added that image. So here are the original SP detections:\n\n\n\n\nThere are two main 3rd party SuperPoint implementations. One of them is in Tensorflow, by Rémi Pautrat and Paul-Edouard Sarlin. I will skip this one, because I am too lazy to install TF on my M1 machine. So I will show another - Pytorch - implementation, which is based on Tensorflow one, and is developed by You-Yi Jau and Rui Zhu.\nBesides being 3rd party implementation, this one also has architectural changes. They are: - adding BatchNorm. - using SoftArgMax2d from torchgeometry (early version of kornia) to achieve subpixel accuracy.\nHere are the detections by this version and original.\n\nHere is also a short report by one of the authors: What have I learned from the implementation of deep learning paper?\nEverything you (didn’t) want to know about image matching"
  },
  {
    "objectID": "posts/2023-01-04-unsuperpoint-family.html#super-starts-with-magic",
    "href": "posts/2023-01-04-unsuperpoint-family.html#super-starts-with-magic",
    "title": "Un-SuperPoint family: who are they?",
    "section": "",
    "text": "If you work in image matching, you know SuperPoint. If you don’t - that is one of the most successful modern local feature, developed by Daniel DeTone, Tomasz Malisiewicz and Andrew Rabinovich in 2017.\nIdea was simple and genious: we know that corners are good keypoints. Let’s train a basic corner (in wide meaning) on unlimited synthetic data. Then we somehow adapt it to realworld images.\n\nThe adaptation is done under augmentation supervision. In other word - we believe that our detector is already good, but noisy and we will cancel this noise by running the same detector on multiple augmented version of the same image, which gives us pseudo-ground truth. The training is done via optimizing cross-entropy loss, which leads to very peaky response map compared to other alternatives such as R2D2.\n\nThat’s it. The resulting detector (and descriptor) was fast enough on GPU and great for SLAM purposes. It was especially good on indoor images with lots of textureless areas.\nOn outdoor images it was not so great, however, as it was shown later, the problem was more in descriptor and matching, not detector. SuperGlue matching on top of SuperPoint local features won 3 competitions at CVPR 2020, including our Image Matching Challenge.\nSo, it was huge success. However, not without drawbacks. The main drawback is that training code was never released. That is why people tried independently re-implement SuperPoint, as well as present new, supposedly better versions of it.\nUnfortunately, none of this version was properly evaluated, so we have no idea how they work in reality. Let me fill this gap and present a small eval of the SuperPoint children.\nI’ll first do a short review of how are they different, and the benchmark will be in the last section.\n\n\nBesides the description, I will show how the detections are different between implementations. I will use two images. One is realworld photo taken by myself on Xiaomi phone, and another one is synthetic from OpenCV Blob detection tutorial. It is kind of adversarial, as SuperPoint is not designed to fire or not fire on circular patterns. Well, that is exactly why I added that image. So here are the original SP detections:\n\n\n\n\nThere are two main 3rd party SuperPoint implementations. One of them is in Tensorflow, by Rémi Pautrat and Paul-Edouard Sarlin. I will skip this one, because I am too lazy to install TF on my M1 machine. So I will show another - Pytorch - implementation, which is based on Tensorflow one, and is developed by You-Yi Jau and Rui Zhu.\nBesides being 3rd party implementation, this one also has architectural changes. They are: - adding BatchNorm. - using SoftArgMax2d from torchgeometry (early version of kornia) to achieve subpixel accuracy.\nHere are the detections by this version and original.\n\nHere is also a short report by one of the authors: What have I learned from the implementation of deep learning paper?"
  },
  {
    "objectID": "posts/2023-01-04-unsuperpoint-family.html#superchildren",
    "href": "posts/2023-01-04-unsuperpoint-family.html#superchildren",
    "title": "Un-SuperPoint family: who are they?",
    "section": "SuperChildren",
    "text": "SuperChildren\n\nReinforced SuperPoint\nThis is a paper by Aritra Bhowmik et al, named Reinforced Feature Points: Optimizing Feature Detection and Description for a High-Level Task. The main idea is that one can use reinforcement learning to optimize non-differentiable downstream metric such as camera pose accuracy through RANSAC.\n\nThe idea is very cool, but the final result is not so. Specifically, the main thing end up optimized is keypoint score function. Which can increase a precision of the keypoint detector, but not the recall. See the image below for the illustration. The synthetic image at the right clearly benefits, but the realworld image - not so much, because of loosing many keypoints on the ground.\nThis also can explain a bit worse performance of the Reinforced SuperPoint in our evaluation - we don’t set the confidence threshold, but instead take top-2048 keypoints whatsoever.\n\n\n\nUnsuperPoint\nThis is a paper from eiva.com and Aarhus University by Peter Hviid Christiansen, which proposed to drop the supervised pretraining and use regression module instead of CE on heatmap for training detector.\n\n\n\nimage.png\n\n\nUnfortunately, there is no implementation available, so I will pretend the paper never existed.\n\n\nKP2D aka KeypointNet aka IONet\nDespite no implementation, UnsuperPoint inspired other people to make a follow-up, which was published at ICLR 2020. The paper name is “Neural Outlier Rejection for Self-Supervised Keypoint Learning” by Jiexiong Tang et al.\nIt made two contributions. First, paper argues that cell-based approach (SuperPoint and friends have 1 keypoint per 8x8 px cell) is unstable for training when keypoints are near the cell border. Second, it introduced yet another loss function, similar to CNe and other outlier rejection methods.\n\nHere are the KP2D detections for all versions. Note that on realworld image it detects corners nicely, despite not being training for it specifically. On the synthetic blob image it mostly work as dark-to-light (or light-to-dark for V0 and V2) edge detector, no idea why.\n\n\n\nLANet\nIt is the most recent paper of the SP family, published at ACCV2022 – “Rethinking Low-level Features for Interest Point Detection and Description” by Changhao Wang et al.\n\nIt is based on KP2D and presents mostly architectural changes into description branch. Second, it has two versions. v0 is similar to the SuperPoint original architecture – lightweight VGG-like network, trained from scratch. v1 is uses vgg16_bn, pretrained on the ImageNet as feature encoder. v1 is like 2-3x slower than v0.\nHere are the detections:"
  },
  {
    "objectID": "posts/2023-01-04-unsuperpoint-family.html#why-eval-only-on-hpatches-is-meaningless.",
    "href": "posts/2023-01-04-unsuperpoint-family.html#why-eval-only-on-hpatches-is-meaningless.",
    "title": "Un-SuperPoint family: who are they?",
    "section": "Why eval only on HPatches is meaningless.",
    "text": "Why eval only on HPatches is meaningless.\nKP2D and 3rd party SuperPoint eval themselves on HPatches only. Let’s look at them. According to KP2D paper, it is slightly better than Superpoint on homography estimation and 2x better on matching score. Which we haven’t seen on realworld IMC data.\n\nAnd here is eval from 3rd part SuperPoint. According to it, Superpoint-coco is almost as good as SP MagicLeap on homography estimation, better on detector metrics and slightle worse on descriptor metrics. Which, again, doesn’t correspond to our results.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask\nHomography estimation\n\n\nDetector metric\n\nDescriptor metric\n\n\n\n\n\n\nEpsilon = 1\n3\n5\nRepeatability\nMLE\nNN mAP\nMatching Score\n\n\nMagicLeap\n0.44\n0.77\n0.83\n0.606\n1.14\n0.81\n0.55\n\n\nsuperpoint_coco_heat2_0_170k_hpatches_sub\n0.46\n0.75\n0.81\n0.63\n1.07\n0.78\n0.42\n\n\nsuperpoint_kitti_heat2_0_50k_hpatches_sub\n0.44\n0.71\n0.77\n0.56\n0.95\n0.78\n0.41"
  },
  {
    "objectID": "posts/2023-01-04-unsuperpoint-family.html#additional-results-from-validation-set-and-recommended-hyper-paramers",
    "href": "posts/2023-01-04-unsuperpoint-family.html#additional-results-from-validation-set-and-recommended-hyper-paramers",
    "title": "Un-SuperPoint family: who are they?",
    "section": "Additional results from validation set and recommended hyper-paramers",
    "text": "Additional results from validation set and recommended hyper-paramers\nHere I will present some results from the tuning on validation set. I believe that they would translate to the other datasets as well.\n\nL2-normalization of descriptor does not help ANY of the evaluated models. It doesn’t hurt either.\nSubpixel with soft-argmax helps 3rd party SuperPoint. One may try to apply it to original model as well. The difference is: mAA = 0.2965 for subpixel vs mAA = 0.2789 no-subpixel on PhotoTourism and 0.3567 vs 0.3274 on PragueParks.\nThe v2 model of the KP2D is much better than then rest. v0 and v1 might be bugged, or require code changes to be run properly maybe?"
  },
  {
    "objectID": "posts/2023-01-04-unsuperpoint-family.html#optimal-hyperparamers",
    "href": "posts/2023-01-04-unsuperpoint-family.html#optimal-hyperparamers",
    "title": "Un-SuperPoint family: who are they?",
    "section": "Optimal hyperparamers",
    "text": "Optimal hyperparamers\nSNN - 2nd nearest neighbor threshold, PT - PhotoTourism, PP - PragueParks, inl_th - inlier threshold for DEGENSAC.\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nmutual SNN threshold\nPT inl_th\nPP inl_th\nPT val stereo mAA\nPP val stereo mAA\nOther\n\n\n\n\nMagicLeap SuperPoint\n0.9\n1.0\n1.5\n0.3746\n0.5628\nn/a\n\n\nReinforced SuperPoint\n0.9\n1.0\n1.5\n0.3491\n0.5497\nn/a\n\n\nSuperPoint 3rd party COCO\n0.95\n0.75\n1.0\n0.2966\n0.3488\nsubpix\n\n\nSuperPoint 3rd party KITTY\n0.95\n0.75\n1.0\n0.1910\n0.2621\nsubpix\n\n\nKP2D\n0.99\n0.75\n1.0\n0.3633\n0.4971\nv2\n\n\nLANet v0\n0.99\n0.5\n1.0\n0.4591\n0.6175\nn/a\n\n\nLANet v1\n0.99\n0.5\n1.0\n0.4838\n0.6127\nn/a"
  },
  {
    "objectID": "posts/2023-01-04-unsuperpoint-family.html#bonus-more-visualizations",
    "href": "posts/2023-01-04-unsuperpoint-family.html#bonus-more-visualizations",
    "title": "Un-SuperPoint family: who are they?",
    "section": "Bonus: more visualizations",
    "text": "Bonus: more visualizations\nLegend: - blue points: COLMAP reconstructed 3D points - red point: non-matched keypoints .\n\n\n\n\n\nAcknowledgements.\nThis blogpost is supported by CTU in Prague RCI computing cluster from OP VVV funded project CZ.02.1.01/0.0/0.0/16 019/0000765 “Research Center for Informatics” grant. Really, it took ~2 compute days to tune all those hyperparams and do tests."
  },
  {
    "objectID": "posts/2020-11-26-lessons-and-future-directions.html",
    "href": "posts/2020-11-26-lessons-and-future-directions.html",
    "title": "Lessons learned and future directions",
    "section": "",
    "text": "I would like to share some lessons I have learned about wide baseline stereo and propose some research directions, which are worth exploring in the short and longer term.\nEverything you (didn’t) want to know about image matching"
  },
  {
    "objectID": "posts/2020-11-26-lessons-and-future-directions.html#benchmark-metrics-implementation-dataset",
    "href": "posts/2020-11-26-lessons-and-future-directions.html#benchmark-metrics-implementation-dataset",
    "title": "Lessons learned and future directions",
    "section": "1. Benchmark = metrics + implementation + dataset",
    "text": "1. Benchmark = metrics + implementation + dataset\nIn our paper “Image Matching across Wide Baselines: From Paper to Practice” we focused on the first two parts. Specifically, metrics – if they are not “downstream” metrics, the improvements in the single component might not translate to the overall system improvements. And implementation – implementing the simplest possible setup is, of course, a valuable tool, but one have to also incorporate the best known practices, e.g. matching, RANSAC tuning as so on.\nI hope, that we have delivered that message to the community. But the last component – the dataset – we have, perhaps, overlooked a bit ourself. The problem with dataset limitations, e.g. lack of illumination or seasonal changes is not that one does not properly address. Usually benchmark papers are adressing their limitations quite clearly. The problem is that researchers (including myself) have tendency to work on improving results, which are easily measurable, therefore the implicitly designing the methods, which solve only some specific problem, encoded in the form of the dataset."
  },
  {
    "objectID": "posts/2020-11-26-lessons-and-future-directions.html#trying-to-work-for-the-most-general-case-might-be-detrimental-for-the-practical-applications.",
    "href": "posts/2020-11-26-lessons-and-future-directions.html#trying-to-work-for-the-most-general-case-might-be-detrimental-for-the-practical-applications.",
    "title": "Lessons learned and future directions",
    "section": "2. Trying to work for the “most general” case might be detrimental for the practical applications.",
    "text": "2. Trying to work for the “most general” case might be detrimental for the practical applications.\nThis is kind of opposite side of the lesson 1. For example, the classical SIFT matching is rotation-invariant, because that was assumed to be the requirement to work in the “real world”. However, the practice of image retrieval and then most of learned local features like R2D2, SuperPoint, DELF and so on, showed that “up-is-up” is a reasonable assumption to built on. The rotational invariance in lots of scenarios hurt more than helps."
  },
  {
    "objectID": "posts/2020-11-26-lessons-and-future-directions.html#when-borrow-idea-from-classical-paper-adapt-it",
    "href": "posts/2020-11-26-lessons-and-future-directions.html#when-borrow-idea-from-classical-paper-adapt-it",
    "title": "Lessons learned and future directions",
    "section": "3. When borrow idea from classical paper, adapt it",
    "text": "3. When borrow idea from classical paper, adapt it\nHardNet borrows the idea of the using second nearest neighbor(SNN) descriptor from the SIFT. However, using SNN ratio for descriptor learning leads to inferior results. We had to modify it to the triplet margin loss.\n## 4. Classic handcrafted algorithms are not dead and can be improved a lot\nModern versions of RANSAC are still necessary for two view matching even when so complex methods as SuperGlue as used. Moreover, they still have quite a lot things for improvement, as proved by my colleague, Daniel Barath\nThey also have a benefit, that once you have an idea, you can make a paper out it faster, as you don’t need gathering data and training the model."
  },
  {
    "objectID": "posts/2020-11-26-lessons-and-future-directions.html#application-specific-local-features",
    "href": "posts/2020-11-26-lessons-and-future-directions.html#application-specific-local-features",
    "title": "Lessons learned and future directions",
    "section": "1. Application-specific local features",
    "text": "1. Application-specific local features\nOne thing about the current local features is that they are kind of universal. SIFT works reasonably well for lots of applications: 3d reconstruction, SLAM, image retrieval, etc. The learned features, like SuperPoint or R2D2, although are biased towards the data they are trained on, still don’t have anything domain specific.\nLet me explain. There are different qualities about the local feature (detectors). It can be more or less robust to nuisance factors like the illumination and camera position. It can be more or less precisely localized. It can be more or less dense and/or evenly distributed over the image.\nFor example, in image retrieval, one does not really care about precise localization, the robustness is much more important. For the 3d reconstruction one would like to have a lot of 3d points to get the reasonable reconstruction. On the other hand, for the SLAM/relocalization application, sparse features would be more advantageous because of smaller memory footprint and computational cost.\nThere are, actually, some steps in that direction. Let me name a few.\n\nHOW local features designed for the image retrieval \nSIPs: Succinct Interest Points from Unsupervised Inlierness Probability Learning sparse local features for the SLAM. \nReinforced Feature Points: Optimizing Feature Detection and Description for a High-Level Task \n\nI believe, that it is only beginning and we are yet to experience AlphaZero moment for the local features."
  },
  {
    "objectID": "posts/2020-11-26-lessons-and-future-directions.html#rethinking-overall-wide-baseline-stereo-pipeline-optimized-for-the-specific-application",
    "href": "posts/2020-11-26-lessons-and-future-directions.html#rethinking-overall-wide-baseline-stereo-pipeline-optimized-for-the-specific-application",
    "title": "Lessons learned and future directions",
    "section": "2. Rethinking overall wide baseline stereo pipeline, optimized for the specific application",
    "text": "2. Rethinking overall wide baseline stereo pipeline, optimized for the specific application\nIt is often perceived, that image matching across the unordered collection of the images is a task with quadratic complexity w.r.t. number of images. Some operations can be done separately, e.g. feature detection, but others, like feature matching and RANSAC cannot. Right?\nNot necessarily. It turns out, that one can avoid running feature matching and RANSAC for more than 90% of image pairs with clever preprocessing, ordering and re-using results from the previous matching. Moreover, in order to do the whole task faster (matching image collections), one may need to introduce additional steps, which are not necessary, or slowing things down for the two images case.\nThat’s what we done for the intial pose estimation for the global SfM in our paper, which reduced the matching runtime from 200 hours to 29.\n\nAnother example would be “SuperGlue: Learning Feature Matching with Graph Neural Networks”, where authors abandoned traditional descriptor matching and instead leveraged all the information for keypoint and descriptors from both images altogether."
  },
  {
    "objectID": "posts/2020-11-26-lessons-and-future-directions.html#rethinking-and-improving-the-process-of-training-data-generation-for-the-wbs",
    "href": "posts/2020-11-26-lessons-and-future-directions.html#rethinking-and-improving-the-process-of-training-data-generation-for-the-wbs",
    "title": "Lessons learned and future directions",
    "section": "3. Rethinking and improving the process of training data generation for the WBS",
    "text": "3. Rethinking and improving the process of training data generation for the WBS\nSo far, all the local feature papers I have seen rely on one of the ground truth source.\n\nSfM data, obtained with COLMAP with, possibly, a cleaned depth information.\nAffine and color augmentation.\nSynthetic images (e.g. corners for SuperPoint).\n\nThere are several problems with them.\nSfM data assumes that the data is matchable by the existing methods, at least for a some extent. That might not always be true for cross-seasonal, medical and other kind of data. It is also not applicable for historical photographies and other types of data. Moreover, SfM data takes quite a time for compute and space to store. I believe that we may do better.\nAffine and color augmentation can take us only this far – we actually want our detectors and descriptors to be robust to the changes, which we don’t know how to simulate/augment.\nSynthetic images as they are in, say, CARLA simulator lack fine details and photorealism. However, I am optimistic about using neural renderers and learned wide baseline stereo is a GAN-like self-improving loop."
  },
  {
    "objectID": "posts/2020-11-26-lessons-and-future-directions.html#matching-with-on-demand-view-synthesis-revisited",
    "href": "posts/2020-11-26-lessons-and-future-directions.html#matching-with-on-demand-view-synthesis-revisited",
    "title": "Lessons learned and future directions",
    "section": "4. Matching with On-Demand View Synthesis revisited",
    "text": "4. Matching with On-Demand View Synthesis revisited\nI like the “on-demand” principle a lot and I think we can explore it much more that we are now. So far we have either affine view synthesis (ASIFT, MODS), or GAN-based stylizations for the day-night matching.\nThat is why I am glad to see papers like Single-Image Depth Prediction Makes Feature Matching Easier, which generate normalized views based on depth in order to help the matching.\nWhy not go further? Combine viewpoint, illumination, season, sensor synthesis?"
  },
  {
    "objectID": "posts/2020-11-26-lessons-and-future-directions.html#moar-inputs",
    "href": "posts/2020-11-26-lessons-and-future-directions.html#moar-inputs",
    "title": "Lessons learned and future directions",
    "section": "5. Moar inputs!",
    "text": "5. Moar inputs!\nI have mentioned above that monocular depth may help the feature matching or camera pose estimation. However, why stop here?\nLet’s use other networks as well, especially given that we will need them on robot or vehicle anyway. Semantic segmentation? Yes, please. Surface normals? Why not? Intrinsic images? Йой, най буде! {% fn 1 %}\n{{ “Ukrainian, means let is be” | fndetail: 1 }}"
  },
  {
    "objectID": "posts/2020-11-26-lessons-and-future-directions.html#what-do-you-think-would-be-good-idea-for-the-wbs-research",
    "href": "posts/2020-11-26-lessons-and-future-directions.html#what-do-you-think-would-be-good-idea-for-the-wbs-research",
    "title": "Lessons learned and future directions",
    "section": "What do you think would be good idea for the WBS research?",
    "text": "What do you think would be good idea for the WBS research?\nLet me know in comments/twitter. I am also going to update this page from time to time"
  },
  {
    "objectID": "posts/2021-05-17-opencv-new-ransacs.html",
    "href": "posts/2021-05-17-opencv-new-ransacs.html",
    "title": "Evaluating OpenCV new RANSACs",
    "section": "",
    "text": "Year ago we published a paper “Image Matching across Wide Baselines: From Paper to Practice”, which, among other messages, has shown that OpenCV RANSAC for fundamental matrix estimation is terrible: it was super inaccurate and slow. Since then my colleague Maksym Ivashechkin has spent a summer 2020 improving OpenCV RANSACs. His work was released as a part of OpenCV 4.5.0 release.\nNow it is time to benchmark them. Let’s go!\nEverything you (didn’t) want to know about image matching"
  },
  {
    "objectID": "posts/2021-05-17-opencv-new-ransacs.html#opencv-ransac-is-dead.-long-live-the-opencv-usac",
    "href": "posts/2021-05-17-opencv-new-ransacs.html#opencv-ransac-is-dead.-long-live-the-opencv-usac",
    "title": "Evaluating OpenCV new RANSACs",
    "section": "",
    "text": "Year ago we published a paper “Image Matching across Wide Baselines: From Paper to Practice”, which, among other messages, has shown that OpenCV RANSAC for fundamental matrix estimation is terrible: it was super inaccurate and slow. Since then my colleague Maksym Ivashechkin has spent a summer 2020 improving OpenCV RANSACs. His work was released as a part of OpenCV 4.5.0 release.\nNow it is time to benchmark them. Let’s go!"
  },
  {
    "objectID": "posts/2021-05-17-opencv-new-ransacs.html#evaluation-methodology",
    "href": "posts/2021-05-17-opencv-new-ransacs.html#evaluation-methodology",
    "title": "Evaluating OpenCV new RANSACs",
    "section": "Evaluation methodology",
    "text": "Evaluation methodology\nThe benchmark is done on the validation subset of the Image Matching Challenge 2021 datasets. We have detected RootSIFT features, matched them with optimal mutual SNN ratio test and feed into the tested RANSACs. The resulting fundamental matrixes were transformed into relative poses and compared to the ground truth poses. You can check details in the paper “Image Matching across Wide Baselines: From Paper to Practice”.\nFor all RANSACs we first determine the optimal inlier threshold by the grid search, whereas number of iterations (max_iter) was set to a reasonable 100k. Then, after fixing this optimal threshold, we vary number of iterations from 10 to 10M. This gives us an accuracy-time curve."
  },
  {
    "objectID": "posts/2021-05-17-opencv-new-ransacs.html#methods-evaluated",
    "href": "posts/2021-05-17-opencv-new-ransacs.html#methods-evaluated",
    "title": "Evaluating OpenCV new RANSACs",
    "section": "Methods evaluated",
    "text": "Methods evaluated\nNon-OpenCV methods:\n\nDEGENSAC - from pydegensac package, based on the original implementation of the method, proposed in CVPR 2005 paper “Two-View Geometry Estimation Unaffected by a Dominant Plane”. It is the default choise for the Image Matching Challenge 2020 and 2021.\nPyRANSAC - also from pydegensac package, with flag enable_degeneracy_check=False, which is equivalent to a vanilla LO-RANSAC implementation.\n\nOpenCV methods, named after the flag, one needs to pass into cv2.findFundamentalMatrix function:\n\nUSAC_DEFAULT – LO-RANSAC + degeneracy tests\nUSAC_FAST – LO-RANSAC + degeneracy tests. Fewer iterations in local optimization step than USAC_DEFAULT. Uses RANSAC score to maximize number of inliers and terminate earlier.\nUSAC_ACCURATE. Implements Graph-Cut RANSAC + degeneracy tests.\nUSAC_MAGSAC – MAGSAC++ implementation + degeneracy tests.\nRANSAC – OpenCV RANSAC implementation from the previous versions of the library, no degeneracy tests\n\nAll OpenCV USAC methods also use SPRT-test for speeding-up the evaluation."
  },
  {
    "objectID": "posts/2021-05-17-opencv-new-ransacs.html#results",
    "href": "posts/2021-05-17-opencv-new-ransacs.html#results",
    "title": "Evaluating OpenCV new RANSACs",
    "section": "Results",
    "text": "Results\nHere are results for all 3 datasets. The lefter and upper is curve, the better. Dashed vertical line marks 1/25 sec (“realtime”) and 0.5 sec (challenge limit) time budget. Legend shows the method name and the optimal inlier threshold for the datasets: Phototourism, GoogleUrban and PragueParks respectively.\n\n\nThe first and main message – all new flags are much better than the old OpenCV implementation (green curve, worst results), which still a default option.\n10k iterations and USAC_ACCURATE (red curve) gives you great results within 0.01 sec\nAll OpenCV advanced USACs are better than for the small/medium time budget (&lt; 0.1 sec per image) than pydegensac (blue curve).\n\nThe best methods for the higher budget are OpenCV USAC_MAGSAC and DEGENSAC from the pydegensac package.\nThere is no point is using flag “USAC_FAST” it is always better to use USAC_DEFAULT, USAC_ACCURATE or USAC_MAGSAC.\nUSAC_MAGSAC is the only method, which optimal threshold is the same across all datasets. This is a valuable property for practice, as it requires the least tuning.\n\nIf you are interesting in results for an individual datasets, here they are.\n\nPhototourism\n\n\n\nimage.png\n\n\n\n\nGoogleUrban\n\n\n\nPragueParks"
  },
  {
    "objectID": "posts/2021-05-17-opencv-new-ransacs.html#why-do-i-tune-and-evaluate-on-the-same-set",
    "href": "posts/2021-05-17-opencv-new-ransacs.html#why-do-i-tune-and-evaluate-on-the-same-set",
    "title": "Evaluating OpenCV new RANSACs",
    "section": "Why do I tune and evaluate on the same set?",
    "text": "Why do I tune and evaluate on the same set?\nIt is true, that tuning and evaluation of the method on the same dataset does not make any sense. However, let me defend my choice. Here are the arguments:\n\nI do not want to compromise an integrity of the test set, which is the basis of the on-going competition Image Matching Challenge 2021 with prize money. That is why I do not want to leak information from the abovementioned test set and this is my primarly optimization objective. I also cannot tune the threshold on the “training subset”, as both GoogleUrban and PragueParks do not have such.\nI am interested more in the rough speed-accuracy trade-off than the precise rankings of the methods. It is quite likely, that those methods, which have an small acuracy gap on the validation set, would switch on the test set – as it happened with DEGENSAC and MAGSAC in our original paper. However, it is very unlikely, that method, which performs poorly on the validation set would magically outperform everyone on the test set. Again, see PyRANSAC vs DEGENSAC in the original paper.\nI clearly state this fact as a limitation and do not publish a paper ;)"
  },
  {
    "objectID": "posts/2021-05-17-opencv-new-ransacs.html#conclusion",
    "href": "posts/2021-05-17-opencv-new-ransacs.html#conclusion",
    "title": "Evaluating OpenCV new RANSACs",
    "section": "Conclusion",
    "text": "Conclusion\nNew OpenCV RANSACs are fast and have comparable accuracy, you can safely pick one of them. However, if you are using pydegensac and have &gt; 0.1 sec time budget, there is no need to switch.\nUse proper RANSACs and be happy :)"
  },
  {
    "objectID": "posts/2022-07-05-imc2022-recap.html",
    "href": "posts/2022-07-05-imc2022-recap.html",
    "title": "Image Matching Challenge 2022 Recap",
    "section": "",
    "text": "Yes, it is the main change, compared to previous years. How it works: - Participants submit notebooks that run offline - Test set remains private (not visible to participants) : very difficult to cheat (big concern at Kaggle) - Allows for quick iterations\nThat allowed us to get 25x more participants and 150x more submissions\n\nThere are other differences compared to the previous years.\n\nNo multiview track. Unlike previous competitions with multivew and stereo tracks, this year we focused on stereo only. There are many reasons for it, but the main is technical difficulty - to run and evaluate this under reasonable time limit. Having a pair of images as an input also could potentially broaden the family of methods used by participants.\nNew dataset and new metric. Unlike previous years, where our GT data was scale-less, now we have a metric ground truth, so we can evaluate translation error more in line with visual re-localization literature. We also used non-public dataset from Google, not available anywhere online. This was also only possible because of Kaggle Code competition nature.\nTime limit. I have mentioned this already, but it is important. The submission should finish in 9 hours on Kaggle GPU virtual instance. This forced teams to think, what to add, and what not. Simple example would be a semantic segmentation masking - teams decided that benefit is not worth computational overhead it brings.\nEverything you (didn’t) want to know about image matching"
  },
  {
    "objectID": "posts/2022-07-05-imc2022-recap.html#what-has-been-changed-in-the-challenge-itself-kaggle",
    "href": "posts/2022-07-05-imc2022-recap.html#what-has-been-changed-in-the-challenge-itself-kaggle",
    "title": "Image Matching Challenge 2022 Recap",
    "section": "",
    "text": "Yes, it is the main change, compared to previous years. How it works: - Participants submit notebooks that run offline - Test set remains private (not visible to participants) : very difficult to cheat (big concern at Kaggle) - Allows for quick iterations\nThat allowed us to get 25x more participants and 150x more submissions\n\nThere are other differences compared to the previous years.\n\nNo multiview track. Unlike previous competitions with multivew and stereo tracks, this year we focused on stereo only. There are many reasons for it, but the main is technical difficulty - to run and evaluate this under reasonable time limit. Having a pair of images as an input also could potentially broaden the family of methods used by participants.\nNew dataset and new metric. Unlike previous years, where our GT data was scale-less, now we have a metric ground truth, so we can evaluate translation error more in line with visual re-localization literature. We also used non-public dataset from Google, not available anywhere online. This was also only possible because of Kaggle Code competition nature.\nTime limit. I have mentioned this already, but it is important. The submission should finish in 9 hours on Kaggle GPU virtual instance. This forced teams to think, what to add, and what not. Simple example would be a semantic segmentation masking - teams decided that benefit is not worth computational overhead it brings."
  },
  {
    "objectID": "posts/2022-07-05-imc2022-recap.html#top-solutions-2022-overview",
    "href": "posts/2022-07-05-imc2022-recap.html#top-solutions-2022-overview",
    "title": "Image Matching Challenge 2022 Recap",
    "section": "Top solutions-2022 overview",
    "text": "Top solutions-2022 overview\nUnlike previous competitions, almost all the top-performing teams have converged to almost the same pipeline, although different in implementation details, but based on the same ideas. Let me condence it is for you, and if you are interested in participants own write-ups, just scroll down, I will provide all the links after the post.\nThe only top-participant, who did not apply any pre/post-processing is 2nd place winner hkchkc. It is based on novel matcher (let’s call it LoFTR 2.0), which he did not want to disclose yet. Anyway, check his write-up here.\nSo, the general pipeline is shown below. The following steps are present in the most of solutions:\n\n\nObtaining initial pool of matches, using a combination of off-the-shelf models. Such models are LoFTR, SuperGlue, QuadTree Attention LoFTR and DKM. Some of the teams also applied test-time scale augmentation (TTA) and concatenated tentative correspondence, which came from matching differently-resize images.\nThe next step is an estimation of the co-visible area between two images, similar to ScaleNet idea, but totally handcrafted and based on tentative matches. The approach is to cluster the matches using K-means or DBSCAN and then find the bounding box in each image, which contains the most of the matches. This can lead to potentially several “co-visible areas”. One may take a step further and estimate the homography between each matching region. An alternative to clustering (from 9th place) is to use MAGSAC++ with few iterations and coarse threshold to reject the most of outliers, and get the bounding box .\nEach of such co-visible and matching regions is then cropped and resized to a predefined image size. Then the base-matcher, or other set of matching algorithms are applied for this “zoomed-in” image pair. All the correspondences are then reprojected into original image pair coordinates and contatenated with original correspondences. Important: “zoomed-in” is used to entend original tentative correspondences, not replace them, otherwise first failure would kill the matching process.\nThe correspondences are (optionally) filtered with some kind of non-maxima-suppression, like ANMS or radius-based NMS. The intuition here is to select the fewer set of more precise matches. It is interesting, that no team run any kind of filtering network as OANet.\nMAGSAC++ from OpenCV (as recommended in my evaluation) is applied to find fundamental matrix."
  },
  {
    "objectID": "posts/2022-07-05-imc2022-recap.html#tricks-which-improve-results",
    "href": "posts/2022-07-05-imc2022-recap.html#tricks-which-improve-results",
    "title": "Image Matching Challenge 2022 Recap",
    "section": "Tricks which improve results",
    "text": "Tricks which improve results\n\nSwitching img1 and img2 for LoFTR-like methods, because the method is asymmetric.\nRunning RANSAC and local feature extraction in separate threads. Although competition specific, this also may help to speed-up some semi-real-time SLAM methods."
  },
  {
    "objectID": "posts/2022-07-05-imc2022-recap.html#works-with-tuning",
    "href": "posts/2022-07-05-imc2022-recap.html#works-with-tuning",
    "title": "Image Matching Challenge 2022 Recap",
    "section": "Works with tuning",
    "text": "Works with tuning\n\nSome teams successfully used two-view keypoint refinement, while others didn’t make it work.\n\n\n\nUnlike MAGSAC++, which works out of the box, VSAC could bring significant time improvements, but needs a lot of hyperparameter tuning, otherwise epipolar geometry quality suffers."
  },
  {
    "objectID": "posts/2022-07-05-imc2022-recap.html#rare-findings",
    "href": "posts/2022-07-05-imc2022-recap.html#rare-findings",
    "title": "Image Matching Challenge 2022 Recap",
    "section": "Rare findings",
    "text": "Rare findings\n\nNormalizing positional embeddings used in LoFTR-like methods. See solution 2.\nWhen doing TTA, run and cache SuperPoint prior to SuperGlue matching, this saves a lot of time. Also, SuperGlue whould be run on the extracted, not reprojected keypoints.\nResize method matters a little bit: Lanczos seems to work the best.\nCoordinates refinement by ECO-TR (improved fast version of COTR, not yet available online)"
  },
  {
    "objectID": "posts/2022-07-05-imc2022-recap.html#does-not-work",
    "href": "posts/2022-07-05-imc2022-recap.html#does-not-work",
    "title": "Image Matching Challenge 2022 Recap",
    "section": "Does not work",
    "text": "Does not work\n\nAdding tentative matches from local features without learned matcher, such as DISK, ALike, etc. Team 1 mentioned that KeyNetAffNetHardNet was the one, which do imporove results, but at the cost of huge computational overhead, so it doesn’t worth it.\nSemantic segmentation masking (sky, people, etc)."
  },
  {
    "objectID": "posts/2022-07-05-imc2022-recap.html#final-thoughts",
    "href": "posts/2022-07-05-imc2022-recap.html#final-thoughts",
    "title": "Image Matching Challenge 2022 Recap",
    "section": "Final thoughts",
    "text": "Final thoughts\nSooo…first think I want to say, is that going Kaggle was definitely worth it. We learned a lot, challenge got visibility, and many new improvements came out of of the challenge.\nSecond, is that our training set was too small and and too different from the test set, that is why it was almost useless. I don’t have any solution yet for this.\nThresholds on the pose accuracy were probably too loose, as you can see from the results.\nOn the lessons from the solution themselves:\n\n2-stage approach of first finding covisible region, and then zooming-in into it seems to be way to go. What is also important, is to delay the decision until the very end.\nIt is better to first fix recall problem, i.e. to get as many tentative correspondences as possible, e.g. from different methods, and only the filter them. Modern RANSACs can recover the pose from a small inlier ratio, but never from small number of correct correspondences themselves.\nLearned methods like LoFTR are very sensitive on the input image size, we probably have to do something about it."
  },
  {
    "objectID": "posts/2022-07-05-imc2022-recap.html#list-of-all-top-solution-write-ups",
    "href": "posts/2022-07-05-imc2022-recap.html#list-of-all-top-solution-write-ups",
    "title": "Image Matching Challenge 2022 Recap",
    "section": "List of all top solution write-ups",
    "text": "List of all top solution write-ups\nIt is here, created by Addison Howard."
  },
  {
    "objectID": "posts/2021-05-27-submitting-to-imc2021-with-custom-matcher.html",
    "href": "posts/2021-05-27-submitting-to-imc2021-with-custom-matcher.html",
    "title": "Submitting to IMC 2021 with custom matcher",
    "section": "",
    "text": "In the previous tutorial we have created a pipeline for submitting to the IMC2021. However, we have not covered all the possibilities. First, we haven’t submissted custom matches, instead we just run a standard Lowe SNN test with cross-check. Second, we haven’t evaluate multiview (COLMAP) part. In this tutorial we will do both. I am assuming that you have completed the previous part. If not, please do, because we will be relying on the already extracted features. Let’s check if the feature are there\ncd imc2021-sample-kornia-submission\nls extracted/cv2-dog-affnet-hardnet8/googleurban/edinburgh/\nangles.h5  descriptors.h5  keypoints.h5  scales.h5  scores.h5\n\n\n\n\n\n\nWarning\n\n\n\nThere is a subtle problem with previous tutorial, so we cannot use pre-extracted features.\n\n\nSpecifically, because OpenCV SIFT does not exactly respect max_features parameter and can sometimes output 8002 features, instead of 8000. When we were importing the features alone, benchmark import_features.py script automatically re-sorted features based on the score and clip the extra 1-3 features. However, this functionality is not available for the importing custom matches. I have already corrected previous post, so you can use it for the re-extration. Or, if you are reading this whole tutorial after May 24, just ignore this.\nNow we will install AdaLAM - one of the winners of IMC2020 Challenge. It uses keypoint geometry to filter out unreliable matches.\npip install git+https://github.com/cavalli1234/AdaLAM.git\nLet’s check if it works on the sample image pair. We will read the pre-extracted features for it.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nimport os\nimport torch\nimport kornia as K\nimport kornia.feature as KF\nimport h5py\nimport json\nfrom PIL import Image\nfrom adalam import AdalamFilter\nfrom kornia_moons.feature import *\n\ndef load_h5(filename):\n    '''Loads dictionary from hdf5 file'''\n\n    dict_to_load = {}\n    try:\n        with h5py.File(filename, 'r') as f:\n            keys = [key for key in f.keys()]\n            for key in keys:\n                dict_to_load[key] = f[key][()]\n    except:\n        print('Cannot find file {}'.format(filename))\n    return dict_to_load\n\nPATH_TO_FEATS = 'extracted/cv2-dog-affnet-hardnet8/googleurban/edinburgh/'\nkps = load_h5(os.path.join(PATH_TO_FEATS, 'keypoints.h5'))\nangles = load_h5(os.path.join(PATH_TO_FEATS, 'angles.h5'))\nscales = load_h5(os.path.join(PATH_TO_FEATS, 'scales.h5'))\ndescs = load_h5(os.path.join(PATH_TO_FEATS, 'descriptors.h5'))\n\nI have selected two images, which are matching:\n\nIMG_DIR = '../imc-2021-data/googleurban/edinburgh/set_100/images/'\nimg1_key = '2b5315968bc5468c995b978620879439'\nimg2_key = '6264aee21d1b48b7985901c4bedfdbd4'\n\nimg1 = cv2.cvtColor(cv2.imread(os.path.join(IMG_DIR, f'{img1_key}.png')), cv2.COLOR_BGR2RGB)\nimg2 = cv2.cvtColor(cv2.imread(os.path.join(IMG_DIR, f'{img2_key}.png')), cv2.COLOR_BGR2RGB)\nplt.imshow(np.concatenate([img1, img2], axis=1))\n\n\n\n\n\n\n\n\nWe will start with matching and drawing the matches with OpenCV for sanity check.\n\ndef opencv_from_imc(kps, sizes, angles):\n    return [cv2.KeyPoint(kp[0], kp[1], float(s), float(a)) for  kp, s, a in zip(kps, sizes, angles)]\n\ndef get_data(kps, angles, scales, descs, img_key):\n    kp1 = kps[img_key]\n    s1 = scales[img_key]\n    a1 = angles[img_key]\n    descs1 = descs[img_key]\n    return kp1, s1, a1, descs1\n \ndef match(img1_key, img2_key, kps, angles, scales, descs):\n    kp1, s1, a1, descs1 = get_data(kps, angles, scales, descs, img1_key)\n    kp2, s2, a2, descs2 = get_data(kps, angles, scales, descs, img2_key)\n    dists, idxs = KF.match_smnn(torch.from_numpy(descs1), torch.from_numpy(descs2), 0.9)\n    return dists, idxs\n\ndef draw_matches(img1_key, img2_key, dists, idxs, kps, angles, scales, descs):\n    tentatives = cv2_matches_from_kornia(dists, idxs)\n    draw_params = dict(matchColor = (255,255,0), # draw matches in yellow color\n                   singlePointColor = None,\n                   matchesMask = [True for x in idxs], # draw only inliers\n                   flags = 2)\n    img1 = cv2.cvtColor(cv2.imread(os.path.join(IMG_DIR, f'{img1_key}.png')), cv2.COLOR_BGR2RGB)\n    img2 = cv2.cvtColor(cv2.imread(os.path.join(IMG_DIR, f'{img2_key}.png')), cv2.COLOR_BGR2RGB)\n    kp1, s1, a1, _ = get_data(kps, angles, scales, descs, img1_key)\n    kp2, s2, a2, descs2 = get_data(kps, angles, scales, descs, img2_key)\n    \n    img_out = cv2.drawMatches(img1,opencv_from_imc(kp1, s1, a1),\n                              img2,opencv_from_imc(kp2, s2, a2),\n                              tentatives,None,**draw_params)\n    plt.figure()\n    fig, ax = plt.subplots(figsize=(15, 15))\n    ax.imshow(img_out, interpolation='nearest')\n    return\n\ndists, idxs = match(img1_key, img2_key, kps, angles, scales, descs)\ndraw_matches(img1_key, img2_key, dists, idxs,  kps, angles, scales, descs)\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nEverything seems to be working quite well. But, as you can see, we have some wrong matches among tentatives. Let’s try AdaLAM\n\ndef match_adalam(img1_key, img2_key, kps, angles, scales, descs):\n    kp1, s1, a1, descs1 = get_data(kps, angles, scales, descs, img1_key)\n    kp2, s2, a2, descs2 = get_data(kps, angles, scales, descs, img2_key)\n    matcher = AdalamFilter()\n    \n    # AdaLAM wants image sizes, so we have to read them.\n    img1 = cv2.cvtColor(cv2.imread(os.path.join(IMG_DIR, f'{img1_key}.png')), cv2.COLOR_BGR2RGB)\n    img2 = cv2.cvtColor(cv2.imread(os.path.join(IMG_DIR, f'{img2_key}.png')), cv2.COLOR_BGR2RGB)\n\n    idxs = matcher.match_and_filter(kp1, kp2,\n                            descs1, descs2,\n                            im1shape=img1.shape[:2], \n                            im2shape=img2.shape[:2],\n                            o1=a1.reshape(-1),\n                            o2=a2.reshape(-1),\n                            s1=s1.reshape(-1),\n                            s2=s2.reshape(-1))\n    # AdaLAM does not provide confidence score, so we will create dummy one\n    dists = torch.ones_like(idxs)[:,0] \n    return dists, idxs\ndists, idxs = match_adalam(img1_key, img2_key, kps, angles, scales, descs)\ndraw_matches(img1_key, img2_key, dists, idxs,  kps, angles, scales, descs)\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nWe have significantly more matches, which are also cleaner, although not ideal. There is one more thing: if we are going to submit custom matches, we have to run RANSAC ourself (if needed), here is a quote from the competition rules:\nThe \"geom\"/\"method\" field must be set to \"cv2-8pt\" if custom matches are enabled, as we assume you tune and run your favourite RANSAC algorithm, if applicable.\nThus, we need to merge RANSAC (we pick inlier threshold from the previous tutorial) and matching:\n\nimport pydegensac\n\ndef match_adalam_with_degensac(img1_key, img2_key, kps, angles, scales, descs, ds_name='phototourism'):\n    kp1, s1, a1, descs1 = get_data(kps, angles, scales, descs, img1_key)\n    kp2, s2, a2, descs2 = get_data(kps, angles, scales, descs, img2_key)\n    matcher = AdalamFilter()\n    \n    # AdaLAM wants image sizes, so we have to read them.\n    img1 = cv2.cvtColor(cv2.imread(os.path.join(IMG_DIR, f'{img1_key}.png')), cv2.COLOR_BGR2RGB)\n    img2 = cv2.cvtColor(cv2.imread(os.path.join(IMG_DIR, f'{img2_key}.png')), cv2.COLOR_BGR2RGB)\n\n    idxs = matcher.match_and_filter(kp1, kp2,\n                            descs1, descs2,\n                            im1shape=img1.shape[:2], \n                            im2shape=img2.shape[:2],\n                            o1=a1.reshape(-1),\n                            o2=a2.reshape(-1),\n                            s1=s1.reshape(-1),\n                            s2=s2.reshape(-1)).detach().cpu().numpy()\n    src_pts = kp1[idxs[:,0]]\n    dst_pts = kp2[idxs[:,1]]\n    \n    max_iters = 100000\n    if ds_name.lower() == 'phototourism':\n        inl_th = 0.5\n    elif ds_name.lower() == 'pragueparks':\n        inl_th = 1.5 \n    elif ds_name.lower() == 'googleurban':\n        inl_th = 0.75\n    else:\n        raise ValueError('Unknown dataset')\n    F, inliers_mask = pydegensac.findFundamentalMatrix(src_pts, dst_pts, inl_th, 0.999999, max_iters)\n    out_idxs = idxs[inliers_mask]\n    # AdaLAM does not provide confidence score, so we will create dummy one\n    dists = np.ones_like(out_idxs)[:,0] \n    return dists, out_idxs\n\ndists, idxs = match_adalam_with_degensac(img1_key, img2_key, kps, angles, scales, descs)\ndraw_matches(img1_key, img2_key, dists, idxs,  kps, angles, scales, descs)\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nThe last, but not least: we have to transpose output indexes, as benchmark expects them in the shape [2 x B]. Do not ask why :)\nSo, we are ready to process and save our matches. I have a bit optimized the loading process, but not much. Do not expect blazing speed :) The full script is accesible here.\n\nimport os\nimport h5py\nfrom tqdm import tqdm\nfrom PIL import Image\n\ndef match_adalam_with_degensac(kp1, kp2, s1, s2, a1, a2, descs1, descs2,\n                               h1, w1, h2, w2, ds_name='phototourism'):\n    matcher = AdalamFilter()\n    \n    idxs = matcher.match_and_filter(kp1, kp2,\n                            descs1, descs2,\n                            im1shape=(h1,w1), \n                            im2shape=(h2,w2),\n                            o1=a1.reshape(-1),\n                            o2=a2.reshape(-1),\n                            s1=s1.reshape(-1),\n                            s2=s2.reshape(-1)).detach().cpu().numpy()\n    if len(idxs) &lt; 7:\n        return np.empty((0,1), dtype=np.float32), np.empty((0,2), dtype=np.int32)\n    src_pts = kp1[idxs[:,0]]\n    dst_pts = kp2[idxs[:,1]]\n    \n    max_iters = 100000\n    if ds_name.lower() == 'phototourism':\n        inl_th = 0.5\n    elif ds_name.lower() == 'pragueparks':\n        inl_th = 1.5 \n    elif ds_name.lower() == 'googleurban':\n        inl_th = 0.75\n    else:\n        raise ValueError('Unknown dataset')\n    F, inliers_mask = pydegensac.findFundamentalMatrix(src_pts, dst_pts, inl_th, 0.999999, max_iters)\n    out_idxs = idxs[inliers_mask]\n    # AdaLAM does not provide confidence score, so we will create dummy one\n    dists = np.ones_like(out_idxs)[:,0] \n    return dists, out_idxs\n\n\nINPUT_DIR = '../imc-2021-data'\nOUT_DIR = 'extracted/cv2-dog-affnet-hardnet8'\n\nos.makedirs(OUT_DIR, exist_ok=True)\ndatasets = os.listdir(INPUT_DIR)\ndatasets = ['googleurban']\nfor ds in datasets:\n    ds_in_path = os.path.join(INPUT_DIR, ds)\n    ds_out_path = os.path.join(OUT_DIR, ds)\n    os.makedirs(ds_out_path, exist_ok=True)\n    seqs = os.listdir(ds_in_path)\n    for seq in seqs:\n        if os.path.isdir(os.path.join(ds_in_path, seq, 'set_100')):\n            seq_in_path = os.path.join(ds_in_path, seq, 'set_100', 'images')\n        else:\n            seq_in_path = os.path.join(ds_in_path, seq)\n        seq_out_path = os.path.join(ds_out_path, seq)\n        kps = load_h5(os.path.join(seq_out_path, 'keypoints.h5'))\n        angles = load_h5(os.path.join(seq_out_path, 'angles.h5'))\n        scales = load_h5(os.path.join(seq_out_path, 'scales.h5'))\n        descs = load_h5(os.path.join(seq_out_path, 'descriptors.h5'))\n        img_fnames = sorted(os.listdir(seq_in_path))[::-1]\n        num_matches = []\n        with h5py.File(f'{seq_out_path}/matches_stereo_0.h5', 'w') as f_m:\n            for i1, img1_fname in tqdm(enumerate(img_fnames)):\n                img1_key = os.path.splitext(os.path.basename(img1_fname))[0]\n                img1_fname_full = os.path.join(seq_in_path, img1_fname)\n                img1 = Image.open(img1_fname_full)\n                w1, h1 = img1.size\n                kp1, s1, a1, descs1 = get_data(kps, angles, scales, descs, img1_key)\n                for img2_fname in img_fnames[i1+1:]:\n                    img2_key = os.path.splitext(os.path.basename(img2_fname))[0]\n                    img2_fname_full = os.path.join(seq_in_path, img2_fname)\n                    img2 = Image.open(img2_fname_full)\n                    w2, h2 = img2.size\n                    match_key = f'{img1_key}-{img2_key}'\n                    kp2, s2, a2, descs2 = get_data(kps, angles, scales, descs, img2_key)\n                    _, idxs = match_adalam_with_degensac(kp1, kp2, s1, s2, a1, a2, descs1, descs2,\n                                   h1, w1, h2, w2, ds_name=ds)\n                    num_matches.append(len(idxs))\n                    if len(idxs) == 0:\n                        idxs = np.empty([0, 2], dtype=np.int32)\n                    idxs = idxs.T\n                    assert idxs.shape[0] == 2\n                    f_m[match_key] = idxs\n            print(f'Finished processing \"{ds}/{seq}\" -&gt; {np.array(num_matches).mean()} matches/image')\n        #We can use a single match file for multiview and stereo, but let's pretend that we have different ones\n        copyfile(f'{seq_out_path}/matches_stereo_0.h5', f'{seq_out_path}/matches_multiview.h5')\n\nMatching will take us couple of hours on GPU.\nEverything you (didn’t) want to know about image matching"
  },
  {
    "objectID": "posts/2021-05-27-submitting-to-imc2021-with-custom-matcher.html#intro",
    "href": "posts/2021-05-27-submitting-to-imc2021-with-custom-matcher.html#intro",
    "title": "Submitting to IMC 2021 with custom matcher",
    "section": "",
    "text": "In the previous tutorial we have created a pipeline for submitting to the IMC2021. However, we have not covered all the possibilities. First, we haven’t submissted custom matches, instead we just run a standard Lowe SNN test with cross-check. Second, we haven’t evaluate multiview (COLMAP) part. In this tutorial we will do both. I am assuming that you have completed the previous part. If not, please do, because we will be relying on the already extracted features. Let’s check if the feature are there\ncd imc2021-sample-kornia-submission\nls extracted/cv2-dog-affnet-hardnet8/googleurban/edinburgh/\nangles.h5  descriptors.h5  keypoints.h5  scales.h5  scores.h5\n\n\n\n\n\n\nWarning\n\n\n\nThere is a subtle problem with previous tutorial, so we cannot use pre-extracted features.\n\n\nSpecifically, because OpenCV SIFT does not exactly respect max_features parameter and can sometimes output 8002 features, instead of 8000. When we were importing the features alone, benchmark import_features.py script automatically re-sorted features based on the score and clip the extra 1-3 features. However, this functionality is not available for the importing custom matches. I have already corrected previous post, so you can use it for the re-extration. Or, if you are reading this whole tutorial after May 24, just ignore this.\nNow we will install AdaLAM - one of the winners of IMC2020 Challenge. It uses keypoint geometry to filter out unreliable matches.\npip install git+https://github.com/cavalli1234/AdaLAM.git\nLet’s check if it works on the sample image pair. We will read the pre-extracted features for it.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nimport os\nimport torch\nimport kornia as K\nimport kornia.feature as KF\nimport h5py\nimport json\nfrom PIL import Image\nfrom adalam import AdalamFilter\nfrom kornia_moons.feature import *\n\ndef load_h5(filename):\n    '''Loads dictionary from hdf5 file'''\n\n    dict_to_load = {}\n    try:\n        with h5py.File(filename, 'r') as f:\n            keys = [key for key in f.keys()]\n            for key in keys:\n                dict_to_load[key] = f[key][()]\n    except:\n        print('Cannot find file {}'.format(filename))\n    return dict_to_load\n\nPATH_TO_FEATS = 'extracted/cv2-dog-affnet-hardnet8/googleurban/edinburgh/'\nkps = load_h5(os.path.join(PATH_TO_FEATS, 'keypoints.h5'))\nangles = load_h5(os.path.join(PATH_TO_FEATS, 'angles.h5'))\nscales = load_h5(os.path.join(PATH_TO_FEATS, 'scales.h5'))\ndescs = load_h5(os.path.join(PATH_TO_FEATS, 'descriptors.h5'))\n\nI have selected two images, which are matching:\n\nIMG_DIR = '../imc-2021-data/googleurban/edinburgh/set_100/images/'\nimg1_key = '2b5315968bc5468c995b978620879439'\nimg2_key = '6264aee21d1b48b7985901c4bedfdbd4'\n\nimg1 = cv2.cvtColor(cv2.imread(os.path.join(IMG_DIR, f'{img1_key}.png')), cv2.COLOR_BGR2RGB)\nimg2 = cv2.cvtColor(cv2.imread(os.path.join(IMG_DIR, f'{img2_key}.png')), cv2.COLOR_BGR2RGB)\nplt.imshow(np.concatenate([img1, img2], axis=1))\n\n\n\n\n\n\n\n\nWe will start with matching and drawing the matches with OpenCV for sanity check.\n\ndef opencv_from_imc(kps, sizes, angles):\n    return [cv2.KeyPoint(kp[0], kp[1], float(s), float(a)) for  kp, s, a in zip(kps, sizes, angles)]\n\ndef get_data(kps, angles, scales, descs, img_key):\n    kp1 = kps[img_key]\n    s1 = scales[img_key]\n    a1 = angles[img_key]\n    descs1 = descs[img_key]\n    return kp1, s1, a1, descs1\n \ndef match(img1_key, img2_key, kps, angles, scales, descs):\n    kp1, s1, a1, descs1 = get_data(kps, angles, scales, descs, img1_key)\n    kp2, s2, a2, descs2 = get_data(kps, angles, scales, descs, img2_key)\n    dists, idxs = KF.match_smnn(torch.from_numpy(descs1), torch.from_numpy(descs2), 0.9)\n    return dists, idxs\n\ndef draw_matches(img1_key, img2_key, dists, idxs, kps, angles, scales, descs):\n    tentatives = cv2_matches_from_kornia(dists, idxs)\n    draw_params = dict(matchColor = (255,255,0), # draw matches in yellow color\n                   singlePointColor = None,\n                   matchesMask = [True for x in idxs], # draw only inliers\n                   flags = 2)\n    img1 = cv2.cvtColor(cv2.imread(os.path.join(IMG_DIR, f'{img1_key}.png')), cv2.COLOR_BGR2RGB)\n    img2 = cv2.cvtColor(cv2.imread(os.path.join(IMG_DIR, f'{img2_key}.png')), cv2.COLOR_BGR2RGB)\n    kp1, s1, a1, _ = get_data(kps, angles, scales, descs, img1_key)\n    kp2, s2, a2, descs2 = get_data(kps, angles, scales, descs, img2_key)\n    \n    img_out = cv2.drawMatches(img1,opencv_from_imc(kp1, s1, a1),\n                              img2,opencv_from_imc(kp2, s2, a2),\n                              tentatives,None,**draw_params)\n    plt.figure()\n    fig, ax = plt.subplots(figsize=(15, 15))\n    ax.imshow(img_out, interpolation='nearest')\n    return\n\ndists, idxs = match(img1_key, img2_key, kps, angles, scales, descs)\ndraw_matches(img1_key, img2_key, dists, idxs,  kps, angles, scales, descs)\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nEverything seems to be working quite well. But, as you can see, we have some wrong matches among tentatives. Let’s try AdaLAM\n\ndef match_adalam(img1_key, img2_key, kps, angles, scales, descs):\n    kp1, s1, a1, descs1 = get_data(kps, angles, scales, descs, img1_key)\n    kp2, s2, a2, descs2 = get_data(kps, angles, scales, descs, img2_key)\n    matcher = AdalamFilter()\n    \n    # AdaLAM wants image sizes, so we have to read them.\n    img1 = cv2.cvtColor(cv2.imread(os.path.join(IMG_DIR, f'{img1_key}.png')), cv2.COLOR_BGR2RGB)\n    img2 = cv2.cvtColor(cv2.imread(os.path.join(IMG_DIR, f'{img2_key}.png')), cv2.COLOR_BGR2RGB)\n\n    idxs = matcher.match_and_filter(kp1, kp2,\n                            descs1, descs2,\n                            im1shape=img1.shape[:2], \n                            im2shape=img2.shape[:2],\n                            o1=a1.reshape(-1),\n                            o2=a2.reshape(-1),\n                            s1=s1.reshape(-1),\n                            s2=s2.reshape(-1))\n    # AdaLAM does not provide confidence score, so we will create dummy one\n    dists = torch.ones_like(idxs)[:,0] \n    return dists, idxs\ndists, idxs = match_adalam(img1_key, img2_key, kps, angles, scales, descs)\ndraw_matches(img1_key, img2_key, dists, idxs,  kps, angles, scales, descs)\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nWe have significantly more matches, which are also cleaner, although not ideal. There is one more thing: if we are going to submit custom matches, we have to run RANSAC ourself (if needed), here is a quote from the competition rules:\nThe \"geom\"/\"method\" field must be set to \"cv2-8pt\" if custom matches are enabled, as we assume you tune and run your favourite RANSAC algorithm, if applicable.\nThus, we need to merge RANSAC (we pick inlier threshold from the previous tutorial) and matching:\n\nimport pydegensac\n\ndef match_adalam_with_degensac(img1_key, img2_key, kps, angles, scales, descs, ds_name='phototourism'):\n    kp1, s1, a1, descs1 = get_data(kps, angles, scales, descs, img1_key)\n    kp2, s2, a2, descs2 = get_data(kps, angles, scales, descs, img2_key)\n    matcher = AdalamFilter()\n    \n    # AdaLAM wants image sizes, so we have to read them.\n    img1 = cv2.cvtColor(cv2.imread(os.path.join(IMG_DIR, f'{img1_key}.png')), cv2.COLOR_BGR2RGB)\n    img2 = cv2.cvtColor(cv2.imread(os.path.join(IMG_DIR, f'{img2_key}.png')), cv2.COLOR_BGR2RGB)\n\n    idxs = matcher.match_and_filter(kp1, kp2,\n                            descs1, descs2,\n                            im1shape=img1.shape[:2], \n                            im2shape=img2.shape[:2],\n                            o1=a1.reshape(-1),\n                            o2=a2.reshape(-1),\n                            s1=s1.reshape(-1),\n                            s2=s2.reshape(-1)).detach().cpu().numpy()\n    src_pts = kp1[idxs[:,0]]\n    dst_pts = kp2[idxs[:,1]]\n    \n    max_iters = 100000\n    if ds_name.lower() == 'phototourism':\n        inl_th = 0.5\n    elif ds_name.lower() == 'pragueparks':\n        inl_th = 1.5 \n    elif ds_name.lower() == 'googleurban':\n        inl_th = 0.75\n    else:\n        raise ValueError('Unknown dataset')\n    F, inliers_mask = pydegensac.findFundamentalMatrix(src_pts, dst_pts, inl_th, 0.999999, max_iters)\n    out_idxs = idxs[inliers_mask]\n    # AdaLAM does not provide confidence score, so we will create dummy one\n    dists = np.ones_like(out_idxs)[:,0] \n    return dists, out_idxs\n\ndists, idxs = match_adalam_with_degensac(img1_key, img2_key, kps, angles, scales, descs)\ndraw_matches(img1_key, img2_key, dists, idxs,  kps, angles, scales, descs)\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nThe last, but not least: we have to transpose output indexes, as benchmark expects them in the shape [2 x B]. Do not ask why :)\nSo, we are ready to process and save our matches. I have a bit optimized the loading process, but not much. Do not expect blazing speed :) The full script is accesible here.\n\nimport os\nimport h5py\nfrom tqdm import tqdm\nfrom PIL import Image\n\ndef match_adalam_with_degensac(kp1, kp2, s1, s2, a1, a2, descs1, descs2,\n                               h1, w1, h2, w2, ds_name='phototourism'):\n    matcher = AdalamFilter()\n    \n    idxs = matcher.match_and_filter(kp1, kp2,\n                            descs1, descs2,\n                            im1shape=(h1,w1), \n                            im2shape=(h2,w2),\n                            o1=a1.reshape(-1),\n                            o2=a2.reshape(-1),\n                            s1=s1.reshape(-1),\n                            s2=s2.reshape(-1)).detach().cpu().numpy()\n    if len(idxs) &lt; 7:\n        return np.empty((0,1), dtype=np.float32), np.empty((0,2), dtype=np.int32)\n    src_pts = kp1[idxs[:,0]]\n    dst_pts = kp2[idxs[:,1]]\n    \n    max_iters = 100000\n    if ds_name.lower() == 'phototourism':\n        inl_th = 0.5\n    elif ds_name.lower() == 'pragueparks':\n        inl_th = 1.5 \n    elif ds_name.lower() == 'googleurban':\n        inl_th = 0.75\n    else:\n        raise ValueError('Unknown dataset')\n    F, inliers_mask = pydegensac.findFundamentalMatrix(src_pts, dst_pts, inl_th, 0.999999, max_iters)\n    out_idxs = idxs[inliers_mask]\n    # AdaLAM does not provide confidence score, so we will create dummy one\n    dists = np.ones_like(out_idxs)[:,0] \n    return dists, out_idxs\n\n\nINPUT_DIR = '../imc-2021-data'\nOUT_DIR = 'extracted/cv2-dog-affnet-hardnet8'\n\nos.makedirs(OUT_DIR, exist_ok=True)\ndatasets = os.listdir(INPUT_DIR)\ndatasets = ['googleurban']\nfor ds in datasets:\n    ds_in_path = os.path.join(INPUT_DIR, ds)\n    ds_out_path = os.path.join(OUT_DIR, ds)\n    os.makedirs(ds_out_path, exist_ok=True)\n    seqs = os.listdir(ds_in_path)\n    for seq in seqs:\n        if os.path.isdir(os.path.join(ds_in_path, seq, 'set_100')):\n            seq_in_path = os.path.join(ds_in_path, seq, 'set_100', 'images')\n        else:\n            seq_in_path = os.path.join(ds_in_path, seq)\n        seq_out_path = os.path.join(ds_out_path, seq)\n        kps = load_h5(os.path.join(seq_out_path, 'keypoints.h5'))\n        angles = load_h5(os.path.join(seq_out_path, 'angles.h5'))\n        scales = load_h5(os.path.join(seq_out_path, 'scales.h5'))\n        descs = load_h5(os.path.join(seq_out_path, 'descriptors.h5'))\n        img_fnames = sorted(os.listdir(seq_in_path))[::-1]\n        num_matches = []\n        with h5py.File(f'{seq_out_path}/matches_stereo_0.h5', 'w') as f_m:\n            for i1, img1_fname in tqdm(enumerate(img_fnames)):\n                img1_key = os.path.splitext(os.path.basename(img1_fname))[0]\n                img1_fname_full = os.path.join(seq_in_path, img1_fname)\n                img1 = Image.open(img1_fname_full)\n                w1, h1 = img1.size\n                kp1, s1, a1, descs1 = get_data(kps, angles, scales, descs, img1_key)\n                for img2_fname in img_fnames[i1+1:]:\n                    img2_key = os.path.splitext(os.path.basename(img2_fname))[0]\n                    img2_fname_full = os.path.join(seq_in_path, img2_fname)\n                    img2 = Image.open(img2_fname_full)\n                    w2, h2 = img2.size\n                    match_key = f'{img1_key}-{img2_key}'\n                    kp2, s2, a2, descs2 = get_data(kps, angles, scales, descs, img2_key)\n                    _, idxs = match_adalam_with_degensac(kp1, kp2, s1, s2, a1, a2, descs1, descs2,\n                                   h1, w1, h2, w2, ds_name=ds)\n                    num_matches.append(len(idxs))\n                    if len(idxs) == 0:\n                        idxs = np.empty([0, 2], dtype=np.int32)\n                    idxs = idxs.T\n                    assert idxs.shape[0] == 2\n                    f_m[match_key] = idxs\n            print(f'Finished processing \"{ds}/{seq}\" -&gt; {np.array(num_matches).mean()} matches/image')\n        #We can use a single match file for multiview and stereo, but let's pretend that we have different ones\n        copyfile(f'{seq_out_path}/matches_stereo_0.h5', f'{seq_out_path}/matches_multiview.h5')\n\nMatching will take us couple of hours on GPU."
  },
  {
    "objectID": "posts/2021-05-27-submitting-to-imc2021-with-custom-matcher.html#creating-config-json-file",
    "href": "posts/2021-05-27-submitting-to-imc2021-with-custom-matcher.html#creating-config-json-file",
    "title": "Submitting to IMC 2021 with custom matcher",
    "section": "Creating config json file",
    "text": "Creating config json file\nIn addition to features and matches, we should submit a config file, which tells the benchmark, how the features should be matched and which RANSAC (no options, only cv2-8pt for using custom matches) we prefer. So, we will create two config files - one, which standard matcher and second one, which uses AdaLAM.\nFirst part of the config is metadata – information about the method and authors. If your method is under review, you may want to set flag publish_anonymously to True.\n\nmetadata_dict =  {\n    \"publish_anonymously\": False,\n    \"authors\": \"Dmytro Mishkin, Milan Pultar and kornia team\",\n    \"contact_email\": \"ducha.aiki@gmail.com\",\n    \"method_name\": \"CV-DoG-AffNet-HardNet8-AdaLAM-DEGENSAC\",\n    \"method_description\": \n    r\"\"\"OpeCV SIFT keypoints 8000 features, followed by the AffNet normalization \n    and HardNet8 descriptor as implemented in kornia.\n    Matched using AdaLAM with DEGENSAC\"\"\",\n    \"link_to_website\": \"https://github.com/kornia/kornia\",\n    \"link_to_pdf\": \"https://arxiv.org/abs/2007.09699\"\n}\n\nSecond part is config_common: it tells the benchmark, which keypoints and descriptors you use. We will also need this names when importing our features during tuning on the validation set.\n\nconfig_common_dict =  {\"json_label\": \"dog-affnet-hardnet8-degensac-adalam\",\n    \"keypoint\": \"cv2dog\",\n    \"descriptor\": \"affnethardnet8\",\n    \"num_keypoints\": 8000}\n\nFinally, we have to specify robust geometry estimation method. We have no other choice than cv2-8pt, as pre-filtering with DEGENSAC is already performed.\n\nfrom copy import deepcopy\n\n\ngeom_template_dict =  {\"method\": \"cv2-8pt\"\n            }\n\nLet’s assemble and save our base config.\n\nimport json\n\nbase_config =  {\n    \"metadata\": metadata_dict,\n    \"config_common\": config_common_dict,\n    \"config_phototourism_stereo\": {\n        \"use_custom_matches\": True,\n        \"custom_matches_name\": \"adalam\",\n        \"geom\": deepcopy(geom_template_dict)\n        },\n    \"config_phototourism_multiview\": {\n        \"use_custom_matches\": True,\n        \"custom_matches_name\": \"adalam\",\n        \"colmap\": {}},\n    \n    \"config_pragueparks_stereo\": {\n        \"use_custom_matches\": True,\n        \"custom_matches_name\": \"adalam\",\n        \"geom\": deepcopy(geom_template_dict)\n        },\n    \"config_pragueparks_multiview\": {\n        \"use_custom_matches\": True,\n        \"custom_matches_name\": \"adalam\",\n        \"colmap\": {}},\n    \"config_googleurban_stereo\": {\n        \"use_custom_matches\": True,\n        \"custom_matches_name\": \"adalam\",\n        \"geom\": deepcopy(geom_template_dict)\n        },\n    \"config_googleurban_multiview\": {\n        \"use_custom_matches\": True,\n        \"custom_matches_name\": \"adalam\",\n        \"colmap\": {}}\n}\n\nFinally, benchmark expects multiple configs, so we have to create a list, and then we can save our config\n\nimport json \n\nwith open('base_config_adalam.json', 'w') as f:\n    json.dump([base_config], f, indent=2)"
  },
  {
    "objectID": "posts/2021-05-27-submitting-to-imc2021-with-custom-matcher.html#creating-final-submission",
    "href": "posts/2021-05-27-submitting-to-imc2021-with-custom-matcher.html#creating-final-submission",
    "title": "Submitting to IMC 2021 with custom matcher",
    "section": "Creating final submission",
    "text": "Creating final submission\nIts time to create our final submission!\n\n\nconfigs = []\ncurrent_config = deepcopy(base_config)\n\n    \ncurrent_config['metadata']['method_name'] = 'KORNIA TUTORIAL AdaLAM CV-DoG-AffNet-HardNet8'\n\nlabel = current_config['config_common']['json_label'] \ncurrent_config['config_common']['json_label']  = f'{label}'\nconfigs.append(current_config)\n\nprint (current_config)\nwith open('final_submission_adalam.json', 'w') as f:\n    json.dump(configs, f, indent=2)\n\n{'metadata': {'publish_anonymously': False, 'authors': 'Dmytro Mishkin, Milan Pultar and kornia team', 'contact_email': 'ducha.aiki@gmail.com', 'method_name': 'KORNIA TUTORIAL AdaLAM CV-DoG-AffNet-HardNet8', 'method_description': 'OpeCV SIFT keypoints 8000 features, followed by the AffNet normalization \\n    and HardNet8 descriptor as implemented in kornia.\\n    Matched using AdaLAM with DEGENSAC', 'link_to_website': 'https://github.com/kornia/kornia', 'link_to_pdf': 'https://arxiv.org/abs/2007.09699'}, 'config_common': {'json_label': 'dog-affnet-hardnet8-degensac-adalam', 'keypoint': 'cv2dog', 'descriptor': 'affnethardnet8', 'num_keypoints': 8000}, 'config_phototourism_stereo': {'use_custom_matches': True, 'custom_matches_name': 'adalam', 'geom': {'method': 'cv2-8pt'}}, 'config_phototourism_multiview': {'use_custom_matches': True, 'custom_matches_name': 'adalam', 'colmap': {}}, 'config_pragueparks_stereo': {'use_custom_matches': True, 'custom_matches_name': 'adalam', 'geom': {'method': 'cv2-8pt'}}, 'config_pragueparks_multiview': {'use_custom_matches': True, 'custom_matches_name': 'adalam', 'colmap': {}}, 'config_googleurban_stereo': {'use_custom_matches': True, 'custom_matches_name': 'adalam', 'geom': {'method': 'cv2-8pt'}}, 'config_googleurban_multiview': {'use_custom_matches': True, 'custom_matches_name': 'adalam', 'colmap': {}}}\n\n\nSubmission Zip file should have folder structure as follow:\n ├── config.json\n ├── [Dataset 1]\n │   ├── [Sequence 1]\n │   │   ├── keypoints.h5\n │   │   ├── descriptors.h5\n │   │   ├── matches.h5\n │   ├── [Sequence 2]\n │   │   ├── ...\n ├── [Dataset 2]\n │   ├── ...\nSo we have to just copy our features, add config and zip them.\ncp final_submission_adalam.json extracted/cv2-dog-affnet-hardnet8/config.json\ncd extracted/cv2-dog-affnet-hardnet8\nzip -r submission.zip *\nLast step before the submission - check the submission for correctness with provided script\ncd ../../../image-matching-benchmark\npython -utt submission_validator.py --submit_file_path ../imc2021-sample-kornia-submission/extracted/cv2-dog-affnet-hardnet8/submission.zip --benchmark_repo_path . --raw_data_path ../imc-2021-data/ --datasets googleurban phototourism pragueparks\nIf everything is correct, you will see:\nValidating method 1/1: \"dog-affnet-hardnet8-degensac\"\n['googleurban', 'phototourism', 'pragueparks']\nRunning: googleurban, stereo track\nRunning: googleurban, multiview track\nRunning: phototourism, stereo track\nRunning: phototourism, multiview track\nRunning: pragueparks, stereo track\nRunning: pragueparks, multiview track\nValidating key \"config_googleurban_stereo\"\nValidating key \"config_googleurban_multiview\"\nValidating key \"config_phototourism_stereo\"\nValidating key \"config_phototourism_multiview\"\nValidating key \"config_pragueparks_stereo\"\nValidating key \"config_pragueparks_multiview\"\nAnd file submission_log.txt will appear near our .zip file.\ncat ../imc2021-sample-kornia-submission/extracted/cv2-dog-affnet-hardnet8/submission_log.txt\nSubmission is in proper format, please submit to IMW 2021 website.\nThat’s all, folks! We can submit! But, please, do not just submit this sample submission - make your own :)"
  },
  {
    "objectID": "posts/2020-03-27-intro.html",
    "href": "posts/2020-03-27-intro.html",
    "title": "The Role of Wide Baseline Stereo in the Deep Learning World",
    "section": "",
    "text": "The wide multiple baseline stereo (WxBS) is a process of establishing a sufficient number of pixel or region correspondences from two or more images depicting the same scene to estimate the geometric relationship between cameras, which produced these images. Typically, WxBS relies on the scene rigidity – the assumption that there is no motion in the scene except the motion of the camera itself. The stereo problem is called wide multiple baseline if the images are significantly different in more than one aspect: viewpoint, illumination, time of acquisition, and so on. Historically, people were focused on the simpler problem with a single baseline, which was geometrical, i.e., viewpoint difference between cameras, and the area was known as wide baseline stereo. Nowadays, the field is mature and research is focused on solving more challenging multi-baseline problems.\nWxBS is a building block of many popular computer vision applications, where spatial localization or 3D world understanding is required – panorama stitching, 3D reconstruction, image retrieval, SLAM, etc.\nIf the wide baseline stereo is a new concept for you, I recommend checking the examplanation in simple terms.\n\nWhere does wide baseline stereo come from?\nAs often happens, a new problem arises from the old – narrow or short baseline stereo. In the narrow baseline stereo, images are taken from nearby positions, often exactly at the same time. One could find correspondence for the point \\((x,y)\\) from the image \\(I_1\\) in the image \\(I_2\\) by simply searching in some small window around \\((x,y)\\) or, assuming that camera pair is calibrated and the images are rectified – by searching along the epipolar line.\n\n\nOne of the first, if not the first, approaches to the wide baseline stereo problem was proposed by Schmid and Mohr in 1995. Given the difficulty of the wide multiple baseline stereo task at the moment, only a single — geometrical – baseline was considered, thus the name – wide baseline stereo (WBS). The idea of Schmid and Mohr was to equip each keypoint with an invariant descriptor. This allowed establishing tentative correspondences between keypoints under viewpoint and illumination changes, as well as occlusions. One of the stepping stones was the corner detector by Harris and Stevens , initially used for the application of tracking. It is worth a mention, that there were other good choices for the local feature detector at the time, starting with the Forstner , Moravec and Beaudet feature detectors .\nThe Schmid and Mohr approach was later extended by Beardsley, Torr and Zisserman by adding RANSAC robust geometry estimation and later refined by Pritchett and Zisserman in 1998. The general pipeline remains mostly the same until now , which is shown in Figure below.\n\n\nLet’s write down the WxBS algorithm:\n\nCompute interest points/regions in all images independently\nFor each interest point/region compute a descriptor of their neigborhood (local patch).\nEstablish tentative correspondences between interest points based on their descriptors.\nRobustly estimate geometric relation between two images based on tentative correspondences with RANSAC.\n\nThe reasoning behind each step is described in this separate post.\nEverything you (didn’t) want to know about image matching"
  },
  {
    "objectID": "posts/2020-03-27-intro.html#rise-of-wide-multiple-baseline-stereo",
    "href": "posts/2020-03-27-intro.html#rise-of-wide-multiple-baseline-stereo",
    "title": "The Role of Wide Baseline Stereo in the Deep Learning World",
    "section": "",
    "text": "The wide multiple baseline stereo (WxBS) is a process of establishing a sufficient number of pixel or region correspondences from two or more images depicting the same scene to estimate the geometric relationship between cameras, which produced these images. Typically, WxBS relies on the scene rigidity – the assumption that there is no motion in the scene except the motion of the camera itself. The stereo problem is called wide multiple baseline if the images are significantly different in more than one aspect: viewpoint, illumination, time of acquisition, and so on. Historically, people were focused on the simpler problem with a single baseline, which was geometrical, i.e., viewpoint difference between cameras, and the area was known as wide baseline stereo. Nowadays, the field is mature and research is focused on solving more challenging multi-baseline problems.\nWxBS is a building block of many popular computer vision applications, where spatial localization or 3D world understanding is required – panorama stitching, 3D reconstruction, image retrieval, SLAM, etc.\nIf the wide baseline stereo is a new concept for you, I recommend checking the examplanation in simple terms.\n\nWhere does wide baseline stereo come from?\nAs often happens, a new problem arises from the old – narrow or short baseline stereo. In the narrow baseline stereo, images are taken from nearby positions, often exactly at the same time. One could find correspondence for the point \\((x,y)\\) from the image \\(I_1\\) in the image \\(I_2\\) by simply searching in some small window around \\((x,y)\\) or, assuming that camera pair is calibrated and the images are rectified – by searching along the epipolar line.\n\n\nOne of the first, if not the first, approaches to the wide baseline stereo problem was proposed by Schmid and Mohr in 1995. Given the difficulty of the wide multiple baseline stereo task at the moment, only a single — geometrical – baseline was considered, thus the name – wide baseline stereo (WBS). The idea of Schmid and Mohr was to equip each keypoint with an invariant descriptor. This allowed establishing tentative correspondences between keypoints under viewpoint and illumination changes, as well as occlusions. One of the stepping stones was the corner detector by Harris and Stevens , initially used for the application of tracking. It is worth a mention, that there were other good choices for the local feature detector at the time, starting with the Forstner , Moravec and Beaudet feature detectors .\nThe Schmid and Mohr approach was later extended by Beardsley, Torr and Zisserman by adding RANSAC robust geometry estimation and later refined by Pritchett and Zisserman in 1998. The general pipeline remains mostly the same until now , which is shown in Figure below.\n\n\nLet’s write down the WxBS algorithm:\n\nCompute interest points/regions in all images independently\nFor each interest point/region compute a descriptor of their neigborhood (local patch).\nEstablish tentative correspondences between interest points based on their descriptors.\nRobustly estimate geometric relation between two images based on tentative correspondences with RANSAC.\n\nThe reasoning behind each step is described in this separate post."
  },
  {
    "objectID": "posts/2020-03-27-intro.html#quick-expansion",
    "href": "posts/2020-03-27-intro.html#quick-expansion",
    "title": "The Role of Wide Baseline Stereo in the Deep Learning World",
    "section": "Quick expansion",
    "text": "Quick expansion\nThis algorithm significantly changed computer vision landscape for next forteen years.\nSoon after the introduction of the WBS algorithm, it became clear that its quality significantly depends on the quality of each component, i.e., local feature detector, descriptor, and geometry estimation. Local feature detectors were designed to be as invariant as possible, backed up by the scale-space theory, most notable developed by Lindenberg . A plethora of new detectors and descriptors were proposed in that time. We refer the interested reader to these two surveys: by Tuytelaars and Mikolajczyk (2008) and by Csurka (2018). Among the proposed local features is one of the most cited computer vision papers ever – SIFT local feature . Besides the SIFT descriptor itself,\nLowe’s paper incorporated several important steps, proposed earlier with his co-authors, to the matching pipeline. Specifically, they are quadratic fitting of the feature responses for precise keypoint localization , using the Best-Bin-First kd-tree as an approximate nearest neightbor search engine to speed-up the tentative correspondences generation, and using second-nearest neighbor (SNN) ratio to filter the tentative matches. It is worth noting that SIFT feature became popular only after Mikolajczyk benchmark paper that showed its superiority to the rest of alternatives.\nRobust geometry estimation was also a hot topic: a lot of improvements over vanilla RANSAC were proposed. For example, LO-RANSAC proposed an additional local optimization step into RANSAC to significantly decrease the number of required steps. PROSAC takes into account the tentative correspondences matching score during sampling to speed up the procedure. DEGENSAC improved the quality of the geometry estimation in the presence of a dominant plane in the images, which is the typical case for urban images. We refer the interested reader to the survey by Choi .\nSuccess of wide baseline stereo with SIFT features led to aplication of its components to other computer vision tasks, which were reformulated through wide baseline stereo lens:\n\nScalable image search. Sivic and Zisserman in famous “Video Google” paper proposed to treat local features as “visual words” and use ideas from text processing for searching in image collections. Later even more WBS elements were re-introduced to image search, most notable – spatial verification: simplified RANSAC procedure to verify if visual word matches were spatially consistent.\n\n\n\nImage classification was performed by placing some classifier (SVM, random forest, etc) on top of some encoding of the SIFT-like descriptors, extracted sparsely or densely.\n\n\n\nObject detection was formulated as relaxed wide baseline stereo problem or as classification of SIFT-like features inside a sliding window \n\n\n\n\nSemantic segmentation was performed by classicication of local region descriptors, typically, SIFT and color features and postprocessing afterwards.\n\nOf course,wide baseline stereo was also used for its direct applications:\n\n3D reconstruction was based on camera poses and 3D points, estimated with help of SIFT features \n\n\n\nSLAM(Simultaneous localization and mapping) were based on fast version of local feature detectors and descriptors. \nPanorama stiching and, more generally, feature-based image registration were initalized with a geometry obtained by WBS and then further optimized"
  },
  {
    "objectID": "posts/2020-03-27-intro.html#deep-learning-invasion-retreal-to-the-geometrical-fortress",
    "href": "posts/2020-03-27-intro.html#deep-learning-invasion-retreal-to-the-geometrical-fortress",
    "title": "The Role of Wide Baseline Stereo in the Deep Learning World",
    "section": "Deep Learning Invasion: retreal to the geometrical fortress",
    "text": "Deep Learning Invasion: retreal to the geometrical fortress\nIn 2012 the deep learning-based AlexNet approach beat all methods in image classification at the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). Soon after, Razavian et al. have shown that convolutional neural networks (CNNs) pre-trained on the Imagenet outperform more complex traditional solutions in image and scene classification, object detection and image search, see Figure below. The performance gap between deep leaning and “classical” solutions was large and quickly increasing. In addition, deep learning pipelines, be it off-the-shelf pretrained, fine-tuned or the end-to-end learned networks, are simple from the engineering perspective. That is why the deep learning algorithms quickly become the default option for lots of computer vision problems.\n\nHowever, there was still a domain, where deep learned solutions failed, sometimes spectacularly: geometry-related tasks. Wide baseline stereo , visual localization and SLAM are still areas, where the classical wide baseline stereo dominates .\nThe full reasons why convolution neural network pipelines are struggling to perform tasks that are related to geometry, and how to fix that, are yet to be understood. The observations from the recent papers are following:\n\nCNN-based pose predictions predictions are roughly equivalent to the retrieval of the most similar image from the training set and outputing its pose . This kind of behaviour is also observed in a related area: single-view 3D reconstruction performed by deep networks is essentially a retrieval of the most similar 3D model from the training set .\nGeometric and arithmetic operations are hard to represent via vanilla neural networks (i.e., matrix multiplication followed by non-linearity) and they may require specialized building blocks, approximating operations of algorithmic or geometric methods, e.g. spatial transformers and arithmetic units . Even with such special-purpose components, the deep learning solutions require “careful initialization, restricting parameter space, and regularizing for sparsity” .\nVanilla CNNs suffer from sensitivity to geometric transformations like scaling and rotation or even translation . The sensitivity to translations might sound counter-intuitive, because the concolution operation by definition is translation-covariant. However, a typical CNN contains also zero-padding and downscaling operations, which break the covariance . Unlike them, classical local feature detectors are grounded on scale-space and image processing theories. Some of the classical methods deal with the issue by explicit geometric normalization of the patches before description.\nCNNs predictions can be altered by a change in a small localized area or even a single pixel , while the wide baseline stereo methods require the consensus of different independent regions."
  },
  {
    "objectID": "posts/2020-03-27-intro.html#today-assimilation-and-merging",
    "href": "posts/2020-03-27-intro.html#today-assimilation-and-merging",
    "title": "The Role of Wide Baseline Stereo in the Deep Learning World",
    "section": "Today: assimilation and merging",
    "text": "Today: assimilation and merging\n\nWide baseline stereo as a task: formulate differentiably and learn modules\nThis leads us to the following question – is deep learning helping WxBS today? The answer is yes. After the quick interest in the black-box-style models, the current trend is to design deep learning solutions for the wide baseline stereo in a modular fashion , resembling the one in Figure below. Such modules are learned separately. For example, the HardNet descriptor replaces SIFT local descriptor. The Hessian detector can be replaced by deep learned detectors like KeyNet or the joint detector-descriptor . The matching and filtering are performed by the SuperGlue matching network, etc. There have been attempts to formulate the full pipeline solving problem like SLAM in a differentiable way, combining the advantages of structured and learning-based approaches.\n\n\n\n\nWide baseline stereo as a idea: consensus of local independent predictions\nOn the other hand, as an algorithm, wide baseline stereo is summarized into two main ideas\n\nImage should be represented as set of local parts, robust to occlusion, and not influencing each other.\nDecision should be based on spatial consensus of local feature correspondences.\n\nOne of modern revisit of wide baseline stereo ideas is Capsule Networks. Unlike vanilla CNNs, capsule networks encode not only the intensity of feature response, but also its location. Geometric agreement between “object parts” is a requirement for outputing a confident prediction.\nSimilar ideas are now explored for ensuring adversarial robustness of CNNs.\nAnother way of using “consensus of local independent predictions” is used in Cross-transformers paper: spatial attention helps to select relevant feature for few-shot learning, see Figure below.\nWhile wide multiple baseline stereo is a mature field now and does not attract even nearly as much attention as before, it continues to play an important role in computer vision."
  },
  {
    "objectID": "posts/2023-07-05-imc2023-recap.html",
    "href": "posts/2023-07-05-imc2023-recap.html",
    "title": "Image Matching Challenge 2023: The Unbearable Weight of the Bundle Adjustment",
    "section": "",
    "text": "This year Image Matching Challenge introduced two big changes. First, we went from two-view matching to full Structure-from-Motion as a task. Actually, the multiview track was present in pre-Kaggle era of IMC – in 2019, 2020 and 2021, so we kind of returned to the roots.\nParticipants were given with the sets of images and output should be the cameras poses for all of them. Second, thanks to our sponsors – Google, Haiper, and Kaggle itself, we were able to propose $50k prize fund. With the strict open license condition (MIT/Apache 2/etc) for the “in-money solution”.\n\n\nWhile going from image pairs to image sets might seem a small change, it has a significant impact on cumpute requirements. For the two-view case, one can run almost everything on GPU, e.g. SuperPoint for feature detection, SuperGlue for image matching, or the LoFTR for detector-less image matching. The only CPU part is the RANSAC, which can take as little as 10ms per image pair for VSAC, so not a big deal.\nFor the multiview case, on the other hand, one needs to performs the bundle-adjustment, which is a CPU-heavy task. Kaggle virtual machines, in addition to that, offer only 2-core CPU, so the 3D reconstruction itself becomes the main computational bottleneck of the whole process.\n\n\n\nTo provide the participants a headstart, we have worked with Kaggle engineers to include the pycolmap into the default Kaggle kernels. Based on it, we have provided an example submission, which uses local features included in kornia library: LoFTR, DISK, and KeyNet-AffNet-HardNet.\nAll of them have Apache 2 license, and has shown a good performance in one of the previous IMCs: LoFTR was a part of top solutions in 2022, recap, DISK - in 2020, recap and 2021, and KeyNet-AffNet-HardNet - one of the leaders in the original IJCV-2020 publication.\nThe LoFTR example, however, was so heavy, that was causing time-out error without the modifications.\nEverything you (didn’t) want to know about image matching"
  },
  {
    "objectID": "posts/2023-07-05-imc2023-recap.html#the-unbearable-weight-of-the-bundle-adjustment-and-50k-money-prize",
    "href": "posts/2023-07-05-imc2023-recap.html#the-unbearable-weight-of-the-bundle-adjustment-and-50k-money-prize",
    "title": "Image Matching Challenge 2023: The Unbearable Weight of the Bundle Adjustment",
    "section": "",
    "text": "This year Image Matching Challenge introduced two big changes. First, we went from two-view matching to full Structure-from-Motion as a task. Actually, the multiview track was present in pre-Kaggle era of IMC – in 2019, 2020 and 2021, so we kind of returned to the roots.\nParticipants were given with the sets of images and output should be the cameras poses for all of them. Second, thanks to our sponsors – Google, Haiper, and Kaggle itself, we were able to propose $50k prize fund. With the strict open license condition (MIT/Apache 2/etc) for the “in-money solution”.\n\n\nWhile going from image pairs to image sets might seem a small change, it has a significant impact on cumpute requirements. For the two-view case, one can run almost everything on GPU, e.g. SuperPoint for feature detection, SuperGlue for image matching, or the LoFTR for detector-less image matching. The only CPU part is the RANSAC, which can take as little as 10ms per image pair for VSAC, so not a big deal.\nFor the multiview case, on the other hand, one needs to performs the bundle-adjustment, which is a CPU-heavy task. Kaggle virtual machines, in addition to that, offer only 2-core CPU, so the 3D reconstruction itself becomes the main computational bottleneck of the whole process.\n\n\n\nTo provide the participants a headstart, we have worked with Kaggle engineers to include the pycolmap into the default Kaggle kernels. Based on it, we have provided an example submission, which uses local features included in kornia library: LoFTR, DISK, and KeyNet-AffNet-HardNet.\nAll of them have Apache 2 license, and has shown a good performance in one of the previous IMCs: LoFTR was a part of top solutions in 2022, recap, DISK - in 2020, recap and 2021, and KeyNet-AffNet-HardNet - one of the leaders in the original IJCV-2020 publication.\nThe LoFTR example, however, was so heavy, that was causing time-out error without the modifications."
  },
  {
    "objectID": "posts/2023-07-05-imc2023-recap.html#new-datasets-new-challenges-uav-to-ground-day-night-repeated-patterns-wiry-objects-scale-change",
    "href": "posts/2023-07-05-imc2023-recap.html#new-datasets-new-challenges-uav-to-ground-day-night-repeated-patterns-wiry-objects-scale-change",
    "title": "Image Matching Challenge 2023: The Unbearable Weight of the Bundle Adjustment",
    "section": "New datasets, new challenges: UAV-to-ground, day-night, repeated patterns, wiry objects, scale change",
    "text": "New datasets, new challenges: UAV-to-ground, day-night, repeated patterns, wiry objects, scale change\nIn 2023 we have prepared 3 new datasets - Heritage, Haiper and Urban.\nEach dataset has been split into “training” – public, and private part. The parts were geographically disjoint, but shared a similar nuisance factors. We haven’t yet decided if we are going to release “hidden” part or not.\n\nHeritage\nThis dataset features the high resolution photos of ancient buildings, taken with DLSR cameras from the ground, as well as UAV photos. Particular challenges are:\n\nlarge scale change (up to 20x), from that overview photo to the close-up of the small detail, together with in-plane rotation\n\n\n\n“banana effect”, when the flat surface is often reconstructed as curved due to low overlap between consecutive frames and slight misalignments, which accumulate altogether.\n\n\n\nhigh-resolution image processing. See above about the computational challenges of bundle adjustment and then multiply that by high resolution AND large image number. Also, the most of deep learning features (DISK, LoFTR) can easily give you CUDA OOM error when run on 20 Mp image pair.\n\n\n\nHaiper\nHaiper (training) dataset is similar to captures for NERFs - layered “dome” of cameras, going around some object. The object itself is often thin (bicycle) or textureless (statue). The test part of the dataset has very small number of images, making the viewpoint difference the biggest challenge.\n\nIf IMC-2023 have discovered and used “Map-free Visual Relocalization” for training their pipelines, that would likely help them with Haiper dataset as well. However, everyone has missed this opportunity, including us - organizers.\n\n\nUrban\nThe Urban dataset is, probably, the most similar one to IMC2020 PhotoTourism dataset. It covers photos of buldings in city, similar to PhotoTourism.\nHere is the photos of the “Kyiv Puppet Theater” – the easiest and public – part of the dataset.\n\n\n\nimage.png\n\n\nIn addition to the day-night photos, the hidden part of the dataset features highly symmetrical objects, such as the photo I took yesterday in Český Krumlov. The camera poses are actually look into each other, so visual overlap is zero. The only chance of non-wrongly-matching them, is either considering all the photos altogether, or utilizing the background. None of the existing feature matching solutions does this, as far as I know."
  },
  {
    "objectID": "posts/2023-07-05-imc2023-recap.html#findings-from-the-competition",
    "href": "posts/2023-07-05-imc2023-recap.html#findings-from-the-competition",
    "title": "Image Matching Challenge 2023: The Unbearable Weight of the Bundle Adjustment",
    "section": "Findings from the competition",
    "text": "Findings from the competition\n\nThere are no 3d reconstruction libraries besides colmap\nAt least, there are none, which you can easily compile on Kaggle kernel and then use from python notebook. I personally really hoped to see some global SfM solution like Theia or maybe OpenSfM to appear among the top-solution,as it could provide a significant speed-up over the incremental SfM like Colmap. Python bindings and ease of compilation is crutial factor here.\n\n\nNo NERF-like or any other solution than classical SfM\nDespite all the progress, it seems that if one doesn’t have any additional information, such as RGBD, or initial ARKit cameras poses, the best thing one can do with a challenging image collection, is classical SfM. No SparsePose, no FORGE\n\n\nGlobal descriptor-based co-visibility is hard\nGiven that exhaustive image matching grows quadratically with number of images, it is very tempting to filter out some of those image pairs, based on some kind of covisibility criterion. And the simplest/fastest to compute is global descriptor one - you get a single vector per image, calculate global image similarity and remove those image pairs, which similarity is below threshold. Or take the top-K most similar images. In fact, I have used such approach in our paper “Efficient Initial Pose-graph Generation for Global SfM”. However, what worked on large (thousands) image collections with dense viewpoint coverage, as 1DSfM, does not work that well on sparse image collections with many close-ups, rotations and illumination changes.\nSome teams had moderate success with NetVLAD and DINOv2 global descriptors, however, the best strategy, it seems, to use the two-view matching itself, but on a smaller resolution – such as SuperGlue, or KeyNet-AffNet-HardNet-AdaLAM.\n\n\nPixelSfM is good idea, but needs improvements\nMany participants has tried to improve the initial SfM camera poses by utilizing PixelPerfectSfM – the feature-metric bundle adjustment. While it improves results, it took a lot of time and especially memory, which rendered it unpractical for many teams. Another challenge is the package compilation itself, which is not easy either.\n1st-place team proposed a novel version of the correspondence and poses refinement instead - called Detector-Free Structure from Motion.\n\n\n\nRotation-invariance is important, but easy to achieve for SuperGlue and LoFTR\nJust rotate one of the images 4 times and select the best matches. Here is the image of the 3rd place solution, explaining it all.\n\n\n\nInitial image pair setup in Colmap is suboptimal\nSeveral teams reported that manually setting the image pair to start incremental reconstruction in Colmap improved results. One even can improve results by running the incremental reconstruction several times.\n\n\nMany things do not work until they do\nMany teams have reported that LoFTR or DKM doesn’t work for them - but both LoFTR and DKM are part of top-5 solutions. Recent “SiLK” keypoints are reported as not working, but maybe nobody just found a proper way to use them.\n\n\n“Old” features are not done yet - KeyNetAffNet-HardNet solution.\n5th place actually end up in money by using an modified example submission with a classical pipeline using such ancient local features as DoG, Harris and GFTT, together with KeyNet (2019), coupled together with local affine shape estimation by AffNet, patch descriptor HardNet and handcrafted AdaLAM matcher.\nBasically, that is cleverly engineered submission of the off-the-shelf local features, available in kornia.\n\n\n\n\nPermissive license and faster SuperGlue is out - LightGlue\nLightGlue - the solution by 7th place team and likely 2nd money prize, presents a SuperGlue-like architecture with early-stopping for easy image pairs and bunch of training recipes. It also uses somewhat unpopular, but well-performing ALIKED local feature\n\nHere is the quite from the paper about tricks that matter:\n\nSince the depth maps of MegaDepth are often incomplete, we also label points with a large epipolar error as unmatch- able. Carefully tuning and annealing the learning rate boosts the accuracy. Training with more points also does: we use 2k per image instead of 1k. The batch size matters: we use gradient checkpointing [10] and mixed-precision to fit 32 image pairs on a single GPU with 24GB VRAM.\n\nIn addition to that, homography-pretraining is crucial for the Light/Super-Glue performance.\n\n\nNo lines, monodepth or semantic segmentation this year.\nActually I hoped to see Limap line SfM or some kind of monocular depth models used. Bad luck."
  },
  {
    "objectID": "posts/2023-07-05-imc2023-recap.html#per-dataset-results",
    "href": "posts/2023-07-05-imc2023-recap.html#per-dataset-results",
    "title": "Image Matching Challenge 2023: The Unbearable Weight of the Bundle Adjustment",
    "section": "Per-dataset results",
    "text": "Per-dataset results\n\nHeritage\nThe Heritage dataset results show the biggest variability among the methods/worst/best case performance.\n\n\n\nimage.png\n\n\n\n\nUrban\nBecause the main Urban dataset challenge – repeated patterns and symmetrical structures – cannot be even theoretically addressed by better two-view feature matching, it is the hardest dataset. No single team tried to work on this part, so surprise it is hard.\n\n\n\nHaiper\nThe Haiper results are either hit or miss w/o much difference between methods."
  },
  {
    "objectID": "posts/2023-07-05-imc2023-recap.html#conclusion-sfm-is-far-from-solved",
    "href": "posts/2023-07-05-imc2023-recap.html#conclusion-sfm-is-far-from-solved",
    "title": "Image Matching Challenge 2023: The Unbearable Weight of the Bundle Adjustment",
    "section": "Conclusion: SfM is far from solved",
    "text": "Conclusion: SfM is far from solved\nAs in many other computer vision tasks, if something seems to be “solved”, that is just because the datasets are old and obsolete. You still need a good dense capture to do the 3D reconstruction. Many images, many compute, or the additional data like inertial module/GPS, or skilled person to do the capture, is required. See you hopefully next year."
  },
  {
    "objectID": "posts/2020-07-09-wxbs.html",
    "href": "posts/2020-07-09-wxbs.html",
    "title": "WxBS: Wide Multiple Baseline Stereo as a task",
    "section": "",
    "text": "Definition of WxBS\nLet us denote observations \\(O_{i}, i=1..n\\), each of which belongs to one of the views \\(V_{j}, j=1..m\\), \\(m \\leq n\\).\nObservation consist of spatial information and the “descriptor”. View contains the information, which is shared and the same for a group of observations.\nFor example, a single observation can be an RGB pixel. Its spatial information is the pixel coordinates and the “descriptor” is RGB value. The view then is the image, with information about the camera pose, camera intrinsics, sensor and the time of the photo. Some of this information can be unknown to the user, i.e. hidden variable.\nAnother example could an event camera. In that case the observation contains the pixel coordinates and the descriptor is the sign of the intensity change. The view will contain the information about the sensor, camera pose and the single observation inside it, because every event has an unique timestamp.\nObservations and views can be of different nature and dimentionality. E.g. \\(V_1\\), \\(V_2\\) – RGB images, \\(V_3\\) – point cloud from a laser scaner, \\(V_4\\) – image from a thermal camera, and so on.\nAn unordered pair of observations \\((O_{i},O_{k})\\) forms a correspondence \\(c_{ik}\\) if they are belong to different views \\(V_{j}\\). The group of observations is called multivew correspondence \\(C_o\\), when there is exactly one observation \\(O_i\\) per view \\(V_j\\). Some of observations \\(O_i\\) can be empty \\(\\varnothing\\), i.e. not observed in the specific view \\(V_j\\).\nThe world model is the set of contraints on views, observations and correspondences. For example, one the popular models are epipolar geometry and ridid motion assumption.\nThe correspondence is called ground truth or veridical if it satisfy the constraints posed be the world model.\nWe can now define a wide baseline stereo.\nBy wide baseline stereo we understand the process of establishing two or multi-view correspondences \\(C_o\\) from observations \\(O_i\\) and images \\(V_{j}\\) and recovering the missing information about the views and estimatting the unknown parameters of the world model.\nMost often in the current thesis we will be using the the following world model. The scene consists of 3 dimentional elements, and is rigid and static. The observations are the 2D projections to the camera plane by the projectice pinhole camera. The relationship between observations in different views is either epipolar geometry, or projective transform. Any moving object does not satisty the world model and therefore is considered an occlusion. We will call the “baseline” the distance between the camera centers.\nFor example, on image below, observations \\(O_i\\) are blue circles and the correspondences \\(c_{jk}\\) are shown as lines. The assumed object \\(X_i\\) is a red circle.\n\nWe will call “wide multiple baseline stereo” or WxBS if the observations have different nature or the conditions under which observation were made are different.\nThe different between wide baseline stereo and short baseline stereo, or, simply stereo is the follwing. In stereo the baseline is small – less then 1 meter – and typically known and fixed. The task is to establish correspondences, which can be done by 1D search along the known epipolar lines.\nIn contrast, in wide baseline stereo the baseline is unknown, mostly unconstrained and the viewpoints of the cameras can vary drastically.\nThe wide baseline stereo, which also outputs the estimation of the latent objects, e.g. in form of 3d point world coordinates we would call rigid structure-from-motion (rigid SfM) or 3D reconstruction. We do not consider object shape approximation with voxels, meshes, etc in the current thesis. Nor we consider the recovery of scene albedo, illumination, and other appearance properties.\nWhile the difference between SfM and WBS is often blurred and the terms are used interchangeably, we would consider WBS as a part of SfM pipeline prior to recovering 3d point cloud.\nOther correspondence problems, as tracking, optical flow or establishing semantic correspondences could be defined using the terminilogy we established.\n\n\n\nReferences\n(Gallego, Delbruck et al., 2020) Gallego Guillermo, Delbruck Tobi, Orchard Garrick Michael et al., ``Event-based Vision: A Survey’’, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. , number , pp. , 2020.\n(Hartley and Zisserman, 2004) R.~I. Hartley and A. Zisserman, ``Multiple View Geometry in Computer Vision’’, 2004.\n(Mishkin, Matas et al., 2015) D. Mishkin, J. Matas, M. Perdoch et al., ``WxBS: Wide Baseline Stereo Generalizations’’, BMVC, 2015.\n\n\n\n\nEverything you (didn’t) want to know about image matching"
  },
  {
    "objectID": "posts/2022-02-05-disc21-recap.html",
    "href": "posts/2022-02-05-disc21-recap.html",
    "title": "Image Similarity Challenge 2021 recap",
    "section": "",
    "text": "Image Similarity Challenge is benchmark of the methods, which should find the original in the database (if any) by the edited query image. The main difference to the classical instance retrieval task (datasets Oxford5k, GLD, etc) is that we should NOT retrieve the different photos of the given instance. Instead only version of the same photos are valid. In other words, we are looking for the image forgery and copyright infringement.\n\n\n\nThe dataset is described in the “The 2021 Image Similarity Dataset and Challenge” paper. It is created by processing YCC100M and DeepFake Detection Challenge images with manually in GIMP editor and automatically with help of AugLy library.\n\nBesides the public part of the test set (Phase I of the competition), there is also final test set (Phase II) was also composed with use of adversarial attack methods and so-called “air-gap attack”. Air-gap, according to the paper is\n\nWe asked manual editors to perform “air-gap” transformations, where the picture is displayed or printed out, and re-captured with a camera or mobile phone. It is a quite common image attack, that is hard to reproduce automatically or even with an image editor like the GIMP. Figure 8 shows an example air-gap augmentation (the zebra). The effect is a combination of over-exposure, geometric framing and blurriness on the picture.\n\n\nThe paper also provides several baselines, mostly based on global and local descriptors\nEverything you (didn’t) want to know about image matching"
  },
  {
    "objectID": "posts/2022-02-05-disc21-recap.html#what-is-image-similarity-challenge",
    "href": "posts/2022-02-05-disc21-recap.html#what-is-image-similarity-challenge",
    "title": "Image Similarity Challenge 2021 recap",
    "section": "",
    "text": "Image Similarity Challenge is benchmark of the methods, which should find the original in the database (if any) by the edited query image. The main difference to the classical instance retrieval task (datasets Oxford5k, GLD, etc) is that we should NOT retrieve the different photos of the given instance. Instead only version of the same photos are valid. In other words, we are looking for the image forgery and copyright infringement.\n\n\n\nThe dataset is described in the “The 2021 Image Similarity Dataset and Challenge” paper. It is created by processing YCC100M and DeepFake Detection Challenge images with manually in GIMP editor and automatically with help of AugLy library.\n\nBesides the public part of the test set (Phase I of the competition), there is also final test set (Phase II) was also composed with use of adversarial attack methods and so-called “air-gap attack”. Air-gap, according to the paper is\n\nWe asked manual editors to perform “air-gap” transformations, where the picture is displayed or printed out, and re-captured with a camera or mobile phone. It is a quite common image attack, that is hard to reproduce automatically or even with an image editor like the GIMP. Figure 8 shows an example air-gap augmentation (the zebra). The effect is a combination of over-exposure, geometric framing and blurriness on the picture.\n\n\nThe paper also provides several baselines, mostly based on global and local descriptors"
  },
  {
    "objectID": "posts/2022-02-05-disc21-recap.html#tracks-descriptor-track-and-matching-track",
    "href": "posts/2022-02-05-disc21-recap.html#tracks-descriptor-track-and-matching-track",
    "title": "Image Similarity Challenge 2021 recap",
    "section": "Tracks: Descriptor track and Matching track",
    "text": "Tracks: Descriptor track and Matching track\nThe competition had 2 tracks.\n\nDescriptor track expects participant to submit global image descriptors, up to 256-dim, which should be compared by L2 distance. Matching image pairs should have small descriptor distances, while non-matching - large.\nMatching track, where the model can do any pairwise (but only pairwise!) comparisons to get the similarity score between two images.\n\nBelow I will overview the winning solutions, presented at the NeurIPS2021 workshop. But first let me show you a couple of slides from the organizer’s analisys.\nThe easiest image transformations are geometrical ones, except cropping the image to a very small size. The hardest ones are those, which involve combination of several images. The global descriptors are very sensitive to image rotatio and vertical flip (not really surprising).\n\nThe models are more or less reasonably robust to adversarial attacks, except really strong ones.\n\n\nFinally organizers found that have not done enough de-duplication in the dataset, meaning that models were able to discover the duplicates in the dataset. While some of this annotation errors were genuine, others were in the gray zone.\nFor example, images, which were taken from the different positions, but the background object DOES look identical. Another example is the camera burst, where the part of the content, which is not the same across the burst images ended up cropped out by annotators or automatic procedure."
  },
  {
    "objectID": "posts/2022-02-05-disc21-recap.html#descriptor-track",
    "href": "posts/2022-02-05-disc21-recap.html#descriptor-track",
    "title": "Image Similarity Challenge 2021 recap",
    "section": "Descriptor track",
    "text": "Descriptor track\n\n1st place: Contrastive Learning with Large Memory Bank and Negative Embedding Subtraction for Accurate Copy Detection\nby Shuhei Yokoo aka lyakaap.\nThe method takes ImageNet-21k-pretrained EfficientNetv2 from timm and trains it with contrastive loss, gradually increasing the augmentations intensity, as well as image size. Final augmentations are quite strong, see the image below. The loss is the simple constrastive loss with memory bank for the hard negatives.\n\n\nFinally, Shuhei Yokoo proposes novel postprocessing technique - to subtract hard negative embedding from all database image embeddings. It added nice final boost to the submission.\narXiv github\n\n\n2nd place: Producing augmentation-invariant embeddings from real-life imagery\nby Sergio Manuel Papadakis and Sanjay Addicam\nThe method relies on the GeM descriptors ( trained EfficientNetv1, EfficientNetv2, NfNet) with ArcFace loss. ArcFace loss assumes classification, not contrastive setup and hard to train with &gt;1M classes. That is why the method gradually increases the number of classes (images used to train) and the classifier head is re-initialized with centroids, obtained from backbone descriptors from the previous stage. Multiple models are combined by Concat -&gt; PCA -&gt; L2Norm.\n\nFinally, the embeddings are modified, to take into account the most similar images from the database, similar to the 1st place, but in a bit more complicated way.\narXiv, github\n\n\n3rd place: Bag of Tricks and A Strong baseline for Image Copy Detection\nby Wenhao Wang, Weipu Zhang, Yifan Sun, Yi Yang\nThe method uses ResNet50 backbone, pretrained in augmentation-supervised fashion (BarlowTwins, unlike 1st and 2nd place, which used supervised pretrained EfficientNetv2. The model is trained with a combination of triplet and cross entropy losses with GeM pooling and WaveBlock. Finally, authors also present hard-negative aware embedding de-normalization called “descriptor stretching” – different variant of the similar idea, also used in 1st and 2nd place post-processing.\n\nNote that before descriptor stretching, results are much weaker than in 1st and 2nd place, but after the post-processing, the margin shrinks\n\narXiv, github"
  },
  {
    "objectID": "posts/2022-02-05-disc21-recap.html#matching-track",
    "href": "posts/2022-02-05-disc21-recap.html#matching-track",
    "title": "Image Similarity Challenge 2021 recap",
    "section": "Matching track",
    "text": "Matching track\n\n1st place D2LV: A Data-Driven and Local-Verification Approach for Image Copy Detection\nby Wenhao Wang, Yifan Sun, Weipu Zhang, Yi Yang (3rd place in descriptor track).\nThe method relies on several ideas:\n\nTraining specialist models on the different augmentations, e.g. “basic”, “basic+superdark”, “basic+superblur”, etc.\nDifferent ways of splitting the image into the subparts: by YOLOv5 object detection, by 4x4 even split, by rotating the original image, etc, each of which is then “matched” by the global descriptor similarity to the right-hand image. Local-to-global means “we aplit query and match vs reference image”, whereas “global-to-local” - - “we match whole query image to splitted reference”.\n\n\n\narXiv github\n\n\n2nd place: 2nd Place Solution to Facebook AI Image Similarity Challenge: Matching Track\nby SeungKee Jeon.\nThe paper uses a simple approach of concatenating query and reference image and feeding such collage into the vision transformer (ViT).\nViT embeddings are also used to create a short-list for the matching, trained with contrastive loss (SimCLR).\nSimilarly to the 1st place, SeungKee Jeon also used image-to-sub-images (1 -&gt; 2x2, 1-&gt;3x3) splitting technique at test time.\n\narXiv github\n\n\n3rd place: A Global and Local Dual Retrieval Solution to Facebook AI Image Similarity Challenge\nby Xinlong Sun, Yangyang Qin, Xuyuan Xu, Guoping Gong, Yang Fang, Yexin Wang\nThis method is probably the most hand-engineered.\n\n\nThe images are pre-processed with overlay-detection – out-of-the-box YOLOv5. Detected overlays are treated as a separate images.\nThe global descriptor EsViT (Swin-B) is pretrained in self-supervised fashion on ImageNet, then with contrastive loss on the challange dataset, with hard-negative memory bank.\n\n\n\nResize images to 300px -&gt; SIFT features (~600 per image) -&gt; 600M descriptors. Use brute-force search in faiss with fp16 precision. That takes 165Gb memory, fit with 8 V100 GPUs. Threshold the descriptor distance first, the number of matched features second.\n\nAccording to the ablation study, SIFT was significantly worse, than global EsViT descriptor in Phase I, but significantly boosted overall score in Phase 2.\n\narXiv, github"
  },
  {
    "objectID": "posts/2022-02-05-disc21-recap.html#conclusion-my-speculations",
    "href": "posts/2022-02-05-disc21-recap.html#conclusion-my-speculations",
    "title": "Image Similarity Challenge 2021 recap",
    "section": "Conclusion & my speculations",
    "text": "Conclusion & my speculations\nEdited image copy detection is a challenging task, which is, nevertheless solvable to some practical extent with existing techniques. It is interesting, the all 3 winners in descriptor track come up with a similar idea of contextualized database image embeddings with hard negatives.\nI also don’t believe that the supervised/unsupervised pretraining and transformer/CNN architecture matter as much, as participants were reporting – precisely because different teams succesfully used different architectures and pretraining. Once one optimized the training procedure for the selected architecture, the differences would probably disappear.\nThe matching track pipelines are quite complex and computationally demanding. Nevertheless, using local features (or at least sub-image matching) noticably improves the score. My personal favorite is the 2nd place in the matching track for its originality and simplicity.\nWhat would be interesting, is to check how much one looses when using visual-words tf-idf encoding for the local features matching, instead of keeping original descriptors.\nChallenge organizers also announced the join paper with winners, overviewing the competition and the best approaches some when in February (i.e. soon). Stay tuned :)"
  },
  {
    "objectID": "posts/2020-11-25-review-of-retrieval-for-localization.html",
    "href": "posts/2020-11-25-review-of-retrieval-for-localization.html",
    "title": "Benchmarking Image Retrieval for Visual Localization",
    "section": "",
    "text": "I would like to share my thoughts on 3DV 2020 paper “Benchmarking Image Retrieval for Visual Localization” by Pion et.al.\n\n\nWhat is the paper about?\nHow one would approach visual localization? The most viable way to do it is hierarchical approach, similar to image retrieval with spatial verification.\nYou get the query image, retrieve the most similar images to it from some database by some efficient method, e.g. global descriptor search. Then given the top-k images you estimate the pose of the query image by doing two-view matching, or some other method.\nThe question is – how much influence the quality of the image retrieval has? Should you spend more or less time improving it? That is the questions, paper trying to answer.\nAuthors design 3 re-localization systems.\n\n“Task 1” system estimates the query image pose as average of the short-list images pose, weighted by the similarity to the query image.\n“Task 2a” system performs pairwise two-view matching between short-list images and query triangulates the query image pose using 3d map built from the successully matches images.\n“Task 2b” pre-builds the 3D map from the database images offline. At the inference time, local feature 2d-3d matching is done on the shortlist images.\n\n\n\nWhat is benchmarked?\nThe paper compares\n\nSIFT-based DenseVLAD\n\nand CNN-based\n\nNetVLAD\nAPGeM\nDELG\n\nWhat is important (and adequately mentioned in the paper, although I would prefer the disclamer in the each figure) is that all CNN-based methods have different architectures AND training data. Basically, the paper uses author-released models. Thus one cannot say if APGeM is better or worse than NetVLAD as method, because they were trained on the very different data. However, I also understand that one cannot easily afford to re-implement and re-train everything.\nAs the sanity check paper provides the results on the Revisited Oxford and Paris image retrieval benchmark. \n\n\nSummary of results\nPaper contains a lot of information and I definitely recommend you to read it. Nevertheless, let me try to summarize paper messages and then my take on it.\n\nFor the task1 (similarity-weighted pose) there is no clear winner. (SIFT)-DenseVLAD works the best for the daytime datasets. Probably DenseVLAD is good because it is not invariant and if it can match images, they are really close -&gt; high pose accuracy. For the night both DeLG and AP-GeM are good. As paper guesses, that it because they are only ones, which were trained on night images as well. \nThere is almost no difference between CNN-based methods for the task2a and task2b (retrieval -&gt; local features matching). This indicates that the limit is the mostly in the number of images and local features.\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\nMy take-away messages\n\nImage Relocalization seems to be is more real-world and engineering task, than image retrieval.\nAnd that it why it actually ALREADY WORKS, because if there some weak spot, it is compensated by the system design. Thhe same conclusion from our IMC paper, experiment with ground truth – if you have 1k images for the 3d model, you can use as bad features, as you want. The COLMAP will recover anyway\n\n\n\nimage.png\n\n\nThe retrieval, on the other hand is more interesting to work on, because it is kind of deliberately hard and you can do some fancy stuff, which do not matter in the real world.\n\n\nTask1 (global descriptor-only) system are quite useless now\nUnless we are speaking about the quite dense image representation. I mean, top-accuracy is 35% vs almost 100% for those, which include local features.\nGood news: it has a LOT of space for the improvement to work on.\n\n\nFor the task 2a and 2b, robust global descriptors are a way to do the retrieval, sorry VLAD.\nThe precision will come from the local features. Which I like a lot, because VLAD is more complex to train and initalize, I never liked it (nothing personal).\n\n\nFor the task2a and 2b we need new metrics, e.g. precisition @ X Mb memory footprint\nBecause otherwise, the task is easily solved by the brute force – either by photo taking, or, at least with image syntesis, see 24/7 place recognition by view synthesis.\nSuch steps are already taken in the paper Learning and aggregating deep local descriptors for instance-level recognition – see the table with memory footprint.\n\n\n\nimage.png\n\n\nThat is how one could have an interesting research challenge, also having some grounds in the real-world – to work in mobile phones. Otherwise, any method would work, if the database is dense enough.\n\n\nRobust local features matter for illumination changes\nIt is a bit hidden in the Appendix, so go directly to the Figure 9. It clearly shows that localization performance is bounded by SIFT, if it is used for two view matching, making retrieval improvements irrelevant. When R2D2 or D2Net are used for matching instead, the overall results for night-time are much better.\n\n\n\nimage.png\n\n\nThat is in line with my small visual benchmark I did recently.\nhttps://twitter.com/ducha_aiki/status/1330495426865344515\n\n\n\nimage.png\n\n\nThat’s all, folks! Now please, check the paper and the code they provided.\n\n\n\n\n\nEverything you (didn’t) want to know about image matching"
  },
  {
    "objectID": "posts/2023-12-03-geometry-encoding-sift-lightglue.html",
    "href": "posts/2023-12-03-geometry-encoding-sift-lightglue.html",
    "title": "Experiments with LightGlue: geometry representation and initialization",
    "section": "",
    "text": "Release of LightGlue (faster, better and open-source SuperGlue) training code made me excited for many reasons, and of them is that I can answer some small research questions about learned feature matching and share results.\nIf you don’t know what is LightGlue - that is transformer-based architecture for local feature matching, where the original descriptors (SuperPoint, SIFT, etc) are fused with local feature geometry (keypoint coordinates, scale, orientation, etc) and new, context-aware descriptors are produced.\nOne of such small questions is: “Does the parameterization of the local feature geometry matter for positional encoding input or not?”\n\n\nUnlike modern keypoint detectors like SuperPoint or DISK, SIFT local feature has an orientation and scale part.\n\nShould we just concatenate scale and orientation (LightGlue code does it) to (normalized) keypoint center, making a heterogenous vector, or should we try to preprocess the input somehow? Or maybe geometry does not help at all?\nTo answer this question, I have trained (only the homography pretraining part) 5 LightGlue SIFT models, which differ only in the input feature geometry representation.\nEverything you (didn’t) want to know about image matching"
  },
  {
    "objectID": "posts/2023-12-03-geometry-encoding-sift-lightglue.html#train-your-own-matcher",
    "href": "posts/2023-12-03-geometry-encoding-sift-lightglue.html#train-your-own-matcher",
    "title": "Experiments with LightGlue: geometry representation and initialization",
    "section": "",
    "text": "Release of LightGlue (faster, better and open-source SuperGlue) training code made me excited for many reasons, and of them is that I can answer some small research questions about learned feature matching and share results.\nIf you don’t know what is LightGlue - that is transformer-based architecture for local feature matching, where the original descriptors (SuperPoint, SIFT, etc) are fused with local feature geometry (keypoint coordinates, scale, orientation, etc) and new, context-aware descriptors are produced.\nOne of such small questions is: “Does the parameterization of the local feature geometry matter for positional encoding input or not?”\n\n\nUnlike modern keypoint detectors like SuperPoint or DISK, SIFT local feature has an orientation and scale part.\n\nShould we just concatenate scale and orientation (LightGlue code does it) to (normalized) keypoint center, making a heterogenous vector, or should we try to preprocess the input somehow? Or maybe geometry does not help at all?\nTo answer this question, I have trained (only the homography pretraining part) 5 LightGlue SIFT models, which differ only in the input feature geometry representation."
  },
  {
    "objectID": "posts/2023-12-03-geometry-encoding-sift-lightglue.html#possible-geometry-representations",
    "href": "posts/2023-12-03-geometry-encoding-sift-lightglue.html#possible-geometry-representations",
    "title": "Experiments with LightGlue: geometry representation and initialization",
    "section": "Possible geometry representations",
    "text": "Possible geometry representations\nThere are 4 possible representation of the SIFT keypoints geometry:\n\nThe simplest is (x,y, angle, radius). Intuitively it makes the least sense, because the vector elements are heterogenious - x,y, radius are in pixels, while angle is in degrees. Moreover, if we compare the scale, it would make more sense to use log(radius). We call it sift_scaori in the graph.\nSo the next version is (x,y, angle, log(radius)). We call it sift_logscaori in the graph.\nThe most homogenious is representation as 2 points - center and the one on the border: (x,y, x2, y2). We call it sift_laf2 in the graph.\nFinally, we are use LAF representation as in affine transformation matrix, used in, for example, kornia: (x,y, x2-x, y2-y) == (x,y, R * cos(angle), R * sin(angle)). We call it sift_laf in the graph.\n\n\nAnd the baseline would be not using keypoint geometry, only center as for SuperPoint – (x,y). We call it sift_clean in the graph.\n\nResults\nAt the beginning of the training it seems, that there is a considerable difference between keypoint geometry representation, (x,y, x2, y2) and (x,y, angle, log(radius)) are clearly better than others.\n\nHowever, in the end of the training, the simplest and the most stupid (x,y, angle, radius) becomes the first.\n\nThe difference is even more pronounced on MegaDepth-1500. Even more important thing is that ranking in match recall/precision on validation is different from the MegaDepth-1500 results. Do NOT compare these results to the LightGlue paper SIFT results, as it we haven’t done full training - only a homography pretraining.\n\n\n\nName\nEncoding\nPose mAA (%)\n\n\n\n\nsift_clean\n(x, y)\n47.0\n\n\nsift_scaori\n(x, y, angle, radius)\n50.0\n\n\nsift_logscaori\n(x, y, angle, log(radius))\n47.3\n\n\nsift_laf1\n(x, y, x2-x, y2-y)\n48.1\n\n\nsift_laf2\n(x, y, x2-x, y2-y)\n48.2\n\n\n\nSo, the conclusion is, as often the case with deep learning - keep it simple. The second conclusion is that additional geometry surely helps, but not that much."
  },
  {
    "objectID": "posts/2023-12-03-geometry-encoding-sift-lightglue.html#can-we-initialize-lightglue-for-features-x-from-lightglue-trained-for-features-y",
    "href": "posts/2023-12-03-geometry-encoding-sift-lightglue.html#can-we-initialize-lightglue-for-features-x-from-lightglue-trained-for-features-y",
    "title": "Experiments with LightGlue: geometry representation and initialization",
    "section": "Can we initialize LightGlue for features X from LightGlue trained for features Y?",
    "text": "Can we initialize LightGlue for features X from LightGlue trained for features Y?\nRecently I have accidentally run ALIKED-LightGlue with DISK features. To my surprise, result was quite good.\n\nUnfortunately, that works only for simple image pairs, not harder ones.\n\nOn the second though, that kind of makes sense - once (in intial layers) we went far from original descriptors, then the attention and positional encoding are kind of similar.\nThis begs for the question - can we initialize LightGlue for new local features with previous ones, and train faster or better?\nI have initialized the LightGlue with SuperPoint LightGlue and started training for DeDoDe features. as_sift here stands for the hyperparameters setup, ignore it.\n\n\nUnfotunately, one cannot rely on the match recall/precision validation metric. When I have evaluated on MegaDepth-1500, the model initialized from SuperPoint was worse.\n\n\n\nName\nPose mAA (%)\n\n\n\n\ndedode_homo\n60.4\n\n\ndedode_homo_from_sp\n56.5\n\n\n—\n—\n\n\ndedode_homo_ft_megadepth\n65.9\n\n\ndedode_homo_from_sp_ft_megadepth\n65.6"
  },
  {
    "objectID": "posts/2023-12-03-geometry-encoding-sift-lightglue.html#summary",
    "href": "posts/2023-12-03-geometry-encoding-sift-lightglue.html#summary",
    "title": "Experiments with LightGlue: geometry representation and initialization",
    "section": "Summary",
    "text": "Summary\nIt was fun to train a couple of LightGlue models to check some hypothesis. Kudos to Philipp Lindenberger and Paul-Edouard Sarlin for releasing such amazing package and the paper.\nI also think that the story with initializing learned matchers from other features, or even universal, feature-agnostic matchers is not over.\n\nAcknowledgements.\nThis blogpost is supported by CTU in Prague RCI computing cluster from OP VVV funded project CZ.02.1.01/0.0/0.0/16 019/0000765 “Research Center for Informatics” grant."
  },
  {
    "objectID": "posts/2020-08-06-affine-view-synthesis.html",
    "href": "posts/2020-08-06-affine-view-synthesis.html",
    "title": "How to match images taken from really extreme viewpoints?",
    "section": "",
    "text": "What to do, if you are in a desperate need of matching this particular image pair?\nWhat are the limitations of the affine-covariant detectors like Hessian-Affine or HesAffNet?\nASIFT: brute-force affine view synthesis\nDo as little as possible: MODS\nWhat is the key factor of affine view synthesis? Ablation study\nEverything you (didn’t) want to know about image matching"
  },
  {
    "objectID": "posts/2020-08-06-affine-view-synthesis.html#what-is-wrong-with-affine-covariant-local-detectors",
    "href": "posts/2020-08-06-affine-view-synthesis.html#what-is-wrong-with-affine-covariant-local-detectors",
    "title": "How to match images taken from really extreme viewpoints?",
    "section": "What is wrong with affine-covariant local detectors?",
    "text": "What is wrong with affine-covariant local detectors?\nOne could say that the goal of affine-covariant detectors like MSER, Hessian-Affine or Hessian-AffNet is to detect the same region on a planar surface, regardless the camera angle change. It is true to some extent, as we demostrate on toy example below with Hessian-Affine feature.\n\nThe problem arises, when the image content, e.g. 3 blobs on the figure below are situated close to each other, so under the tilt transform the merge into a single blob. So it is not the shape of region, which is detected incorrectly, but the center of the features themselves. For clarity, we omited affine shape estimation on the image below."
  },
  {
    "objectID": "posts/2020-08-06-affine-view-synthesis.html#asift-brute-force-affine-view-synthesis",
    "href": "posts/2020-08-06-affine-view-synthesis.html#asift-brute-force-affine-view-synthesis",
    "title": "How to match images taken from really extreme viewpoints?",
    "section": "ASIFT: brute-force affine view synthesis",
    "text": "ASIFT: brute-force affine view synthesis\nSo, to solve the problem explained above, Morel & Yu proposed to do a lot affine warps of each image, as shown on the Figure below, as match each view against all others, which is \\(O(n^2)\\) complexity, where \\(n\\) is number of views generated.\n\nThe motivation do doing so it that assuming, original image to be a fronto-parallel one, to cover viewsphere really dense, as shown in the Figure below.\n\nThis leads to impressive performance on a very challenging image pairs, see an example below\n\n\n\nimage.png\n\n\nIn this section I have used great illustrations done by Mariano Rodríguez for his paper “Fast Affine Invariant Image Matching” . Please, checkout his blog."
  },
  {
    "objectID": "posts/2020-08-06-affine-view-synthesis.html#mods-do-as-little-as-possible",
    "href": "posts/2020-08-06-affine-view-synthesis.html#mods-do-as-little-as-possible",
    "title": "How to match images taken from really extreme viewpoints?",
    "section": "MODS: do as little as possible",
    "text": "MODS: do as little as possible\nThe main drawback of ASIFT algorithm is a huge computational cost: 82 views are generated regardless of the image pair difficulty. To overcome this, we proposed MODS algorithm: Matching with On-Demand Synthesis.\n\nOne starts with the fastest detector-descriptor without view synthesys and then uses more and more computationally expensive methods if needed. Moreover, by using affine-covariant detectors like MSER or Hessian-Affine, one could synthetise significantly less views, saving computations spent on local descriptor and matching.\nThis, together with FGINN matching strategy, specifically designed for the handling re-detections, MODS is able to match more challenging image pairs in less time than ASIFT."
  },
  {
    "objectID": "posts/2020-08-06-affine-view-synthesis.html#why-does-affine-synthesis-help",
    "href": "posts/2020-08-06-affine-view-synthesis.html#why-does-affine-synthesis-help",
    "title": "How to match images taken from really extreme viewpoints?",
    "section": "Why does affine synthesis help?",
    "text": "Why does affine synthesis help?\nDespite that ASIFT and other view-synthesis based approaches are know more than decade, we are not aware of a study, why does affine synthesis helps in practice. Could one get a similar performance without view synthesis? Specificallly:\n\nMay it be that the most of improvements come from the fact that we have much more features? That is why we fix the number of features for all approaches.\nSome regions from ASIFT, when reprojected to the original image, are quite narrow. Could we get them just by removing edge-like feature filtering, which is done in SIFT, Hessian and other detectors. Denoted +edge\nInstead of doing affine view synthesis, one could directly use the same affine parameters to get the affine regions to describe, so the each keypoint would have several associated regions+descriptors. Denoted +MD\nUsing AffNet to directly estimated local affine shape without multiple descriptors. Denoted +AffNet\nCombine (1), (2) and (3).\n\nSo, we did the study on HPatches Sequences dataset, the hardest image pairs (1-6) of viewpoint subset. The metric is similar to one used in the “Image Matching across Wide Baselines: From Paper to Practice” and CVPR 2020 RANSAC in 2020 - mean average accuracy of the estimated homography.\n![](2020-08-04-affine-view-synthesis_files/att_00011.png ” mean average accuracy of the estimated homography - used metric”)\nWe run Hessian detector with RootSIFT descriptor, FLANN matching and LO-RANSAC, as implemented in MODS. Features are sorted according the the detector response and their total number is clipped to 2048 or 8000 to ensure that the improvements do not come from just having more features.\nNote, that we do not study, if view synthesis helps for the regular image pairs - it might actually hurt performance, similarly to affine features. Instead we are focusing on the case, when view synthesis definitely helps: matching obscure views of the mostly planar scenes.\n\n8000 feature budget\nResults are in Figure below. Indeed, all of the factors: detecting more edge-like features, having multiple descriptors or better affine shape improve results over the plain Hessian detector, but even all of the combined are not good enough to match performance of the affine view synthesis + plain Hessian detector.\nBut the best setup is to use both Hessian-AffNet and view synthesis.\n\n\n\n2048 feature budget\nThe picture is a bit different in a small feature budget: neither multiple-(affine)-descriptors per keypoint, nor allowing edge-like feature help. From other hand, affine view synthesis still improves results of the Hessian. And, again, the best performance is achieved with combination of view synthesis and AffNet shape estimation."
  },
  {
    "objectID": "posts/2020-08-06-affine-view-synthesis.html#summary",
    "href": "posts/2020-08-06-affine-view-synthesis.html#summary",
    "title": "How to match images taken from really extreme viewpoints?",
    "section": "Summary",
    "text": "Summary\nAffine view synthesis helps for matching challenging image pairs and its improvement are not just because of more local features used. It can be done effective and efficient – in the iterative MODS framework."
  },
  {
    "objectID": "posts/2021-05-12-submitting-to-imc2021-step-by-step.html",
    "href": "posts/2021-05-12-submitting-to-imc2021-step-by-step.html",
    "title": "Submitting to Image Matching Challenge 2021",
    "section": "",
    "text": "IMC is a benchmark and challenge for the local features (such as SIFT, SuperPoint, etc), matching methods (CNe, SuperGlue, etc.) and robust geometry estimators such as RANSAC, at CVPR 2021 Workshop on Image Matching.\nI will walk you through the submission process, including writing setting-up an environment, writing a processing script and tuning matching and RANSAC for the best performance. As this is a tutorial, not a research paper, we use local feature descriptor available in kornia for this sample submission.\nAll the codes and scripts in this tutorial are also avilable at https://github.com/ducha-aiki/imc2021-sample-kornia-submission. Let’s go!\nEverything you (didn’t) want to know about image matching"
  },
  {
    "objectID": "posts/2021-05-12-submitting-to-imc2021-step-by-step.html#what-is-image-matching-challenge",
    "href": "posts/2021-05-12-submitting-to-imc2021-step-by-step.html#what-is-image-matching-challenge",
    "title": "Submitting to Image Matching Challenge 2021",
    "section": "",
    "text": "IMC is a benchmark and challenge for the local features (such as SIFT, SuperPoint, etc), matching methods (CNe, SuperGlue, etc.) and robust geometry estimators such as RANSAC, at CVPR 2021 Workshop on Image Matching.\nI will walk you through the submission process, including writing setting-up an environment, writing a processing script and tuning matching and RANSAC for the best performance. As this is a tutorial, not a research paper, we use local feature descriptor available in kornia for this sample submission.\nAll the codes and scripts in this tutorial are also avilable at https://github.com/ducha-aiki/imc2021-sample-kornia-submission. Let’s go!"
  },
  {
    "objectID": "posts/2021-05-12-submitting-to-imc2021-step-by-step.html#setting-up-the-environment",
    "href": "posts/2021-05-12-submitting-to-imc2021-step-by-step.html#setting-up-the-environment",
    "title": "Submitting to Image Matching Challenge 2021",
    "section": "Setting up the environment",
    "text": "Setting up the environment\nFirst, let’s clone the benchmark repository.\ngit clone https://github.com/ubc-vision/image-matching-benchmark\ncd image-matching-benchmark\ngit submodule update --init --recursive\nNow we need to create conda virtual environment. I assume that you have conda installed, and if not - please, follow instructions here\nconda env create -f system/conda_env_dm.yml\nAfter successfull environment creation, let’s activate it\nconda activate sfm"
  },
  {
    "objectID": "posts/2021-05-12-submitting-to-imc2021-step-by-step.html#downloading-the-data",
    "href": "posts/2021-05-12-submitting-to-imc2021-step-by-step.html#downloading-the-data",
    "title": "Submitting to Image Matching Challenge 2021",
    "section": "Downloading the data",
    "text": "Downloading the data\nIMC-2021 data consists of 3 datasets: Phototourism, PragueParks and GoogleUrban. You can download first two freely, but need to request a credentials and accept license agreement for the GoogleUrban dataset.\nLatter is done by writing an email to image-matching@googlegroups.com and asking for the password. All datasets consist of two parts each: test part, which contains only images and validation part, which comes together with ground truth. Validation ground truth can (and should!) be used for hyperparameter tuning for your submission.\ncd ..\nmkdir imc-2021-data\ncd imc-2021-data\nwget https://www.cs.ubc.ca/research/kmyi_data/imc2021-public/imc-2021-test-public-pragueparks.tar.gz\nwget https://www.cs.ubc.ca/research/kmyi_data/imc2021-public/imc-2021-validation-pragueparks.tar.gz\nwget https://www.cs.ubc.ca/research/kmyi_data/imc2021-public/imc-2021-test-public-phototourism.tar.gz\nwget https://www.cs.ubc.ca/research/kmyi_data/imc2021-public/imc-2021-validation-phototourism.tar.gz\nI am assuming that you have requsted an access to the GoogleUrban dataset, downloaded it and put in the same directory, as the rest of the data:\n(sfm) mishkdmy@n33:~/dev/imc-2021-data$ ls\nimc-2021-test-public-googleurban.tar.gz   imc-2021-validation-googleurban.tar.gz\nimc-2021-test-public-phototourism.tar.gz  imc-2021-validation-phototourism.tar.gz\nimc-2021-test-public-pragueparks.tar.gz   imc-2021-validation-pragueparks.tar.gz\nNow let’s unpack it.\nfor f in *.tar.gz ; do tar -xzf $f; done\nNow the directory should look like this:\n(sfm) mishkdmy@n33:~/dev/imc-2021-data$ ls\ngoogleurban                               imc-2021-validation-phototourism.tar.gz\nimc-2021-test-public-googleurban.tar.gz   imc-2021-validation-pragueparks.tar.gz\nimc-2021-test-public-phototourism.tar.gz  phototourism\nimc-2021-test-public-pragueparks.tar.gz   pragueparks\nimc-2021-validation-googleurban.tar.gz"
  },
  {
    "objectID": "posts/2021-05-12-submitting-to-imc2021-step-by-step.html#extracting-the-features",
    "href": "posts/2021-05-12-submitting-to-imc2021-step-by-step.html#extracting-the-features",
    "title": "Submitting to Image Matching Challenge 2021",
    "section": "Extracting the features",
    "text": "Extracting the features\nLet’s start with creating the directory for our scripts (or you can clone it from here )\ncd ..\nmkdir imc2021-sample-kornia-submission\ncd imc2021-sample-kornia-submission\nNow we will create a script, which extracts AffNet-HardNet8 descriptors on top of OpenCV SIFT keypoints.\nWe need to install pytorch and kornia for this:\npip install torch torchvision kornia\npip install kornia_moons --no-deps\nGreat! Now we are ready to extract the features from the images. Required imports and initializations:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nimport torch\nimport kornia as K\nimport kornia.feature as KF\nfrom kornia_moons.feature import *\n\ndevice = torch.device('cpu')\ntry:\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n        print (\"GPU mode\")\nexcept:\n    print ('CPU mode')\n#device = torch.device('cpu')\n# SIFT (DoG) Detector\nsift_det =  cv2.SIFT_create(8000, contrastThreshold=-10000, edgeThreshold=-10000)\n\n# HardNet8 descriptor\n\nhardnet8 = KF.HardNet8(True).eval().to(device)\n\n# Affine shape estimator\naffnet = KF.LAFAffNetShapeEstimator(True).eval().to(device)\n\nNow we can define extract_features function. Feel free to modify it for your own features.\n\ndef extract_features(img_fname, detector, affine, descriptor, device, visualize=False):\n    img = cv2.cvtColor(cv2.imread(img_fname), cv2.COLOR_BGR2RGB)\n    if visualize:\n        plt.imshow(img)\n    kpts = detector.detect(img, None)[:8000]\n    # We will not train anything, so let's save time and memory by no_grad()\n    with torch.no_grad():\n        timg = K.image_to_tensor(img, False).float()/255.\n        timg = timg.to(device)\n        timg_gray = K.rgb_to_grayscale(timg)\n        # kornia expects keypoints in the local affine frame format. \n        # Luckily, kornia_moons has a conversion function\n        lafs = laf_from_opencv_SIFT_kpts(kpts, device=device)\n        lafs_new = affine(lafs, timg_gray)\n        if visualize:\n            visualize_LAF(timg, lafs_new, 0)\n        patches = KF.extract_patches_from_pyramid(timg_gray, lafs_new, 32)\n        B, N, CH, H, W = patches.size()\n        # Descriptor accepts standard tensor [B, CH, H, W], while patches are [B, N, CH, H, W] shape\n        # So we need to reshape a bit :) \n        descs = descriptor(patches.view(B * N, CH, H, W)).view(B * N, -1).detach().cpu().numpy()    \n    return kpts, descs\n\nLet’s check how it works on a single image.\n\nimg_fname = '../imc-2021-data/pragueparks/wooden_lady/set_100/images/IMG_9603.MOV_frame000001.jpg'\nkpts, descs = extract_features(img_fname,  sift_det, affnet, hardnet8, device, True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo far, so good. Now we need to convert our keypoints from OpenCV format to the benchmark format, which is numpy.array [N x dim]\n\ndef convert_kpts_to_imc(cv2_kpts):\n    keypoints = np.array([(x.pt[0], x.pt[1]) for x in cv2_kpts ]).reshape(-1, 2)\n    scales = np.array([12.0* x.size for x in cv2_kpts ]).reshape(-1, 1)\n    angles = np.array([x.angle for x in cv2_kpts ]).reshape(-1, 1)\n    responses = np.array([x.response for x in cv2_kpts]).reshape(-1, 1)\n    return keypoints, scales, angles, responses\n\nNow we are ready to write a script, which extracts local features for all images in the IMC-2021. The full script is accesible here\n\nimport os\nimport h5py\nfrom tqdm import tqdm\n\n\nINPUT_DIR = '../imc-2021-data'\nOUT_DIR = 'extracted/cv2-dog-affnet-hardnet8'\n\nos.makedirs(OUT_DIR, exist_ok=True)\ndatasets = os.listdir(INPUT_DIR)\nfor ds in datasets:\n    ds_in_path = os.path.join(INPUT_DIR, ds)\n    ds_out_path = os.path.join(OUT_DIR, ds)\n    os.makedirs(ds_out_path, exist_ok=True)\n    seqs = os.listdir(ds_in_path)\n    for seq in seqs:\n        if os.path.isdir(os.path.join(ds_in_path, seq, 'set_100')):\n            seq_in_path = os.path.join(ds_in_path, seq, 'set_100', 'images')\n        else:\n            seq_in_path = os.path.join(ds_in_path, seq)\n        seq_out_path = os.path.join(ds_out_path, seq)\n        os.makedirs(seq_out_path, exist_ok=True)\n        img_fnames = os.listdir(seq_in_path)\n        num_kp = []\n        with h5py.File(f'{seq_out_path}/keypoints.h5', 'w') as f_kp, \\\n             h5py.File(f'{seq_out_path}/descriptors.h5', 'w') as f_desc, \\\n             h5py.File(f'{seq_out_path}/scores.h5', 'w') as f_score, \\\n             h5py.File(f'{seq_out_path}/angles.h5' 'w') as f_ang, \\\n             h5py.File(f'{seq_out_path}/scales.h5', 'w') as f_scale:\n            for img_fname in tqdm(img_fnames):\n                img_fname_full = os.path.join(seq_in_path, img_fname)\n                key = os.path.splitext(os.path.basename(img_fname))[0]\n                kpts, descs = extract_features(img_fname_full,  sift_det, affnet, hardnet8, device, False)\n                keypoints, scales, angles, responses = convert_kpts_to_imc(kpts)\n                f_kp[key] = keypoints\n                f_desc[key] = descs.reshape(-1, 128)\n                f_score[key] = responses\n                f_ang[key] = angles\n                f_scale[key] = scales\n                num_kp.append(len(keypoints))\n            print(f'Finished processing \"{ds}/{seq}\" -&gt; {np.array(num_kp).mean()} features/image')"
  },
  {
    "objectID": "posts/2021-05-12-submitting-to-imc2021-step-by-step.html#creating-config-json-file",
    "href": "posts/2021-05-12-submitting-to-imc2021-step-by-step.html#creating-config-json-file",
    "title": "Submitting to Image Matching Challenge 2021",
    "section": "Creating config json file",
    "text": "Creating config json file\nIn addition to features, we should submit a config file, which tells the benchmark, how the features should be matched and which RANSAC we prefer. In priciple, we can just write an arbitrary config file and submit already, but this may lead to the bad results. Let’s instead generate a config file from python, so we can easily re-generate it. Why would we need this? Quite simple - to try different parameters on the validation set and only then create a final config.\nFirst part of the config is metadata – information about the method and authors. If your method is under review, you may want to set flag publish_anonymously to True.\n\nmetadata_dict =  {\n    \"publish_anonymously\": False,\n    \"authors\": \"Dmytro Mishkin, Milan Pultar and kornia team\",\n    \"contact_email\": \"ducha.aiki@gmail.com\",\n    \"method_name\": \"CV-DoG-AffNet-HardNet8 (kornia)\",\n    \"method_description\": \n    r\"\"\"OpeCV SIFT keypoints 8000 features, followed by the AffNet normalization \n    and HardNet8 descriptor as implemented in kornia.\n    Matched using the built-in matcher (bidirectional filter with the 'both' strategy, \n    hopefully optimal inlier and ratio test thresholds) with DEGENSAC\"\"\",\n    \"link_to_website\": \"https://github.com/kornia/kornia\",\n    \"link_to_pdf\": \"https://arxiv.org/abs/2007.09699\"\n}\n\nSecond part is config_common: it tells the benchmark, which keypoints and descriptors you use. We will also need this names when importing our features during tuning on the validation set.\n\nconfig_common_dict =  {\"json_label\": \"dog-affnet-hardnet8-degensac\",\n    \"keypoint\": \"cv2dog\",\n    \"descriptor\": \"affnethardnet8\",\n    \"num_keypoints\": 8000}\n\nNow comes the information how to match your local features. It may vary from dataset to dataset and also be different for the multiview and stereo mode. That is why we will create a template dictionary and change some parameters later.\nSpecifically, we have to specify, which distance our descriptor prefers: L2, L1 and Hamming are supported.\nThen comes the tentative matches filtering. One can pass none for no filtering, snn_ratio_pairwise for Lowe’s SNN ratio and fginn_ratio_pairwise for FGINN. If you are not familiar with filtering strategies, checkout this blogpost: “How to match: to learn or not to learn?”. The threshold is what have to be tunes.\nWe will use SNN, because of simplicity. Finally, we would like to make sure that tentative matches are cross-consistent, that is why we will enable symmetric matching.\nWarning! We will use FLANN approximate nearest neighbor matching for speed-up tuning procedure, but it is better to turn it off for the final submission.\n\nfrom copy import deepcopy\n\nmatcher_template_dict = {\n     \"method\": \"nn\",\n     \"distance\": \"L2\",\n     \"flann\": True,\n     \"num_nn\": 1,\n     \"filtering\": {\n         \"type\": \"snn_ratio_pairwise\",\n         \"threshold\": 0.90\n     },\n     \"symmetric\": {\n         \"enabled\": True,\n         \"reduce\": \"both\",\n     }\n}\n\nFinally, we have to specify robust geometry estimation method. We will pick the default choise from the previous challenge - DEGENSAC. threshold is what have to be tuned, the rest of parameters are already optimal, or fixed by the competition rules – max_iter.\n\ngeom_template_dict =  {\"method\": \"cmp-degensac-f\",\n                \"threshold\": 0.5,\n                \"confidence\": 0.999999,\n                \"max_iter\": 100000,\n                \"error_type\": \"sampson\",\n                \"degeneracy_check\": True,\n            }\n\nLet’s assemble and save our base config.\n\nimport json\n\nbase_config =  {\n    \"metadata\": metadata_dict,\n    \"config_common\": config_common_dict,\n    \"config_phototourism_stereo\": {\n        \"use_custom_matches\": False,\n        \"matcher\": deepcopy(matcher_template_dict),\n        \"outlier_filter\": { \"method\": \"none\" },\n        \"geom\": deepcopy(geom_template_dict)\n        },\n    \"config_phototourism_multiview\": {\n        \"use_custom_matches\": False,\n        \"matcher\": deepcopy(matcher_template_dict),\n        \"outlier_filter\": { \"method\": \"none\" },\n        \"colmap\": {}},\n    \n    \"config_pragueparks_stereo\": {\n        \"use_custom_matches\": False,\n        \"matcher\": deepcopy(matcher_template_dict),\n        \"outlier_filter\": { \"method\": \"none\" },\n        \"geom\": deepcopy(geom_template_dict)\n        },\n    \"config_pragueparks_multiview\": {\n        \"use_custom_matches\": False,\n        \"matcher\": deepcopy(matcher_template_dict),\n        \"outlier_filter\": { \"method\": \"none\" },\n        \"colmap\": {}},\n    \"config_googleurban_stereo\": {\n        \"use_custom_matches\": False,\n        \"matcher\": deepcopy(matcher_template_dict),\n        \"outlier_filter\": { \"method\": \"none\" },\n        \"geom\": deepcopy(geom_template_dict)\n        },\n    \"config_googleurban_multiview\": {\n        \"use_custom_matches\": False,\n        \"matcher\": deepcopy(matcher_template_dict),\n        \"outlier_filter\": { \"method\": \"none\" },\n        \"colmap\": {}}\n}\n\nFinally, benchmark expects multiple configs, so we have to create a list, and then we can save our config\n\nimport json \n\nwith open('base_config.json', 'w') as f:\n    json.dump([base_config], f, indent=2)"
  },
  {
    "objectID": "posts/2021-05-12-submitting-to-imc2021-step-by-step.html#hyperparameters-tuning",
    "href": "posts/2021-05-12-submitting-to-imc2021-step-by-step.html#hyperparameters-tuning",
    "title": "Submitting to Image Matching Challenge 2021",
    "section": "Hyperparameters tuning",
    "text": "Hyperparameters tuning\nLet’s now tune our hyperparameters, specifically, matching threshold and RANSAC inlier ratio. First we tune RANSAC and then matching ratio. We need to generate jsons for each configuration.\n\ninl_ths = [0.25, 0.5, 0.75, 1.0, 1.5, 2.0, 2.5, 3.0]\nconfigs = []\n\nfor inl_th in inl_ths:\n    current_config = deepcopy(base_config)\n    for dset in ['phototourism', 'pragueparks', 'googleurban']:\n        current_config[f'config_{dset}_stereo']['geom']['threshold'] = inl_th\n    label = current_config['config_common']['json_label'] \n    current_config['config_common']['json_label']  = f'{label}-inlth-{inl_th}'\n    configs.append(current_config)\n\nwith open('ransac_tuning.json', 'w') as f:\n    json.dump(configs, f, indent=2)\n\ncd ../image-matching-benchmark/\npython -utt run.py --run_mode=interactive --json_method=../imc2021-sample-kornia-submission/base_config.json --subset=val  --eval_multiview=False --path_data ../imc-2021-data/ --path_results ../benchmark-results --is_challenge false\ncd ../imc2021-sample-kornia-submission\nNow we will write a function, which reads results and picks the best threshold per dataset\n\nhashname='dog-affnet-hardnet8-degensac'\nres_dict = {}\ndatasets = ['phototourism', 'pragueparks', 'googleurban']\nfor dset in datasets:\n    res_dict[dset] = {}\n\ntask = 'stereo'\nmetric = 'qt_auc_10_th_0.1'\nfor inl_th in inl_ths:\n    res_fname = os.path.join('../image-matching-benchmark/packed-val', f'{hashname}-inlth-{inl_th}.json')\n    try:\n        with open(res_fname, 'r') as f:\n            results = json.load(f)\n    except:\n        continue\n    submission_name = results['config']['metadata']['method_name']\n    res_dict[inl_th] = {}\n    for dset in datasets:\n        mAA = results[dset]['results']['allseq'][task]['run_avg'][metric]['mean']\n        res_dict[dset][inl_th] = mAA\nfig, ax = plt.subplots(figsize=(5,5))\ncolors = ['r','b','k']\nfinal_ths = {}\nfor i, dset in enumerate(datasets):\n    inl_ths = []\n    mAAs = []\n    for inl_th, mAA in res_dict[dset].items():\n        inl_ths.append(inl_th)\n        mAAs.append(mAA)\n    best_th_idx = np.argmax(np.array(mAAs))\n    best_th = inl_ths[best_th_idx]\n    best_mAA = mAAs[best_th_idx]\n    print (f'Best {dset} mAA = {best_mAA:.4f} with inl_th = {best_th}')\n    ax.plot(inl_ths, mAAs, label=dset, color=colors[i])\n    ax.plot(best_th, best_mAA, label = f'{dset}-best', marker='x', linestyle='', color=colors[i])\n    final_ths[dset] = best_th\n    \nax.legend()\nax.set_ylabel('mAA')\nax.set_xlabel('DEGENSAC inlier threshold')\n    \n\nBest phototourism mAA = 0.7108 with inl_th = 0.5\nBest pragueparks mAA = 0.6700 with inl_th = 1.5\nBest googleurban mAA = 0.3116 with inl_th = 0.75\n\n\nText(0.5, 0, 'DEGENSAC inlier threshold')"
  },
  {
    "objectID": "posts/2021-05-12-submitting-to-imc2021-step-by-step.html#creating-final-submission",
    "href": "posts/2021-05-12-submitting-to-imc2021-step-by-step.html#creating-final-submission",
    "title": "Submitting to Image Matching Challenge 2021",
    "section": "Creating final submission",
    "text": "Creating final submission\nIts time to create our final submission!\n\n\nconfigs = []\ncurrent_config = deepcopy(base_config)\nfor dset in ['phototourism', 'pragueparks', 'googleurban']:\n    current_config[f'config_{dset}_stereo']['geom']['threshold'] = final_ths[dset]\n    # I did a little bit of tuning offline for multiview, so we will put it here\n    current_config[f'config_{dset}_multiview']['matcher']['filtering']['threshold'] = 0.95\n    \n    #Remember, that we should not forget to turn FLANN ofd\n    current_config[f'config_{dset}_multiview']['matcher']['flann'] = False\n    current_config[f'config_{dset}_stereo']['matcher']['flann'] = False\n    \n    \ncurrent_config['metadata']['method_name'] = 'KORNIA TUTORIAL CV-DoG-AffNet-HardNet8'\n\nlabel = current_config['config_common']['json_label'] \ncurrent_config['config_common']['json_label']  = f'{label}'\nconfigs.append(current_config)\n\nprint (current_config)\nwith open('final_submission.json', 'w') as f:\n    json.dump(configs, f, indent=2)\n\n{'metadata': {'publish_anonymously': False, 'authors': 'Dmytro Mishkin, Milan Pultar and kornia team', 'contact_email': 'ducha.aiki@gmail.com', 'method_name': 'KORNIA TUTORIAL CV-DoG-AffNet-HardNet8', 'method_description': \"OpeCV SIFT keypoints 8000 features, followed by the AffNet normalization \\n    and HardNet8 descriptor as implemented in kornia.\\n    Matched using the built-in matcher (bidirectional filter with the 'both' strategy, \\n    hopefully optimal inlier and ratio test thresholds) with DEGENSAC\", 'link_to_website': 'https://github.com/kornia/kornia', 'link_to_pdf': 'https://arxiv.org/abs/2007.09699'}, 'config_common': {'json_label': 'dog-affnet-hardnet8-degensac', 'keypoint': 'cv2dog', 'descriptor': 'affnethardnet8', 'num_keypoints': 8000}, 'config_phototourism_stereo': {'use_custom_matches': False, 'matcher': {'method': 'nn', 'distance': 'L2', 'flann': False, 'num_nn': 1, 'filtering': {'type': 'snn_ratio_pairwise', 'threshold': 0.9}, 'symmetric': {'enabled': True, 'reduce': 'both'}}, 'outlier_filter': {'method': 'none'}, 'geom': {'method': 'cmp-degensac-f', 'threshold': 0.5, 'confidence': 0.999999, 'max_iter': 100000, 'error_type': 'sampson', 'degeneracy_check': True}}, 'config_phototourism_multiview': {'use_custom_matches': False, 'matcher': {'method': 'nn', 'distance': 'L2', 'flann': False, 'num_nn': 1, 'filtering': {'type': 'snn_ratio_pairwise', 'threshold': 0.95}, 'symmetric': {'enabled': True, 'reduce': 'both'}}, 'outlier_filter': {'method': 'none'}, 'colmap': {}}, 'config_pragueparks_stereo': {'use_custom_matches': False, 'matcher': {'method': 'nn', 'distance': 'L2', 'flann': False, 'num_nn': 1, 'filtering': {'type': 'snn_ratio_pairwise', 'threshold': 0.9}, 'symmetric': {'enabled': True, 'reduce': 'both'}}, 'outlier_filter': {'method': 'none'}, 'geom': {'method': 'cmp-degensac-f', 'threshold': 1.5, 'confidence': 0.999999, 'max_iter': 100000, 'error_type': 'sampson', 'degeneracy_check': True}}, 'config_pragueparks_multiview': {'use_custom_matches': False, 'matcher': {'method': 'nn', 'distance': 'L2', 'flann': False, 'num_nn': 1, 'filtering': {'type': 'snn_ratio_pairwise', 'threshold': 0.95}, 'symmetric': {'enabled': True, 'reduce': 'both'}}, 'outlier_filter': {'method': 'none'}, 'colmap': {}}, 'config_googleurban_stereo': {'use_custom_matches': False, 'matcher': {'method': 'nn', 'distance': 'L2', 'flann': False, 'num_nn': 1, 'filtering': {'type': 'snn_ratio_pairwise', 'threshold': 0.9}, 'symmetric': {'enabled': True, 'reduce': 'both'}}, 'outlier_filter': {'method': 'none'}, 'geom': {'method': 'cmp-degensac-f', 'threshold': 0.75, 'confidence': 0.999999, 'max_iter': 100000, 'error_type': 'sampson', 'degeneracy_check': True}}, 'config_googleurban_multiview': {'use_custom_matches': False, 'matcher': {'method': 'nn', 'distance': 'L2', 'flann': False, 'num_nn': 1, 'filtering': {'type': 'snn_ratio_pairwise', 'threshold': 0.95}, 'symmetric': {'enabled': True, 'reduce': 'both'}}, 'outlier_filter': {'method': 'none'}, 'colmap': {}}}\n\n\nSubmission Zip file should have folder structure as follow:\n ├── config.json\n ├── [Dataset 1]\n │   ├── [Sequence 1]\n │   │   ├── keypoints.h5\n │   │   ├── descriptors.h5\n │   │   ├── matches.h5\n │   ├── [Sequence 2]\n │   │   ├── ...\n ├── [Dataset 2]\n │   ├── ...\nSo we have to just copy our features, add config and zip them.\ncp final_submission.json extracted/cv2-dog-affnet-hardnet8/config.json\ncd extracted/cv2-dog-affnet-hardnet8\nzip -r submission.zip *\nLast step before the submission - check the submission for correctness with provided script\ncd ../../../image-matching-benchmark\npython -utt submission_validator.py --submit_file_path ../imc2021-sample-kornia-submission/extracted/cv2-dog-affnet-hardnet8/submission.zip --benchmark_repo_path . --raw_data_path ../imc-2021-data/ --datasets googleurban phototourism pragueparks\nIf everything is correct, you will see:\nValidating method 1/1: \"dog-affnet-hardnet8-degensac\"\n['googleurban', 'phototourism', 'pragueparks']\nRunning: googleurban, stereo track\nRunning: googleurban, multiview track\nRunning: phototourism, stereo track\nRunning: phototourism, multiview track\nRunning: pragueparks, stereo track\nRunning: pragueparks, multiview track\nValidating key \"config_googleurban_stereo\"\nValidating key \"config_googleurban_multiview\"\nValidating key \"config_phototourism_stereo\"\nValidating key \"config_phototourism_multiview\"\nValidating key \"config_pragueparks_stereo\"\nValidating key \"config_pragueparks_multiview\"\nAnd file submission_log.txt will appear near our .zip file.\ncat ../imc2021-sample-kornia-submission/extracted/cv2-dog-affnet-hardnet8/submission_log.txt\nSubmission is in proper format, please submit to IMW 2021 website.\nThat’s all, folks! We can submit! But, please, do not just submit this sample submission - make your own :)"
  },
  {
    "objectID": "posts/2025-01-02-chatgpt-image-matching.html",
    "href": "posts/2025-01-02-chatgpt-image-matching.html",
    "title": "ChatGPT and Image Matching",
    "section": "",
    "text": "Given the recent success of OpenAI o3 in ARC Challenge, I decided to check the performace of the currently available o1 and 4o models in image matching.\nHowever, asking LLMs to find relative pose or pixel-wise correspondences is pretty stupid (of course, I have tried to do this as well, see the end of this post), that’s why I have focused on the simpler classification task – given two images, tell if they depict covisible areas, or not.\n\nDespite the simiplicity of the task formulation, it is a yet unsolved task, being one of the hardest things in the Image Matching Challenge 2024, because it take a single misregistered pair to ruin your 3D reconstruction.\nIn a nutshell, methods, based on the global descriptor (such as ArcGeM, DINOv2 + pooling, etc) usually lack in recall and fail under zoom/viewpoint change. And the methods, based on the local descriptor similarity, even with learned matching like LightGlue or RoMA lack in precision and fail for repeated patterns and symmetries.\nRecently, there were works tackling a specific instance of this problem - Doppelgangers, and Doppelgangers++ with a task-specific training and the dataset. But this post of not about them.\nEverything you (didn’t) want to know about image matching"
  },
  {
    "objectID": "posts/2025-01-02-chatgpt-image-matching.html#image-co-visibility-as-a-proxy-task",
    "href": "posts/2025-01-02-chatgpt-image-matching.html#image-co-visibility-as-a-proxy-task",
    "title": "ChatGPT and Image Matching",
    "section": "",
    "text": "Given the recent success of OpenAI o3 in ARC Challenge, I decided to check the performace of the currently available o1 and 4o models in image matching.\nHowever, asking LLMs to find relative pose or pixel-wise correspondences is pretty stupid (of course, I have tried to do this as well, see the end of this post), that’s why I have focused on the simpler classification task – given two images, tell if they depict covisible areas, or not.\n\nDespite the simiplicity of the task formulation, it is a yet unsolved task, being one of the hardest things in the Image Matching Challenge 2024, because it take a single misregistered pair to ruin your 3D reconstruction.\nIn a nutshell, methods, based on the global descriptor (such as ArcGeM, DINOv2 + pooling, etc) usually lack in recall and fail under zoom/viewpoint change. And the methods, based on the local descriptor similarity, even with learned matching like LightGlue or RoMA lack in precision and fail for repeated patterns and symmetries.\nRecently, there were works tackling a specific instance of this problem - Doppelgangers, and Doppelgangers++ with a task-specific training and the dataset. But this post of not about them."
  },
  {
    "objectID": "posts/2025-01-02-chatgpt-image-matching.html#summary",
    "href": "posts/2025-01-02-chatgpt-image-matching.html#summary",
    "title": "ChatGPT and Image Matching",
    "section": "Summary",
    "text": "Summary\nBoth ChatGPTs are surprisingly good at image co-visibility, although the specialised models like MASt3R and based on it Doppelgangers++ seem to be better at the moment.\nThe most impressive things to me are the fact that ChatGPT is likely to be never trained for that task, yet exhibiliting very strong performance.\nMoreover, its explanations are very human-like, reminding me my own attempt to explain image matching in simple terms\nThe prompt I gave it, was quite simple:\nHi, \nLet's do the following thing - I will give you two images \nand you will tell me if they share some common part of the scene, or not. \n\nThe 4o answer is:\nBased on the two images provided, it appears that they do share a common part of the scene. Both images include:\n- A waterfall-like feature in the background with a stone or rock wall surrounding it.\n- A statue in the center of the pool in the foreground.\n- A pool of water with rocks and vegetation around its edges.\nWhile the weather and lighting conditions differ between the two images (one has snow and the other is without snow), the structural elements of the scene indicate that they depict the same location.\n\nThe o1 answer:\nIt appears they do share the same setting. Both images show the same stone waterfall in the background, the same pond in front of it, and the same statue at the water’s edge (though captured from slightly different angles and likely at different times).\nI will cover more cases with details below."
  },
  {
    "objectID": "posts/2025-01-02-chatgpt-image-matching.html#more-examples-some-hard-some-easy",
    "href": "posts/2025-01-02-chatgpt-image-matching.html#more-examples-some-hard-some-easy",
    "title": "ChatGPT and Image Matching",
    "section": "More examples, some hard, some easy",
    "text": "More examples, some hard, some easy\nAll the experiments were done within the same session, so I haven’t duplicated detailed prompt.\n\nFamous landmarks\nWhen the location is known to the model, ChatGPT directly uses its information:\n\n4o answer:\nThese two images do not depict the same location or share common parts of a scene.\n\n- The first image is of Maidan Nezalezhnosti (Independence Square) in Kyiv, which features a central monument and surrounding urban landscape.\n- The second image is of the House with Chimeras, an iconic building in Kyiv known for its ornate and unique architectural design, located near the Presidential Office.\n\nWhile both are landmarks in Kyiv, they are distinct locations with no overlapping elements in these particular views.\n\n\nReal photo versus miniature model\nBoth, o1 and 4o are able match the real object to its miniature, but also point out their different nature:\n\no1 answer:\nThought about co-visible parts in images for 11 seconds\n\nThey depict the same place—Kyiv’s Independence Square (Maidan Nezalezhnosti)—but in two very different forms. The first is a real, full-scale photo looking down on the square, and the second is a scale model or miniature re-creation of that same layout (including the central column with the statue, fountain domes, and the surrounding buildings).\nThis kind of distinction is impossible with classical local feature matching pipeline. Here is DISK-LightGlue-RANSAC, and RoMA-RANSAC matching result, run using Realcat image matching demo.\n\n\n\n\nObjects\nThe next thing I have tried, were the objects from Google Scanned Objects dataset.\n\no1 gave me the following answer\nIt looks like both photos show the same bundle of plain brown bags, just viewed from different angles. The label and overall shape match, so they are likely the same object.\n\n&gt; Now try to answer, if there are same parts of the box shown, or the two viewpoints are completely disjoint (different sides of the coin are disjoint in that sense)\n\nEven though the second shot is mostly from “above” and the first shot is more of a “front” view, you can still see parts of the same surfaces (for instance, the top flap/edge of the package) in both images. They are not completely disjoint angles—there is overlap where the top and corners of the package appear in both photos.\nI was trying to ask for some visualizations, but o1 was not able to do it:\nI’m afraid I don’t have the ability to directly generate and attach a new image file here. However, I can describe how you might create one yourself (e.g., in an image-editing tool) to highlight the co-visible areas:\n\nLoad both images into an editor like Photoshop or GIMP, each on its own layer (or in two separate windows so that you can work side by side).\nWhile 4o have tried to use python for this, although now succesfully:\nI have generated the images with color-coded overlays for the approximate covisible regions. You can download them using the links below, and the code\nfrom PIL import Image, ImageDraw\n\n# Load the two images\nimage1_path = \"/mnt/data/image_1.png\"\nimage2_path = \"/mnt/data/image_126.png\"\nimage1 = Image.open(image1_path).convert(\"RGBA\")\nimage2 = Image.open(image2_path).convert(\"RGBA\")\n\n# Create overlays for covisible parts\noverlay1 = Image.new(\"RGBA\", image1.size, (255, 0, 0, 100))  # Red overlay\noverlay2 = Image.new(\"RGBA\", image2.size, (0, 0, 255, 100))  # Blue overlay\n\n# Simulating an approximation of covisible regions (mockup as actual analysis is unavailable)\n# Approximate region to mark on both images\ndraw1 = ImageDraw.Draw(overlay1)\ndraw2 = ImageDraw.Draw(overlay2)\n\n# Assume circular region near the cap for covisibility in both images\n# Marking covisible areas\ndraw1.ellipse([50, 10, 150, 110], fill=(255, 0, 0, 100))\ndraw2.ellipse([60, 20, 160, 120], fill=(0, 0, 255, 100))\n\n# Combine overlays with original images\nimage1_with_overlay = Image.alpha_composite(image1, overlay1)\nimage2_with_overlay = Image.alpha_composite(image2, overlay2)\n\n# Save the output images for visualization\noutput_image1_path = \"/mnt/data/image1_with_covisible_overlay.png\"\noutput_image2_path = \"/mnt/data/image2_with_covisible_overlay.png\"\nimage1_with_overlay.save(output_image1_path)\nimage2_with_overlay.save(output_image2_path)\n\noutput_image1_path, output_image2_path\nThen I have it a squirrel:\n\nHere is conversation with 4o:\nThese two images depict what appears to be the same object (a stuffed animal), but from different angles.\n\nAnalysis:\n- Similarities: The fur texture, tail, and general proportions are consistent across both images.\n- Differences: The perspective changes the visible features (e.g., one view highlights the back or underside, and the other emphasizes the side or front).\n\n&gt;Same, or not? Give me a probability if they are the same\n\n\nProbability Assessment\nGiven the above observations, I would estimate a 75-85% probability that these images are representations or variations of the same base design, considering the consistent features and patterns across both. However, the exact arrangement and perspective differences make them not entirely identical.\n\n&gt;You are right, it is the same stuffed animal -- squirrel. Now try to answer, if there are same parts of the animal shown, or the two viewpoints are completely disjoint (different sides of the coin are disjoint in that sense)\n\n\nThe two images are not completely disjoint; there are some overlapping parts of the stuffed animal visible in both viewpoints. Here's why:\n\nOverlapping Parts:\n- The tail is partially visible in both images, though it is viewed from different angles.\n- The fur texture on the sides and parts of the body overlaps in both images, though the specific patches might vary.\n\nSome portions of the ears and legs are shared between the two viewpoints.\n\nDisjoint Parts:\n\n- The front-facing details like the face or front paws are visible only in one image.\n- The underside or back is featured more prominently in the other image.\n\nConclusion:\n\nThe viewpoints are partially overlapping, as they show some shared features (e.g., the tail and parts of the body) but also highlight distinct parts of the stuffed animal. This is not a fully disjoint \"different sides of the coin\" scenario."
  },
  {
    "objectID": "posts/2025-01-02-chatgpt-image-matching.html#actually-difficult-examples-from-wxbs-evd-and-imc-2024",
    "href": "posts/2025-01-02-chatgpt-image-matching.html#actually-difficult-examples-from-wxbs-evd-and-imc-2024",
    "title": "ChatGPT and Image Matching",
    "section": "Actually difficult examples from WxBS, EVD and IMC-2024",
    "text": "Actually difficult examples from WxBS, EVD and IMC-2024\nNow I am going to the some difficult image pairs. The IMC-2024 example was too hard\n\no1 answer:\nThey’re definitely photos of the same building (a distinctive white-and-dark-brick façade with statues and that large clock tower). The first shot is a closer, more front-facing view of the façade, while the second steps back and off to the side so you can see the tower and circular window. You can still spot common elements in both—like the rectangular “niches” with statues and the pattern of bricks—just viewed from different angles.\n\n&gt;Same building - yes, but are there covisible areas or not?\n\nYes, there is some overlap. In the second shot (the more diagonal view), you can still see portions of the same façade panels and statues that appear front-and-center in the first photo. They’re just seen from an angle rather than straight on, so there are co-visible areas.\n4o answer:\nThese two images depict the same building but from different angles.\n\nObservations:\nShared Features (Covisible Elements):\n- The distinctive brick facade with white details and windows is visible in both images.\n- The architectural design, including the cross at the top, the large circular clock, and the unique geometric patterns, is consistent.\n\nDisjoint Features:\n- The first image focuses on the tower side of the building with the large circular clock.\n- The second image highlights the front entrance with statues above the doorways and decorative details at the top.\n\nConclusion:\n\nThese images partially overlap in their view of the building's facade and overall architectural style but focus on different sides. They are not completely disjoint since the same materials and design elements (e.g., brick and white accents) are visible in both images.\nSo, both model fail here. Cannot blame them though, all image matching methods fail as well.\n\nWall with graffity is easy for ChatGPT\n\n4o:\n\nThese two images appear to show the same wall of a building, but from different perspectives.\n\nObservations:\nShared Features (Covisible Elements):\n - Both images display the same decorative emblem/mural on the building's wall.\n - The mural's design, including the scroll-like banner and ornate details, is consistent across both images.\n\nDisjoint Features:\n - The first image shows additional context, such as the parking sign, street lamp, and part of the surrounding building.\n - The second image provides a closer and more vertical perspective, focusing entirely on the mural and the upper portion of the wall.\n \nConclusion:\nThese images are not fully disjoint, as the mural is covisible in both. However, the first image provides a wider context, while the second image is zoomed in and tilted upward to focus on the decorative details.\no1:\n\n\nThought about co-visible areas in images for 13 seconds\nThey do show the same painted crest on the same façade—just photographed from two different angles. In the first image, you see the crest at an angle (with a parking sign in the foreground), and in the second shot you’re looking almost straight up at that same crest. So yes, there is a co‐visible area: the mural/coat of arms and the surrounding wall are present in both.\n\nIt didn’t fall for the non-matching image pair as well.\no1:\n\nThey appear to be two different graffiti walls or surfaces rather than two angles of the same one. The first image shows a rough‐textured wall with multiple tags (including black, purple, and orange) around a door. The second is a smoother wall with a large, more “illustrated” design featuring pink bubbles or orbs on white/purple. There’s no obvious overlap or matching elements that would indicate it’s the same wall or the same artwork. They look like separate, unrelated graffiti pieces.\n4o gave a similar, but more verbose answer.\n\n\nZoom is a still a problem\nNow I went for the IMC-2024 training set, quite hard image pair, showing zoomed-in night versus zoomed out day\n\no1:\nThought about co-visible areas in images for 7 seconds\nThey appear to be entirely different subjects/scenes:\n\n- First photo: A daytime shot of an outdoor area with stone balustrades, steps, and what looks like a sundial or sculpture in the middle, plus a pavilion/roofed structure in the background.\n- Second photo: A close-up (at night) of a small metal lizard or reptile sculpture attached to a wall or another surface.\nThere’s no obvious overlap of location or objects, so they don’t seem to be showing the same place or the same elements.\n4o:\n\nThank you for the kind words! Let’s analyze these two images.\n\nObservations:\nFirst Image:\n- Depicts an outdoor scene with a stone sundial surrounded by vegetation.\n- There are stairs and a curved stone railing, indicating a public or park-like setting.\n- A modern pavilion is visible at the top of the image.\n\nSecond Image:\n- Shows a close-up of a lizard-like figure, possibly a sculpture or decorative piece.\n- The lighting suggests it was taken at night, with shadows and a dim atmosphere.\n\nRelationship Between the Images:\n- These two images do not seem to depict the same scene or object. The first image is a broader outdoor setting, while the second focuses on a specific decorative or sculptural element.\n\nThere is no clear overlap or common elements between the two views, indicating they are unrelated or from entirely different locations.\nThen I gave them a hint, which o1 was able to pick-up immediately:\nAh, I see! If you zoom in on the first image, there’s indeed a small metal sculpture (looks like a lizard or salamander) at the base of that sundial stone. The second image is a close‐up photo of that same little creature—just taken at night (which is why it isn’t obvious in the wide daytime shot). So yes, they’re showing the same decorative lizard sculpture, but from very different distances/lighting conditions.\nwhile 4o needed an additional push:\n&gt;What if you zoom into first image? Below the sundial\nLet me analyze the first image more closely and zoom into the area below the sundial to see if there’s any resemblance to the second image. One moment while I enhance the details.\n\nI have zoomed into the area below the sundial in the first image. You can download and inspect the zoomed-in portion using the link below.\n\n&gt;You have zoomed in, and what is your answer now? \n\nUpon zooming into the area below the sundial in the first image, I can confirm the presence of a small lizard sculpture, which resembles the object in the second image. Here's the updated analysis:\n\nObservations:\n\n1. Zoomed-in Area of First Image:\nReveals a lizard-like sculpture positioned on the ground below the sundial.\n- The structure and texture of the lizard sculpture are consistent with what is seen in the second image.\n- The lighting conditions in the first image differ (daylight), but the lizard sculpture matches.\n\n2. Second Image:\n- A close-up view of the same lizard sculpture.\n- Taken at night, highlighting the texture and details under dim lighting.\nConclusion:\nThe two images are related and depict the same lizard sculpture. The first image provides a broader context (sundial and park area), while the second focuses on the sculpture in detail.\n\n\n\nImage pair, humans have problem with\nI have shown this image pair to the members of my family, and, unless the person I asked, was there, they fail to match these images. However, it wasn’t an issue for the ChatGPT\n\n4o:\nThese two images depict the same location, but under different lighting and atmospheric conditions.\n\nObservations:\n1. First Image:\nShows the field during a misty morning with a clear blue sky.\n- The distant trees are partially obscured by fog, creating a calm and serene atmosphere.\n- The light suggests early morning or a cool, foggy day.\n\n2. Second Image:\nDepicts the same field during a cloudy sunset or dusk.\n- The warm tones of the sky and the lighting give a dramatic, golden-hour effect.\n- Two people are visible in the foreground, adding a sense of scale and life to the image.\n\n3. Shared Elements:\n- The hills and tree line in the background are identical in both images.\n- The pathways and grassy fields match, though they appear more vibrant in the second image due to the lighting.\n- The perspectives are similar, though the second image includes human subjects.\n\nConclusion:\nThese are two views of the same location taken at different times of the day and under different weather conditions, emphasizing its versatility and natural beauty.\no1:\nYes, these two photos show the same meadow and tree-covered hill in the background—it’s just taken at different times/conditions (one in early morning fog, the other under a dramatic sky with two people in the shot). You can match the ridgeline and tree layout on the hill in both images."
  },
  {
    "objectID": "posts/2025-01-02-chatgpt-image-matching.html#conclusion",
    "href": "posts/2025-01-02-chatgpt-image-matching.html#conclusion",
    "title": "ChatGPT and Image Matching",
    "section": "Conclusion",
    "text": "Conclusion\nOverall I believe, that having a VLM, possibly equipped with access to some low-level SIFT/LightGlue-like instruments is the proper way to handle the image covisibility estimation problem.\nIt is hard to distinguish between photo of the real object and its miniature, or other similar things without more general, semantic understanding of the scene. And the way how ChatGPT handles this, is similar to the way human person would handle it.\nP.S. The images used in the post come from:\n\nImage Matching Challenge 2024\nWxBS dataset\nEVD dataset\nIGoToWorld Київ в мініатюрі\nGoogle Scanned Objects dataset\n\nSpecial thanks to image-matching-webui\n\nBonus: ChatGPT as homography estimator\n\n\no1 response\n# Thought about homography matrix for 31 seconds\n\nimport numpy as np\n\n# Just the resulting 3x3 matrix (homography from image1 to image2),\n# with no further explanation:\n\nH = np.array([\n    [ 1.087234, -0.153872,  34.5912 ],\n    [ 0.214762,  1.028347, -47.2135 ],\n    [ 0.000492,  0.000631,   1.0    ]\n], dtype=np.float64)\n\nprint(H)\nAnd resulting warp was wrong, but in a correct direction:\n\nThe guess process is not stable though, here is another attempt with exactly same prompt:\n\nAfter I have asked it, how it end up with it, o1 was kind of avoiding the answer,\n&gt;Have you generated it just looking at images, or were you using some coding?\n\nChatGPT\n\nThought about homography estimation process for 16 seconds\nI obtained that matrix using a quick coding approach to estimate the homography.\n\n&gt;Could you please share the code you used?\n\nChatGPT\n\nThought about homography estimation code for 22 seconds\nimport cv2\nimport numpy as np\n\n# Just a minimal snippet of the code used to find the homography:\n\n# Example matched points (not the real ones from the images),\n# replaced here with placeholders:\npts_img1 = np.float32([\n    [100, 150],\n    [200, 150],\n    [200, 250],\n    [100, 250]\n])\npts_img2 = np.float32([\n    [132, 107],\n    [245, 158],\n    [243, 276],\n    [108, 230]\n])\n\n# Compute homography:\nH, _ = cv2.findHomography(pts_img1, pts_img2, cv2.RANSAC)\n\nprint(\"Estimated Homography matrix:\\n\", H)\n\nThis snippet outlines the general approach used; the actual point correspondences were specific to these images.\nFinally, when I insisted, 1o shared the SIFT matching code with ratio test of 0.75.\n\n\n4o answer\nimport numpy as np\n\nH = np.array([\n    [0.103082467, -0.246502775, 20.9770923],\n    [-0.208395367, -0.944791621, 370.672629],\n    [-0.000139310557, -0.00286464375, 1.00000000]\n])\n\nAfter I have asked it, how it end up with it, 4o said, that it was using ORB matching.\n&gt;Have you generated it just looking at images, or were you using some coding?\nChatGPT\nI used Python code to calculate the homography matrix. Specifically, I utilized feature matching and keypoint detection techniques (ORB detector and RANSAC for outlier removal) to estimate the transformation matrix between the two uploaded images.\nI have suggested to use SIFT instead, and the result was not better (the graffity pair is too hard, and there were no Lowe ratio test in the code. Still cool though.\n# Re-attempt using SIFT for feature detection and matching\nsift = cv2.SIFT_create()\n\n# Detect SIFT keypoints and descriptors\nkp1, des1 = sift.detectAndCompute(gray1, None)\nkp2, des2 = sift.detectAndCompute(gray2, None)\n\n# Use BFMatcher with L2 norm for SIFT descriptors\nbf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\nmatches = bf.match(des1, des2)\n\n# Sort matches by distance\nmatches = sorted(matches, key=lambda x: x.distance)\n\n# Extract matched keypoints\nsrc_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\ndst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n\n# Estimate homography\nH, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n\nH"
  },
  {
    "objectID": "posts/2021-09-04-review-of-pseudogt-limitations.html",
    "href": "posts/2021-09-04-review-of-pseudogt-limitations.html",
    "title": "Review of On the Limits of Pseudo Ground Truth in Visual Camera Re-localisation",
    "section": "",
    "text": "I would like to share my thoughts on ICCV 2021 paper “On the Limits of Pseudo Ground Truth in Visual Camera Re-localisation” by Brachmann et.al. But first let’s recap, what is Visual relocalization and how it is usually evaluated.\nEverything you (didn’t) want to know about image matching"
  },
  {
    "objectID": "posts/2021-09-04-review-of-pseudogt-limitations.html#my-take-on-the-paper.",
    "href": "posts/2021-09-04-review-of-pseudogt-limitations.html#my-take-on-the-paper.",
    "title": "Review of On the Limits of Pseudo Ground Truth in Visual Camera Re-localisation",
    "section": "My take on the paper.",
    "text": "My take on the paper.\nThe paper asks a great question and provides good answers. It is also well-written and the experiments done with rigor. OK, time to stop “CVPR-review” mode and provide my thoughts.\nI love the paper. However, (post-hoc) I believe that paper should stop earlier or go deeper.\nFirst and main message of the paper (which paper does NOT formulate in that way):\nThe combination of the current methods (SfM or SLAM) with a current way of generating training sequence (single camera going around the place once) does not lead to the robust and precise enough camera poses.\nI don’t buy the paper’s argument that “depending on the metric you use, one or another is better”. No. There are only 3 possible explanations:\n\nSfM estimates wrong poses from the sequence\nRGBD-SLAM estimates wrong poses from the sequence\nboth SfM and RGBD-SLAM estimate wrong poses. The variant: noone cannot estimate precise poses from the given images.\n\nWe are not in the quantum mechanics world, there is a single, well-defined (possibly unknown) correct camera pose. The correct pose exists and the question is whether SfM or SLAM (or neither) is closer to it. Yes, it would require some additional sensors, or running in simulated world, but anyway - the message should not be “just provide 2 versions of pGT”. The message is: if your 2 versions of pGT does not agree, GET MORE/BETTER DATA.\nIn addition to that I would like to see a paper, which answers the following question: given tha data we have (i.e. bad data) but with also available real GT camera poses, under which conditions, which algorithm (SfM or Depth SLAM) provides better results?\n\nWhich algorithm can adapt to (wrong) training data and be consistent with it?\nOne thing, which paper focuses on is how the “test” pGT is generated. However, even the bigger impact can be from the fact that training data may conflict with the poses, that algorithm estimates.\nImagine the augmented reality scenario, where one needs to draw the (virtual) teddy bear on the coach. In that case we actually don’t care if the camera pose estimate is correct, or if our model of the room is correct. What we care about, is how naturally looking will be position of the Teddy bear (or Pikachu). It also important that bear not jumps around when we move our camera slightly.\nGiven the images and results in the paper, it seems that (learning-based) DSAC can suits this much better than (non-learning) AS. That is also stated in the paper conclusions. In the same time, it seems that the variance of the pose estimates is smaller for the SfM pGT than for the SLAM even for DSAC.\nHowever, I would also like to see a paper, which studies how the error/bias/variance in the training data affects the different algorithms, so they could adapt to use bad data better, in more detail."
  },
  {
    "objectID": "posts/2021-07-30-reviving-wxbs-benchmark.html",
    "href": "posts/2021-07-30-reviving-wxbs-benchmark.html",
    "title": "WxBS: Relaunching challenging benchmark for Image Matching",
    "section": "",
    "text": "Previously we have discussed what is WxBS in general, how to match images from a really different viewpoints and also took a look into Image Matching Challenge 2020. Let’s now speak about how to measure progress on images, which are really hard to match even to human?\nFor example, which have been taked in day-vs-night, years apart AND from different camera positions? One cannot register them to some 3d model, because they are too hard. And there is no 3D model either.\n\nWell, in that case one could go back to basics and handlabel the correspondences between the images. Such annotation is unlikely to be very precise, but it is going to be quite robust, if the labeler is careful.\nThat is what I have done 6 years ago and published such a dataset at BMVC 2015 together with a benchmark of popular image matchers of that time. None of them was able to successfully match even a fraction of the WxBS dataset.\n\nHowever, as I was quite inexpereinced in product packaging, as well as in software engineering, the benchmark existed as dataset + bunch of bash and Matlab scripts. It is needless to say, that nobody evaluated their algorithms on it. That made me sad, however, I never found time to make this benchmark easy to use. Well, now I did it and want to share the evaluation of the recent agorithms on such a hard data.\nEverything you (didn’t) want to know about image matching"
  },
  {
    "objectID": "posts/2021-07-30-reviving-wxbs-benchmark.html#the-hardest-image-matching-benchmark",
    "href": "posts/2021-07-30-reviving-wxbs-benchmark.html#the-hardest-image-matching-benchmark",
    "title": "WxBS: Relaunching challenging benchmark for Image Matching",
    "section": "",
    "text": "Previously we have discussed what is WxBS in general, how to match images from a really different viewpoints and also took a look into Image Matching Challenge 2020. Let’s now speak about how to measure progress on images, which are really hard to match even to human?\nFor example, which have been taked in day-vs-night, years apart AND from different camera positions? One cannot register them to some 3d model, because they are too hard. And there is no 3D model either.\n\nWell, in that case one could go back to basics and handlabel the correspondences between the images. Such annotation is unlikely to be very precise, but it is going to be quite robust, if the labeler is careful.\nThat is what I have done 6 years ago and published such a dataset at BMVC 2015 together with a benchmark of popular image matchers of that time. None of them was able to successfully match even a fraction of the WxBS dataset.\n\nHowever, as I was quite inexpereinced in product packaging, as well as in software engineering, the benchmark existed as dataset + bunch of bash and Matlab scripts. It is needless to say, that nobody evaluated their algorithms on it. That made me sad, however, I never found time to make this benchmark easy to use. Well, now I did it and want to share the evaluation of the recent agorithms on such a hard data."
  },
  {
    "objectID": "posts/2021-07-30-reviving-wxbs-benchmark.html#not-that-fast-problem-with-a-fundamental-matrix-and-how-to-solve-them",
    "href": "posts/2021-07-30-reviving-wxbs-benchmark.html#not-that-fast-problem-with-a-fundamental-matrix-and-how-to-solve-them",
    "title": "WxBS: Relaunching challenging benchmark for Image Matching",
    "section": "Not that fast! Problem with a fundamental matrix and how to solve them",
    "text": "Not that fast! Problem with a fundamental matrix and how to solve them\nOK, we have ground truth correspondences. Now what? Ideally one can estimate epipolar geometry from as little, as 7 correspondences, but in practice? No. The problem is that there could be many relative camera poses, which are consistent with the correspondences, especially when correspondences lie in plane and the camera calibration is unknown.\nLet’s consider a simple example: we have 13 correspondences and they seems to be correct. Are they enough for finding good epipolar geometry? This question can be answered via leave-one-out validation procedure, described in VSAC paper. We remove a single correspondence, estimate a fundamental matrix via DLT algorithm on the rest of the correspondences and see how the estimated epipolar geometry is changed (or not).\nAs you can see on the animation below, the epipolar geometry estimation from our 13 correspondences is not stable and removing any of them results in noticable change.\n\nSo, if even manually labeled corresposndences are not good enough, what can be done?\nFirst, simply add more correspondences, which are spread across image AND depth levels as even as possible. That is what I did recently, in order to make WxBS labeling better. However, it is not always possible – sometimes you have lots of correspondence on “faraway background plane” and very little somewhere on foreground, see next picture.\n\nWhat else can be done? We can go other way round and not provide any ground truth fundamental matrix at all. Instead, we ask methods, which we evaluate, to give us such matrix F_est, then check, how many ground truth correspondences are consident with it.\nThe bad thing about it, is that unless all 100% GT correspondences are consistent with F_est, F_est is completely wrong in terms of camera pose accuracy. So, such a binary signal on a very challenging pairs is not that useful. The good thing about it, is that if, say, 50% GT correspondences are consistent with F_est, it means that detected (not GT) correspondences on the part of the image are correct (e.g. on some dominant plane) and at least there we are OK.\nIf you are interested in more mathematical definition, here it is:\nThe recall on ground truth correspondences \\(C_i\\) of image pair \\(i\\) and for geometry model \\(\\mathbf{M}_i\\) is computed as a function of a threshold \\(\\theta\\)\n\\[\\begin{equation}\n\\mathrm{r}_{i,\\mathbf{M}_i}(\\theta) = \\frac{| \\{(\\mathbf{u},\\mathbf{v}) : (\\mathbf{u},\\mathbf{v}) \\in C_i,  e(\\mathbf{M}_i,\\mathbf{u},\\mathbf{v}) &lt; \\theta\\}|}{| C_i |}\n% \\mathrm{r}_{i,\\mathbf{M}_i}(\\theta) = \\frac{| \\{(\\mathbf{u}_i,\\mathbf{v}_i) : (\\mathbf{u}_i,\\mathbf{v}_i) \\in C_i,  e(\\mathbf{M}_i,\\mathbf{u},\\mathbf{v}) &lt; \\theta\\}|}{| C_i |}\n\\end{equation}\\]\nusing symmetric epipolar distance."
  },
  {
    "objectID": "posts/2021-07-30-reviving-wxbs-benchmark.html#lets-evaluate",
    "href": "posts/2021-07-30-reviving-wxbs-benchmark.html#lets-evaluate",
    "title": "WxBS: Relaunching challenging benchmark for Image Matching",
    "section": "Let’s evaluate!",
    "text": "Let’s evaluate!\nFirst I will show you how to run the WxBS benchmark yourself and then present the results I got. Benchmark is available from pip:\n\n!pip install wxbs_benchmark\n!pip install kornia_moons\n\nNow we will write a simple function using OpenCV RootSIFT and MAGSAC++. RootSIFT is a gold standard handcrafted local feature and MAGSAC++ is a state-of-the-art RANSAC, which is not sensitive to an inlier threshold, as my recent benchmark shows. We will match then using mutual second nearest ratio, as implemented in kornia\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nimport torch\nimport kornia.feature as KF\nfrom kornia_moons.feature import *\nfrom wxbs_benchmark.dataset import *\nfrom wxbs_benchmark.evaluation import *\nfrom tqdm import tqdm \n\ndef sift2rootsift(desc):\n    desc /= desc.sum(axis=1, keepdims=True) + 1e-8\n    desc = np.sqrt(desc)\n    return desc\n\ndef estimate_F_WithRootSIFT(img1, img2):\n    det = cv2.SIFT_create(8000, contrastThreshold=-10000, edgeThreshold=10000)\n    kps1, descs1 = det.detectAndCompute(img1, None)\n    kps2, descs2 = det.detectAndCompute(img2, None)\n    descs1 = sift2rootsift(descs1)\n    descs2 = sift2rootsift(descs2)\n    snn_ratio, idxs = KF.match_smnn(torch.from_numpy(descs1),\n                                    torch.from_numpy(descs2), 0.95)\n    tentatives = cv2_matches_from_kornia(snn_ratio, idxs)\n    src_pts = np.float32([ kps1[m.queryIdx].pt for m in tentatives ]).reshape(-1,2)\n    dst_pts = np.float32([ kps2[m.trainIdx].pt for m in tentatives ]).reshape(-1,2)\n    F, inlier_mask = cv2.findFundamentalMat(src_pts, dst_pts, cv2.USAC_MAGSAC, 0.25, 0.999, 100000)\n    return F\n\n\nsubset = 'test'\ndset = WxBSDataset('.WxBS', subset=subset, download=True)\n\n\nF_results_RootSIFT = []\n\nfor pair_dict in tqdm(dset):\n    current_F = estimate_F_WithRootSIFT(pair_dict['img1'], pair_dict['img2'])\n    F_results_RootSIFT.append(current_F)\n\nresult_dict_rootsift, thresholds = evaluate_Fs(F_results_RootSIFT, subset)\n\nWe can check results for individual image pairs, or just take an average\n\nprint(result_dict_rootsift.keys())\nplt.figure()\nplt.plot(thresholds, result_dict_rootsift['average'], '-x')\nplt.ylim([0,1.05])\nplt.xlabel('Thresholds')\nplt.ylabel('Recall on GT corrs')\nplt.grid(True)\nplt.legend(['RootSIFT + MAGSAC++'])\n\ndict_keys(['WGABS/kremlin', 'WGABS/kyiv', 'WGABS/strahov', 'WGABS/vatutin', 'WGALBS/bridge', 'WGALBS/flood', 'WGALBS/kyiv_dolltheater', 'WGALBS/rovenki', 'WGALBS/stadium', 'WGALBS/submarine', 'WGALBS/submarine2', 'WGALBS/tyn', 'WGALBS/zanky', 'WGBS/kn-church', 'WGLBS/alupka', 'WGLBS/berlin', 'WGLBS/charlottenburg', 'WGLBS/church', 'WGLBS/him', 'WGLBS/maidan', 'WGLBS/ministry', 'WGLBS/silasveta2', 'WGLBS/warsaw', 'WGSBS/kettle', 'WGSBS/kettle2', 'WGSBS/lab', 'WGSBS/lab2', 'WGSBS/window', 'WLABS/dh', 'WLABS/kpi', 'WLABS/kyiv', 'WLABS/ministry', 'average'])\n\n\n/opt/homebrew/Caskroom/miniforge/base/envs/python39/lib/python3.9/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n\n\n\n\n\n\n\n\n\nI have run several popular recent methods in this Colab. The code is very dirty, that is why I don’t put it here, and just present results."
  },
  {
    "objectID": "posts/2021-07-30-reviving-wxbs-benchmark.html#methods-tested",
    "href": "posts/2021-07-30-reviving-wxbs-benchmark.html#methods-tested",
    "title": "WxBS: Relaunching challenging benchmark for Image Matching",
    "section": "Methods tested",
    "text": "Methods tested\n\nLocal features without learned matching.\nRootSIFT (ICCV1999 - CVPR-2012), DoG+HardNet (NeurIPS 2017), R2D2 (NeurIPS 2019), DISK (NeurIPS 2020).\n\n\nLocal features with learned matching.\nSuperGlue (CVPR2020) uses attention-based graph neural network to match SuperPoint local features.\n\n\nDense matching\nPDCNet (CVPR 2021). On top of pretrained ImageNet coarse-to-fine feature correlation, PDCNet predicts a dense flow and a confidence for each pixel. Trained on huge dataset with synthetic warps.\nPatch2pix (CVPR 2021). Similarly to PDCNet, patch2pix is uses pretrained ImageNet features to get the initial corresponcences. Then the matches are progressively refined or rejected with a regressor network.\nDFM (CVPRW 2021). Like PDCNet and Patch2pix, DFM uses ImageNet-pretrained and frozen VGGNet backbone as dense feature extractor. Unlike them, DFM does not requite any training and relies on simple handcrafted coarse-to-fine matching from conv5 to conv1.\n\n\nDense matching with transformers\nCOTR, (ICCV2021). Given the pixel location in a query image, COTR finds a corresponing location in the reference image. It operated on the 16x16 feature map produced by pretrained ImageNet model, which then are fed together with a positional embeddings to a transformer-based regressor. It is also the slowest method among evaluated - because you have to run it per each pixel separately, if want dense correspondences.\nLoFTR (CVPR2021). LoFTR is very similar to Patch2pix, with the difference of using transformer architecture to for both coarse initial matching and consequent refinement. The second difference, is that, unlike all other methods, which are using ImageNet-pretrained models as a feature backbone, LoFTR trains ResNet18 from scratch ."
  },
  {
    "objectID": "posts/2021-07-30-reviving-wxbs-benchmark.html#results-discussion",
    "href": "posts/2021-07-30-reviving-wxbs-benchmark.html#results-discussion",
    "title": "WxBS: Relaunching challenging benchmark for Image Matching",
    "section": "Results & Discussion",
    "text": "Results & Discussion\n\n\n\nimage.png\n\n\nThe clear leader is LoFTR, then we have (patch2pix, COTR and SuperGlue) group, followed by (PDCNet, DISK, R2D2). Then DoG-HardNet and RootSIFT, where latter has barely matched couple of image pairs correctly.\nI have no idea, why LoFTR is so good, my guesses would be richer training data and training the backbone from scratch, instead of relying on ImageNet feaures. I am also quite surprised by the great performance of the patch2pix, which probably can be improved even more, if the backbone would be at least fine-tuned.\nAnother surprise is a great performance of the DFM matcher, which is slightly better than PDCNet and worse (but comparable) than COTR. My hypothesis is that all this methods rely on the same ImageNet backbone: if they fail, all the methods fail.\nRegarding SuperGlue - one of my guesses is that original SuperPoint features were not trained for different illuminations. Moreover, in some images, there just not enough matching corners - when I was annotating the correspondences, I have to use also “center of the blobs”.\nFinally, I would like to that than even excellent LofTR result is far from the ideal – there are stil tens of pairs, where it fails.\nP.S. I would also like to thank Prune Truong and Ufuk Efe who helped me to debug my originally wrong run of the PDCNet and DFM respectively.\n\nQualiative examples\nHere are some examples of how different methods work on the same image pairs.\n\n\nSeason + viewpoint\nLoFTR\n\n\n\nimage.png\n\n\nSuperGlue\n\n\n\nimage.png\n\n\npatch2pix\n\n\n\nimage.png\n\n\n\n\nThermal vs visible + viewpoint\nLoFTR\n\n\n\nimage.png\n\n\nSuperGlue\n\n\n\nimage.png\n\n\npatch2pix\n\n\n\nimage.png\n\n\n\n\nSeason + illumination + viewpoint\nLoFTR\n\n\n\nimage.png\n\n\nSuperGlue\n\n\n\nimage.png\n\n\npatch2pix\n\n\n\nimage.png\n\n\n\n\nIllumination + viewpoint change (zoom)\nLoFTR\n\n\n\nimage.png\n\n\nSuperGlue \npatch2pix\n\n\n\nimage.png\n\n\nAs you can, see, there are many incorrect correspondences even in those images, where methods are performing well.\nHere I will conclude and go to vacation.\nIn the 2nd part of the post, I would like to evaluate COTR, PDCNet and different dense descriptors directly on the GT correspondences, by giving a point in image A and asking for point in image B."
  },
  {
    "objectID": "posts/2021-01-09-wxbs-in-simple-terms.html",
    "href": "posts/2021-01-09-wxbs-in-simple-terms.html",
    "title": "Wide multiple baseline stereo in simple terms",
    "section": "",
    "text": "Imagine you have a nice photo you took in autumn and would like to take one in summer, from the same spot. How would you achieve that? You go to the place and you start to compare what you see on the camera screen and on the printed photo. Specifically, you would probably try to locate the same objects, e.g., “that high lamppost” or “this wall clock”. Then one would estimate how differently they are arranged on the old photo and camera screen. For example, by checking whether the lamppost is occluding the clock on the tower or not. That would give an idea of how you should move your camera.\nNow, what if you are not allowed to take that photo with you, because it is a museum photo and taking pictures is prohibited there. Instead you can create a description of it. In that case, it is likely that you would try to make a list of features and objects in the photo together with the descriptions, which are sufficient to distinguish the objects. For example, “a long staircase on the left side”, “The nice building with a dark roof and two towers” or “the top of the lampost”. It would be useful to also describe where these objects and features are pictured in the photo, “The lamp posts are on the left, the closest to the viewer is in front of the left tower with a clock. The clock tower is not the part of the building and stands on its own”. Then when arriving, you would try to find those objects, match them to the description you have, and try to estimate where you should go. You repeat the procedure until the camera screen shows a picture which is fitting the description you have and the image you have in your memory.\nCongratulations! You just have successfully registered the two images, which have a significant difference in viewpoint, appearance, and illumination. In the process of doing so, you were solving multiple times the wide multiple-baseline stereo problem (WxBS) – estimating the relative camera pose from a pair of images, different in many aspects, yet depicting the same scene.\nLet us write down the steps, which we took.\n\nIdentify salient objects and features in the images – “trees”, “statues”, “tip of the tower”, etc in images.\nDescribe the objects and features, taking into account their neighborhood: “statue with a blue left ear”.\nEstablish potential correspondences between features in the different images, based on their descriptors.\nEstimate, which direction one should move the camera to align the objects and features.\n\nThat is it! detailed explanation of the each of the steps is in this post. If you are interested in the formal definition, check here, and the history of the WxBS is here.\nEverything you (didn’t) want to know about image matching"
  },
  {
    "objectID": "posts/2021-01-09-wxbs-in-simple-terms.html#what-is-wide-multiple-baseline-stereo",
    "href": "posts/2021-01-09-wxbs-in-simple-terms.html#what-is-wide-multiple-baseline-stereo",
    "title": "Wide multiple baseline stereo in simple terms",
    "section": "",
    "text": "Imagine you have a nice photo you took in autumn and would like to take one in summer, from the same spot. How would you achieve that? You go to the place and you start to compare what you see on the camera screen and on the printed photo. Specifically, you would probably try to locate the same objects, e.g., “that high lamppost” or “this wall clock”. Then one would estimate how differently they are arranged on the old photo and camera screen. For example, by checking whether the lamppost is occluding the clock on the tower or not. That would give an idea of how you should move your camera.\nNow, what if you are not allowed to take that photo with you, because it is a museum photo and taking pictures is prohibited there. Instead you can create a description of it. In that case, it is likely that you would try to make a list of features and objects in the photo together with the descriptions, which are sufficient to distinguish the objects. For example, “a long staircase on the left side”, “The nice building with a dark roof and two towers” or “the top of the lampost”. It would be useful to also describe where these objects and features are pictured in the photo, “The lamp posts are on the left, the closest to the viewer is in front of the left tower with a clock. The clock tower is not the part of the building and stands on its own”. Then when arriving, you would try to find those objects, match them to the description you have, and try to estimate where you should go. You repeat the procedure until the camera screen shows a picture which is fitting the description you have and the image you have in your memory.\nCongratulations! You just have successfully registered the two images, which have a significant difference in viewpoint, appearance, and illumination. In the process of doing so, you were solving multiple times the wide multiple-baseline stereo problem (WxBS) – estimating the relative camera pose from a pair of images, different in many aspects, yet depicting the same scene.\nLet us write down the steps, which we took.\n\nIdentify salient objects and features in the images – “trees”, “statues”, “tip of the tower”, etc in images.\nDescribe the objects and features, taking into account their neighborhood: “statue with a blue left ear”.\nEstablish potential correspondences between features in the different images, based on their descriptors.\nEstimate, which direction one should move the camera to align the objects and features.\n\nThat is it! detailed explanation of the each of the steps is in this post. If you are interested in the formal definition, check here, and the history of the WxBS is here."
  },
  {
    "objectID": "posts/2020-07-22-patch-extraction.html",
    "href": "posts/2020-07-22-patch-extraction.html",
    "title": "Patch extraction: devil in details",
    "section": "",
    "text": "When working with local features one needs to pay attention to even a smallest details, or the whole process can be ruined. One of such details is how to extract the patch, which will be described by local descriptor such as SIFT or HardNet.\nUnfortunately, we cannot just extract patch from the image by cropping the patch and then resizing it. Or can we? Let’s check. We will use two versions of image: original and 4x smaller one and would like to extract same-looking fixed size patch from both of them. The patch we want to crop is showed by oriented red circle.\nEverything you (didn’t) want to know about image matching"
  },
  {
    "objectID": "posts/2020-07-22-patch-extraction.html#aliasing",
    "href": "posts/2020-07-22-patch-extraction.html#aliasing",
    "title": "Patch extraction: devil in details",
    "section": "Aliasing",
    "text": "Aliasing\nAnd here what we get by doing a simple crop and resize to 32x32 pixels.\n\nDoesn’t look good. It is called “aliasing” - a problem, which arise when we are trying to downscale big images into small resolution. Specifically: the original image contains finer details, than we could represent in thumbnail, which leads to artifacts."
  },
  {
    "objectID": "posts/2020-07-22-patch-extraction.html#the-solution-anti-aliasing",
    "href": "posts/2020-07-22-patch-extraction.html#the-solution-anti-aliasing",
    "title": "Patch extraction: devil in details",
    "section": "The solution: anti-aliasing",
    "text": "The solution: anti-aliasing\nThe solution, which follows out of sampling theorem is known: remove the details, which cannot be seens in small image first, then resample image to small size.\nThe simplest way to remove the fine details is to blur image with the Gaussian kernel.\nLets do it and compare the results. \nBy the way, you can try for yourself, all the required code is here, in kornia-examples"
  },
  {
    "objectID": "posts/2020-07-22-patch-extraction.html#performance",
    "href": "posts/2020-07-22-patch-extraction.html#performance",
    "title": "Patch extraction: devil in details",
    "section": "Performance",
    "text": "Performance\nThe problem is solved. Or is it?\nThe problem with properly antialiased patch extraction is that it is quite slow for two reasons. First, blurring a whole image is a costly operation. But, the worst part is that the required amount of blur depends on the patch size in the original image, or, in other words, keypoint scale. So for extracting, say 8000 patches, one needs to perform blurring 8000 times. Moreover, if one wants to extract elongated region and warp it to the square patch, the amount of blur in vertical and horizontal directions should be different!\nWhat can be done? Well, instead of doing blurring 8000 times, one could create so called scale pyramid and then pick the level, which is the closest to optimal one, predicted by theorem.\n\nThis is exactly, what kornia function extract_patches_from_pyramid does.\nAlso - I have a bit cheated with you above: the “anti-aliased” patches were actually extracted using the function above."
  },
  {
    "objectID": "posts/2020-07-22-patch-extraction.html#how-it-impacts-local-descriptor-matching",
    "href": "posts/2020-07-22-patch-extraction.html#how-it-impacts-local-descriptor-matching",
    "title": "Patch extraction: devil in details",
    "section": "How it impacts local descriptor matching?",
    "text": "How it impacts local descriptor matching?\nLet’s do the toy example first - describe four patches we have in the example above with HardNet descriptor and calculate the distance between them.\n\nSo the descriptor difference between antialiased patches is 0.09 and between naively extracted – 0.44. 0.09 is not a big deal, but 0.44 is a lot, actually.\nLet’s move to the non-toy example from the paper devoted to this topic: “A Few Things One Should Know About Feature Extraction, Description and Matching”.\nThe original data is lost I am too lazy to redo the experiments for the post, so I will just copy-past images with results. Here are abbrevations used in the paper:\n\nOPE – Optimal Patch Extraction. The most correct and slow way of extracting, including different amount of bluring in different directions.\nNBPE – No-Blur Patch Extraction. The most naive way we started with\nPNBPE – Pyramid, No-Blur Patch Extraction. The one, we described above - sampling patches from scale pyramid.\nPSPE Pyramid-Smoothing Patch Extraction. Pick the matching pyramid level and then add anisotropic blur missing.\n\nAs you can see, doing things optimally is quite slow. \nNow let’s see how it influences performance.\n\nIt looks like that influence is smaller than we thought. But recall that the experiment above is for SIFT descriptor only. Doing pyramid helps for the small viewpoint change almost as good, as going fully optimal, but with increasing the viewpoint difference, such approximation degrades. Moreover, it influnces MSER detector much more that than Hessian-Affine.\nHow does it work with deep descriptors like HardNet or SoSNet? That is the question which not answered yet. Drop me a message if you want to do it yourself and we can do the follow-up post together."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wide Baseline Stereo Blog",
    "section": "",
    "text": "This website is devoted to wide baseline stereo, one of the most important problems in computer vision.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChatGPT and Image Matching\n\n\n\n\n\nAre we done yet?\n\n\n\n\n\nJan 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nExperiments with LightGlue: geometry representation and initialization\n\n\n\n\n\nKeep it simple, stupid\n\n\n\n\n\nDec 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nImage Matching Challenge 2023: The Unbearable Weight of the Bundle Adjustment\n\n\n\n\n\n3D reconstruction is harder than two view matching\n\n\n\n\n\nJul 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nUn-SuperPoint family: who are they?\n\n\n\n\n\ndoes it worth to know them?\n\n\n\n\n\nJan 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nImage Matching Challenge 2022 Recap\n\n\n\n\n\nLook, LoFTRs and SuperGlue everywhere\n\n\n\n\n\nJul 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nImage Similarity Challenge 2021 recap\n\n\n\n\n\nCan you detect edited copies?\n\n\n\n\n\nFeb 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nReview of On the Limits of Pseudo Ground Truth in Visual Camera Re-localisation\n\n\n\n\n\nGet better data!\n\n\n\n\n\nSep 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nWxBS: Relaunching challenging benchmark for Image Matching\n\n\n\n\n\nAnd I mean really challenging\n\n\n\n\n\nJul 30, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nSubmitting to IMC 2021 with custom matcher\n\n\n\n\n\nAnd COLMAP\n\n\n\n\n\nMay 27, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluating OpenCV new RANSACs\n\n\n\n\n\nThey become much better now\n\n\n\n\n\nMay 17, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nImage Matching Challenge 2020 Recap\n\n\n\n\n\nSuperGlue, AdaLAM, DISK and others\n\n\n\n\n\nMay 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nSubmitting to Image Matching Challenge 2021\n\n\n\n\n\nStep by step tutorial\n\n\n\n\n\nMay 12, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nWxBS step by step\n\n\n\n\n\nOr why it is as it is\n\n\n\n\n\nFeb 11, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nWide multiple baseline stereo in simple terms\n\n\n\n\n\nOr how to explain it to everyone\n\n\n\n\n\nJan 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nLessons learned and future directions\n\n\n\n\n\nWhat I think is worthy to work on in wide baseline stereo\n\n\n\n\n\nNov 26, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nBenchmarking Image Retrieval for Visual Localization\n\n\n\n\n\nShort review\n\n\n\n\n\nNov 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nRevisiting Brown patch dataset and benchmark\n\n\n\n\n\nHow to create useful development set\n\n\n\n\n\nSep 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nHow to match images taken from really extreme viewpoints?\n\n\n\n\n\nUntil you succeed, try harder.\n\n\n\n\n\nAug 6, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nPatch extraction: devil in details\n\n\n\n\n\nHow to be smooth and fast\n\n\n\n\n\nJul 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nLocal affine features: useful side product\n\n\n\n\n\nWhy to throw away an important information?\n\n\n\n\n\nJul 17, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nWxBS: Wide Multiple Baseline Stereo as a task\n\n\n\n\n\nProblem definition\n\n\n\n\n\nJul 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nThe Role of Wide Baseline Stereo in the Deep Learning World\n\n\n\n\n\nShort history of wide baseline stereo in computer vision\n\n\n\n\n\nMar 27, 2020\n\n\n\n\n\n\nNo matching items\n\nEverything you (didn’t) want to know about image matching"
  }
]